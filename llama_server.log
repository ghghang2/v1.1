ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
common_download_file_single_online: trying to download model from https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-F16.gguf to /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf.downloadInProgress (etag:"78f73a4ef91c8f92d4df971f570ff3719007201f6d955b8695384a1b21b04a80")...
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14992 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1578 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 64000 -> need 1580 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.73 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14992 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
srv  log_server_r: request: GET /health 127.0.0.1 503
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 64000
llama_context: n_ctx_seq     = 64000
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (64000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 64000 cells
llama_kv_cache:      CUDA0 KV buffer size =  1500.00 MiB
llama_kv_cache: size = 1500.00 MiB ( 64000 cells,  12 layers,  4/1 seqs), K (f16):  750.00 MiB, V (f16):  750.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   132.65 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 100.33 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  1 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  2 | task -1 | new slot, n_ctx = 64000
slot   load_model: id  3 | task -1 | new slot, n_ctx = 64000
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-02

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 37441
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.054699
slot update_slots: id  3 | task 0 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.109399
slot update_slots: id  3 | task 0 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.164098
slot update_slots: id  3 | task 0 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.218798
slot update_slots: id  3 | task 0 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.273497
slot update_slots: id  3 | task 0 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.328196
slot update_slots: id  3 | task 0 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.382896
slot update_slots: id  3 | task 0 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.437595
slot update_slots: id  3 | task 0 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.492295
slot update_slots: id  3 | task 0 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.546994
slot update_slots: id  3 | task 0 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.601693
slot update_slots: id  3 | task 0 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.656393
slot update_slots: id  3 | task 0 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.711092
slot update_slots: id  3 | task 0 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.765792
slot update_slots: id  3 | task 0 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.820491
slot update_slots: id  3 | task 0 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.875190
slot update_slots: id  3 | task 0 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.929890
slot update_slots: id  3 | task 0 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 36864, batch.n_tokens = 2048, progress = 0.984589
slot update_slots: id  3 | task 0 | n_tokens = 36864, memory_seq_rm [36864, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 37377, batch.n_tokens = 513, progress = 0.998291
slot update_slots: id  3 | task 0 | n_tokens = 37377, memory_seq_rm [37377, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 37441, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 37441, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 5.58 ms, tokens: text = 37441, total = 37441
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 36353, pos_max = 37376, size = 24.012 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =   44872.27 ms / 37441 tokens (    1.20 ms per token,   834.39 tokens per second)
       eval time =    6236.63 ms /   211 tokens (   29.56 ms per token,    33.83 tokens per second)
      total time =   51108.90 ms / 37652 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 37651, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 231 | processing task, is_child = 0
slot update_slots: id  3 | task 231 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 37544
slot update_slots: id  3 | task 231 | n_tokens = 37442, memory_seq_rm [37442, end)
slot update_slots: id  3 | task 231 | prompt processing progress, n_tokens = 37480, batch.n_tokens = 38, progress = 0.998295
slot update_slots: id  3 | task 231 | n_tokens = 37480, memory_seq_rm [37480, end)
slot update_slots: id  3 | task 231 | prompt processing progress, n_tokens = 37544, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 231 | prompt done, n_tokens = 37544, batch.n_tokens = 64
slot init_sampler: id  3 | task 231 | init sampler, took 5.21 ms, tokens: text = 37544, total = 37544
slot update_slots: id  3 | task 231 | created context checkpoint 2 of 8 (pos_min = 36627, pos_max = 37479, size = 20.002 MiB)
slot print_timing: id  3 | task 231 | 
prompt eval time =     474.97 ms /   102 tokens (    4.66 ms per token,   214.75 tokens per second)
       eval time =   55015.25 ms /  1885 tokens (   29.19 ms per token,    34.26 tokens per second)
      total time =   55490.22 ms /  1987 tokens
slot      release: id  3 | task 231 | stop processing: n_tokens = 39428, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.957 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2118 | processing task, is_child = 0
slot update_slots: id  3 | task 2118 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 39226
slot update_slots: id  3 | task 2118 | n_past = 37545, slot.prompt.tokens.size() = 39428, seq_id = 3, pos_min = 38404, n_swa = 128
slot update_slots: id  3 | task 2118 | restored context checkpoint (pos_min = 36627, pos_max = 37479, size = 20.002 MiB)
slot update_slots: id  3 | task 2118 | n_tokens = 37479, memory_seq_rm [37479, end)
slot update_slots: id  3 | task 2118 | prompt processing progress, n_tokens = 39162, batch.n_tokens = 1683, progress = 0.998368
slot update_slots: id  3 | task 2118 | n_tokens = 39162, memory_seq_rm [39162, end)
slot update_slots: id  3 | task 2118 | prompt processing progress, n_tokens = 39226, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2118 | prompt done, n_tokens = 39226, batch.n_tokens = 64
slot init_sampler: id  3 | task 2118 | init sampler, took 7.36 ms, tokens: text = 39226, total = 39226
slot update_slots: id  3 | task 2118 | created context checkpoint 3 of 8 (pos_min = 38138, pos_max = 39161, size = 24.012 MiB)
slot print_timing: id  3 | task 2118 | 
prompt eval time =    3133.25 ms /  1747 tokens (    1.79 ms per token,   557.57 tokens per second)
       eval time =   37228.56 ms /  1252 tokens (   29.74 ms per token,    33.63 tokens per second)
      total time =   40361.81 ms /  2999 tokens
slot      release: id  3 | task 2118 | stop processing: n_tokens = 40477, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3372 | processing task, is_child = 0
slot update_slots: id  3 | task 3372 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 40166
slot update_slots: id  3 | task 3372 | n_past = 39227, slot.prompt.tokens.size() = 40477, seq_id = 3, pos_min = 39453, n_swa = 128
slot update_slots: id  3 | task 3372 | restored context checkpoint (pos_min = 38138, pos_max = 39161, size = 24.012 MiB)
slot update_slots: id  3 | task 3372 | n_tokens = 39161, memory_seq_rm [39161, end)
slot update_slots: id  3 | task 3372 | prompt processing progress, n_tokens = 40102, batch.n_tokens = 941, progress = 0.998407
slot update_slots: id  3 | task 3372 | n_tokens = 40102, memory_seq_rm [40102, end)
slot update_slots: id  3 | task 3372 | prompt processing progress, n_tokens = 40166, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3372 | prompt done, n_tokens = 40166, batch.n_tokens = 64
slot init_sampler: id  3 | task 3372 | init sampler, took 8.25 ms, tokens: text = 40166, total = 40166
slot update_slots: id  3 | task 3372 | created context checkpoint 4 of 8 (pos_min = 39078, pos_max = 40101, size = 24.012 MiB)
slot print_timing: id  3 | task 3372 | 
prompt eval time =    1810.13 ms /  1005 tokens (    1.80 ms per token,   555.21 tokens per second)
       eval time =   26259.02 ms /   928 tokens (   28.30 ms per token,    35.34 tokens per second)
      total time =   28069.15 ms /  1933 tokens
slot      release: id  3 | task 3372 | stop processing: n_tokens = 41093, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.977 (> 0.100 thold), f_keep = 0.977
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4302 | processing task, is_child = 0
slot update_slots: id  3 | task 4302 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 41096
slot update_slots: id  3 | task 4302 | n_past = 40166, slot.prompt.tokens.size() = 41093, seq_id = 3, pos_min = 40069, n_swa = 128
slot update_slots: id  3 | task 4302 | restored context checkpoint (pos_min = 39078, pos_max = 40101, size = 24.012 MiB)
slot update_slots: id  3 | task 4302 | n_tokens = 40101, memory_seq_rm [40101, end)
slot update_slots: id  3 | task 4302 | prompt processing progress, n_tokens = 41032, batch.n_tokens = 931, progress = 0.998443
slot update_slots: id  3 | task 4302 | n_tokens = 41032, memory_seq_rm [41032, end)
slot update_slots: id  3 | task 4302 | prompt processing progress, n_tokens = 41096, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4302 | prompt done, n_tokens = 41096, batch.n_tokens = 64
slot init_sampler: id  3 | task 4302 | init sampler, took 8.38 ms, tokens: text = 41096, total = 41096
slot update_slots: id  3 | task 4302 | created context checkpoint 5 of 8 (pos_min = 40008, pos_max = 41031, size = 24.012 MiB)
slot print_timing: id  3 | task 4302 | 
prompt eval time =    1771.99 ms /   995 tokens (    1.78 ms per token,   561.51 tokens per second)
       eval time =   22367.82 ms /   786 tokens (   28.46 ms per token,    35.14 tokens per second)
      total time =   24139.82 ms /  1781 tokens
slot      release: id  3 | task 4302 | stop processing: n_tokens = 41881, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.981
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5090 | processing task, is_child = 0
slot update_slots: id  3 | task 5090 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 41921
slot update_slots: id  3 | task 5090 | n_tokens = 41096, memory_seq_rm [41096, end)
slot update_slots: id  3 | task 5090 | prompt processing progress, n_tokens = 41857, batch.n_tokens = 761, progress = 0.998473
slot update_slots: id  3 | task 5090 | n_tokens = 41857, memory_seq_rm [41857, end)
slot update_slots: id  3 | task 5090 | prompt processing progress, n_tokens = 41921, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5090 | prompt done, n_tokens = 41921, batch.n_tokens = 64
slot init_sampler: id  3 | task 5090 | init sampler, took 8.06 ms, tokens: text = 41921, total = 41921
slot update_slots: id  3 | task 5090 | created context checkpoint 6 of 8 (pos_min = 40857, pos_max = 41856, size = 23.449 MiB)
slot print_timing: id  3 | task 5090 | 
prompt eval time =    1653.00 ms /   825 tokens (    2.00 ms per token,   499.09 tokens per second)
       eval time =    1801.88 ms /    62 tokens (   29.06 ms per token,    34.41 tokens per second)
      total time =    3454.88 ms /   887 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5090 | stop processing: n_tokens = 41982, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.962 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5154 | processing task, is_child = 0
slot update_slots: id  3 | task 5154 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 43576
slot update_slots: id  3 | task 5154 | n_tokens = 41921, memory_seq_rm [41921, end)
slot update_slots: id  3 | task 5154 | prompt processing progress, n_tokens = 43512, batch.n_tokens = 1591, progress = 0.998531
slot update_slots: id  3 | task 5154 | n_tokens = 43512, memory_seq_rm [43512, end)
slot update_slots: id  3 | task 5154 | prompt processing progress, n_tokens = 43576, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5154 | prompt done, n_tokens = 43576, batch.n_tokens = 64
slot init_sampler: id  3 | task 5154 | init sampler, took 5.99 ms, tokens: text = 43576, total = 43576
slot update_slots: id  3 | task 5154 | created context checkpoint 7 of 8 (pos_min = 42488, pos_max = 43511, size = 24.012 MiB)
slot print_timing: id  3 | task 5154 | 
prompt eval time =    3044.73 ms /  1655 tokens (    1.84 ms per token,   543.56 tokens per second)
       eval time =   13775.06 ms /   466 tokens (   29.56 ms per token,    33.83 tokens per second)
      total time =   16819.79 ms /  2121 tokens
slot      release: id  3 | task 5154 | stop processing: n_tokens = 44041, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.989
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5622 | processing task, is_child = 0
slot update_slots: id  3 | task 5622 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 43797
slot update_slots: id  3 | task 5622 | n_tokens = 43576, memory_seq_rm [43576, end)
slot update_slots: id  3 | task 5622 | prompt processing progress, n_tokens = 43733, batch.n_tokens = 157, progress = 0.998539
slot update_slots: id  3 | task 5622 | n_tokens = 43733, memory_seq_rm [43733, end)
slot update_slots: id  3 | task 5622 | prompt processing progress, n_tokens = 43797, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5622 | prompt done, n_tokens = 43797, batch.n_tokens = 64
slot init_sampler: id  3 | task 5622 | init sampler, took 6.82 ms, tokens: text = 43797, total = 43797
slot update_slots: id  3 | task 5622 | created context checkpoint 8 of 8 (pos_min = 43165, pos_max = 43732, size = 13.319 MiB)
slot print_timing: id  3 | task 5622 | 
prompt eval time =     581.41 ms /   221 tokens (    2.63 ms per token,   380.11 tokens per second)
       eval time =    1847.36 ms /    63 tokens (   29.32 ms per token,    34.10 tokens per second)
      total time =    2428.78 ms /   284 tokens
slot      release: id  3 | task 5622 | stop processing: n_tokens = 43859, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5687 | processing task, is_child = 0
slot update_slots: id  3 | task 5687 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 44262
slot update_slots: id  3 | task 5687 | n_tokens = 43797, memory_seq_rm [43797, end)
slot update_slots: id  3 | task 5687 | prompt processing progress, n_tokens = 44198, batch.n_tokens = 401, progress = 0.998554
slot update_slots: id  3 | task 5687 | n_tokens = 44198, memory_seq_rm [44198, end)
slot update_slots: id  3 | task 5687 | prompt processing progress, n_tokens = 44262, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5687 | prompt done, n_tokens = 44262, batch.n_tokens = 64
slot init_sampler: id  3 | task 5687 | init sampler, took 8.39 ms, tokens: text = 44262, total = 44262
slot update_slots: id  3 | task 5687 | erasing old context checkpoint (pos_min = 36353, pos_max = 37376, size = 24.012 MiB)
slot update_slots: id  3 | task 5687 | created context checkpoint 8 of 8 (pos_min = 43576, pos_max = 44197, size = 14.586 MiB)
slot print_timing: id  3 | task 5687 | 
prompt eval time =     982.09 ms /   465 tokens (    2.11 ms per token,   473.48 tokens per second)
       eval time =    5449.37 ms /   186 tokens (   29.30 ms per token,    34.13 tokens per second)
      total time =    6431.45 ms /   651 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5687 | stop processing: n_tokens = 44447, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5875 | processing task, is_child = 0
slot update_slots: id  3 | task 5875 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 44426
slot update_slots: id  3 | task 5875 | n_tokens = 44262, memory_seq_rm [44262, end)
slot update_slots: id  3 | task 5875 | prompt processing progress, n_tokens = 44362, batch.n_tokens = 100, progress = 0.998559
slot update_slots: id  3 | task 5875 | n_tokens = 44362, memory_seq_rm [44362, end)
slot update_slots: id  3 | task 5875 | prompt processing progress, n_tokens = 44426, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5875 | prompt done, n_tokens = 44426, batch.n_tokens = 64
slot init_sampler: id  3 | task 5875 | init sampler, took 9.01 ms, tokens: text = 44426, total = 44426
slot update_slots: id  3 | task 5875 | erasing old context checkpoint (pos_min = 36627, pos_max = 37479, size = 20.002 MiB)
slot update_slots: id  3 | task 5875 | created context checkpoint 8 of 8 (pos_min = 43576, pos_max = 44361, size = 18.431 MiB)
slot print_timing: id  3 | task 5875 | 
prompt eval time =     567.78 ms /   164 tokens (    3.46 ms per token,   288.85 tokens per second)
       eval time =    6215.38 ms /   212 tokens (   29.32 ms per token,    34.11 tokens per second)
      total time =    6783.15 ms /   376 tokens
slot      release: id  3 | task 5875 | stop processing: n_tokens = 44637, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.964 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6089 | processing task, is_child = 0
slot update_slots: id  3 | task 6089 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 46081
slot update_slots: id  3 | task 6089 | n_tokens = 44426, memory_seq_rm [44426, end)
slot update_slots: id  3 | task 6089 | prompt processing progress, n_tokens = 46017, batch.n_tokens = 1591, progress = 0.998611
slot update_slots: id  3 | task 6089 | n_tokens = 46017, memory_seq_rm [46017, end)
slot update_slots: id  3 | task 6089 | prompt processing progress, n_tokens = 46081, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6089 | prompt done, n_tokens = 46081, batch.n_tokens = 64
slot init_sampler: id  3 | task 6089 | init sampler, took 6.93 ms, tokens: text = 46081, total = 46081
slot update_slots: id  3 | task 6089 | erasing old context checkpoint (pos_min = 38138, pos_max = 39161, size = 24.012 MiB)
slot update_slots: id  3 | task 6089 | created context checkpoint 8 of 8 (pos_min = 44993, pos_max = 46016, size = 24.012 MiB)
slot print_timing: id  3 | task 6089 | 
prompt eval time =    3036.02 ms /  1655 tokens (    1.83 ms per token,   545.12 tokens per second)
       eval time =    4246.45 ms /   145 tokens (   29.29 ms per token,    34.15 tokens per second)
      total time =    7282.47 ms /  1800 tokens
slot      release: id  3 | task 6089 | stop processing: n_tokens = 46225, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6236 | processing task, is_child = 0
slot update_slots: id  3 | task 6236 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 46874
slot update_slots: id  3 | task 6236 | n_tokens = 46081, memory_seq_rm [46081, end)
slot update_slots: id  3 | task 6236 | prompt processing progress, n_tokens = 46810, batch.n_tokens = 729, progress = 0.998635
slot update_slots: id  3 | task 6236 | n_tokens = 46810, memory_seq_rm [46810, end)
slot update_slots: id  3 | task 6236 | prompt processing progress, n_tokens = 46874, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6236 | prompt done, n_tokens = 46874, batch.n_tokens = 64
slot init_sampler: id  3 | task 6236 | init sampler, took 7.01 ms, tokens: text = 46874, total = 46874
slot update_slots: id  3 | task 6236 | erasing old context checkpoint (pos_min = 39078, pos_max = 40101, size = 24.012 MiB)
slot update_slots: id  3 | task 6236 | created context checkpoint 8 of 8 (pos_min = 45786, pos_max = 46809, size = 24.012 MiB)
slot print_timing: id  3 | task 6236 | 
prompt eval time =    1599.06 ms /   793 tokens (    2.02 ms per token,   495.92 tokens per second)
       eval time =    8252.74 ms /   282 tokens (   29.27 ms per token,    34.17 tokens per second)
      total time =    9851.80 ms /  1075 tokens
slot      release: id  3 | task 6236 | stop processing: n_tokens = 47155, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6520 | processing task, is_child = 0
slot update_slots: id  3 | task 6520 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 47059
slot update_slots: id  3 | task 6520 | n_tokens = 46874, memory_seq_rm [46874, end)
slot update_slots: id  3 | task 6520 | prompt processing progress, n_tokens = 46995, batch.n_tokens = 121, progress = 0.998640
slot update_slots: id  3 | task 6520 | n_tokens = 46995, memory_seq_rm [46995, end)
slot update_slots: id  3 | task 6520 | prompt processing progress, n_tokens = 47059, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6520 | prompt done, n_tokens = 47059, batch.n_tokens = 64
slot init_sampler: id  3 | task 6520 | init sampler, took 9.33 ms, tokens: text = 47059, total = 47059
slot update_slots: id  3 | task 6520 | erasing old context checkpoint (pos_min = 40008, pos_max = 41031, size = 24.012 MiB)
slot update_slots: id  3 | task 6520 | created context checkpoint 8 of 8 (pos_min = 46131, pos_max = 46994, size = 20.260 MiB)
slot print_timing: id  3 | task 6520 | 
prompt eval time =     714.61 ms /   185 tokens (    3.86 ms per token,   258.88 tokens per second)
       eval time =    5517.95 ms /   188 tokens (   29.35 ms per token,    34.07 tokens per second)
      total time =    6232.56 ms /   373 tokens
slot      release: id  3 | task 6520 | stop processing: n_tokens = 47246, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6710 | processing task, is_child = 0
slot update_slots: id  3 | task 6710 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 47216
slot update_slots: id  3 | task 6710 | n_tokens = 47059, memory_seq_rm [47059, end)
slot update_slots: id  3 | task 6710 | prompt processing progress, n_tokens = 47152, batch.n_tokens = 93, progress = 0.998645
slot update_slots: id  3 | task 6710 | n_tokens = 47152, memory_seq_rm [47152, end)
slot update_slots: id  3 | task 6710 | prompt processing progress, n_tokens = 47216, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6710 | prompt done, n_tokens = 47216, batch.n_tokens = 64
slot init_sampler: id  3 | task 6710 | init sampler, took 6.62 ms, tokens: text = 47216, total = 47216
slot update_slots: id  3 | task 6710 | erasing old context checkpoint (pos_min = 40857, pos_max = 41856, size = 23.449 MiB)
slot update_slots: id  3 | task 6710 | created context checkpoint 8 of 8 (pos_min = 46222, pos_max = 47151, size = 21.808 MiB)
slot print_timing: id  3 | task 6710 | 
prompt eval time =     533.22 ms /   157 tokens (    3.40 ms per token,   294.44 tokens per second)
       eval time =    5501.49 ms /   187 tokens (   29.42 ms per token,    33.99 tokens per second)
      total time =    6034.71 ms /   344 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 6710 | stop processing: n_tokens = 47402, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6899 | processing task, is_child = 0
slot update_slots: id  3 | task 6899 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 47568
slot update_slots: id  3 | task 6899 | n_tokens = 47216, memory_seq_rm [47216, end)
slot update_slots: id  3 | task 6899 | prompt processing progress, n_tokens = 47504, batch.n_tokens = 288, progress = 0.998655
slot update_slots: id  3 | task 6899 | n_tokens = 47504, memory_seq_rm [47504, end)
slot update_slots: id  3 | task 6899 | prompt processing progress, n_tokens = 47568, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 6899 | prompt done, n_tokens = 47568, batch.n_tokens = 64
slot init_sampler: id  3 | task 6899 | init sampler, took 9.26 ms, tokens: text = 47568, total = 47568
slot update_slots: id  3 | task 6899 | erasing old context checkpoint (pos_min = 42488, pos_max = 43511, size = 24.012 MiB)
slot update_slots: id  3 | task 6899 | created context checkpoint 8 of 8 (pos_min = 46480, pos_max = 47503, size = 24.012 MiB)
slot print_timing: id  3 | task 6899 | 
prompt eval time =     797.02 ms /   352 tokens (    2.26 ms per token,   441.64 tokens per second)
       eval time =    5646.60 ms /   192 tokens (   29.41 ms per token,    34.00 tokens per second)
      total time =    6443.62 ms /   544 tokens
slot      release: id  3 | task 6899 | stop processing: n_tokens = 47759, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7093 | processing task, is_child = 0
slot update_slots: id  3 | task 7093 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 47725
slot update_slots: id  3 | task 7093 | n_tokens = 47568, memory_seq_rm [47568, end)
slot update_slots: id  3 | task 7093 | prompt processing progress, n_tokens = 47661, batch.n_tokens = 93, progress = 0.998659
slot update_slots: id  3 | task 7093 | n_tokens = 47661, memory_seq_rm [47661, end)
slot update_slots: id  3 | task 7093 | prompt processing progress, n_tokens = 47725, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7093 | prompt done, n_tokens = 47725, batch.n_tokens = 64
slot init_sampler: id  3 | task 7093 | init sampler, took 7.05 ms, tokens: text = 47725, total = 47725
slot update_slots: id  3 | task 7093 | erasing old context checkpoint (pos_min = 43165, pos_max = 43732, size = 13.319 MiB)
slot update_slots: id  3 | task 7093 | created context checkpoint 8 of 8 (pos_min = 46735, pos_max = 47660, size = 21.714 MiB)
slot print_timing: id  3 | task 7093 | 
prompt eval time =     541.23 ms /   157 tokens (    3.45 ms per token,   290.08 tokens per second)
       eval time =    5583.37 ms /   189 tokens (   29.54 ms per token,    33.85 tokens per second)
      total time =    6124.60 ms /   346 tokens
slot      release: id  3 | task 7093 | stop processing: n_tokens = 47913, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7284 | processing task, is_child = 0
slot update_slots: id  3 | task 7284 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 47882
slot update_slots: id  3 | task 7284 | n_tokens = 47725, memory_seq_rm [47725, end)
slot update_slots: id  3 | task 7284 | prompt processing progress, n_tokens = 47818, batch.n_tokens = 93, progress = 0.998663
slot update_slots: id  3 | task 7284 | n_tokens = 47818, memory_seq_rm [47818, end)
slot update_slots: id  3 | task 7284 | prompt processing progress, n_tokens = 47882, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7284 | prompt done, n_tokens = 47882, batch.n_tokens = 64
slot init_sampler: id  3 | task 7284 | init sampler, took 10.12 ms, tokens: text = 47882, total = 47882
slot update_slots: id  3 | task 7284 | erasing old context checkpoint (pos_min = 43576, pos_max = 44197, size = 14.586 MiB)
slot update_slots: id  3 | task 7284 | created context checkpoint 8 of 8 (pos_min = 46889, pos_max = 47817, size = 21.784 MiB)
slot print_timing: id  3 | task 7284 | 
prompt eval time =     541.47 ms /   157 tokens (    3.45 ms per token,   289.95 tokens per second)
       eval time =    1477.05 ms /    49 tokens (   30.14 ms per token,    33.17 tokens per second)
      total time =    2018.52 ms /   206 tokens
slot      release: id  3 | task 7284 | stop processing: n_tokens = 47930, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.980 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7335 | processing task, is_child = 0
slot update_slots: id  3 | task 7335 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 48884
slot update_slots: id  3 | task 7335 | n_tokens = 47882, memory_seq_rm [47882, end)
slot update_slots: id  3 | task 7335 | prompt processing progress, n_tokens = 48820, batch.n_tokens = 938, progress = 0.998691
slot update_slots: id  3 | task 7335 | n_tokens = 48820, memory_seq_rm [48820, end)
slot update_slots: id  3 | task 7335 | prompt processing progress, n_tokens = 48884, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7335 | prompt done, n_tokens = 48884, batch.n_tokens = 64
slot init_sampler: id  3 | task 7335 | init sampler, took 6.70 ms, tokens: text = 48884, total = 48884
slot update_slots: id  3 | task 7335 | erasing old context checkpoint (pos_min = 43576, pos_max = 44361, size = 18.431 MiB)
slot update_slots: id  3 | task 7335 | created context checkpoint 8 of 8 (pos_min = 47796, pos_max = 48819, size = 24.012 MiB)
slot print_timing: id  3 | task 7335 | 
prompt eval time =    1905.65 ms /  1002 tokens (    1.90 ms per token,   525.81 tokens per second)
       eval time =   11268.97 ms /   379 tokens (   29.73 ms per token,    33.63 tokens per second)
      total time =   13174.61 ms /  1381 tokens
slot      release: id  3 | task 7335 | stop processing: n_tokens = 49262, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.992
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7716 | processing task, is_child = 0
slot update_slots: id  3 | task 7716 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 49068
slot update_slots: id  3 | task 7716 | n_tokens = 48884, memory_seq_rm [48884, end)
slot update_slots: id  3 | task 7716 | prompt processing progress, n_tokens = 49004, batch.n_tokens = 120, progress = 0.998696
slot update_slots: id  3 | task 7716 | n_tokens = 49004, memory_seq_rm [49004, end)
slot update_slots: id  3 | task 7716 | prompt processing progress, n_tokens = 49068, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7716 | prompt done, n_tokens = 49068, batch.n_tokens = 64
slot init_sampler: id  3 | task 7716 | init sampler, took 7.13 ms, tokens: text = 49068, total = 49068
slot update_slots: id  3 | task 7716 | erasing old context checkpoint (pos_min = 44993, pos_max = 46016, size = 24.012 MiB)
slot update_slots: id  3 | task 7716 | created context checkpoint 8 of 8 (pos_min = 48238, pos_max = 49003, size = 17.962 MiB)
slot print_timing: id  3 | task 7716 | 
prompt eval time =     634.23 ms /   184 tokens (    3.45 ms per token,   290.12 tokens per second)
       eval time =    9581.09 ms /   319 tokens (   30.03 ms per token,    33.29 tokens per second)
      total time =   10215.32 ms /   503 tokens
slot      release: id  3 | task 7716 | stop processing: n_tokens = 49386, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8037 | processing task, is_child = 0
slot update_slots: id  3 | task 8037 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 49241
slot update_slots: id  3 | task 8037 | n_tokens = 49068, memory_seq_rm [49068, end)
slot update_slots: id  3 | task 8037 | prompt processing progress, n_tokens = 49177, batch.n_tokens = 109, progress = 0.998700
slot update_slots: id  3 | task 8037 | n_tokens = 49177, memory_seq_rm [49177, end)
slot update_slots: id  3 | task 8037 | prompt processing progress, n_tokens = 49241, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8037 | prompt done, n_tokens = 49241, batch.n_tokens = 64
slot init_sampler: id  3 | task 8037 | init sampler, took 9.28 ms, tokens: text = 49241, total = 49241
slot update_slots: id  3 | task 8037 | erasing old context checkpoint (pos_min = 45786, pos_max = 46809, size = 24.012 MiB)
slot update_slots: id  3 | task 8037 | created context checkpoint 8 of 8 (pos_min = 48362, pos_max = 49176, size = 19.111 MiB)
slot print_timing: id  3 | task 8037 | 
prompt eval time =     585.40 ms /   173 tokens (    3.38 ms per token,   295.52 tokens per second)
       eval time =    5946.10 ms /   199 tokens (   29.88 ms per token,    33.47 tokens per second)
      total time =    6531.50 ms /   372 tokens
slot      release: id  3 | task 8037 | stop processing: n_tokens = 49439, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8238 | processing task, is_child = 0
slot update_slots: id  3 | task 8238 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 49398
slot update_slots: id  3 | task 8238 | n_tokens = 49241, memory_seq_rm [49241, end)
slot update_slots: id  3 | task 8238 | prompt processing progress, n_tokens = 49334, batch.n_tokens = 93, progress = 0.998704
slot update_slots: id  3 | task 8238 | n_tokens = 49334, memory_seq_rm [49334, end)
slot update_slots: id  3 | task 8238 | prompt processing progress, n_tokens = 49398, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8238 | prompt done, n_tokens = 49398, batch.n_tokens = 64
slot init_sampler: id  3 | task 8238 | init sampler, took 6.89 ms, tokens: text = 49398, total = 49398
slot update_slots: id  3 | task 8238 | erasing old context checkpoint (pos_min = 46131, pos_max = 46994, size = 20.260 MiB)
slot update_slots: id  3 | task 8238 | created context checkpoint 8 of 8 (pos_min = 48415, pos_max = 49333, size = 21.550 MiB)
slot print_timing: id  3 | task 8238 | 
prompt eval time =     545.93 ms /   157 tokens (    3.48 ms per token,   287.58 tokens per second)
       eval time =    1610.52 ms /    54 tokens (   29.82 ms per token,    33.53 tokens per second)
      total time =    2156.45 ms /   211 tokens
slot      release: id  3 | task 8238 | stop processing: n_tokens = 49451, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.993 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8294 | processing task, is_child = 0
slot update_slots: id  3 | task 8294 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 49750
slot update_slots: id  3 | task 8294 | n_tokens = 49398, memory_seq_rm [49398, end)
slot update_slots: id  3 | task 8294 | prompt processing progress, n_tokens = 49686, batch.n_tokens = 288, progress = 0.998714
slot update_slots: id  3 | task 8294 | n_tokens = 49686, memory_seq_rm [49686, end)
slot update_slots: id  3 | task 8294 | prompt processing progress, n_tokens = 49750, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8294 | prompt done, n_tokens = 49750, batch.n_tokens = 64
slot init_sampler: id  3 | task 8294 | init sampler, took 6.89 ms, tokens: text = 49750, total = 49750
slot update_slots: id  3 | task 8294 | erasing old context checkpoint (pos_min = 46222, pos_max = 47151, size = 21.808 MiB)
slot update_slots: id  3 | task 8294 | created context checkpoint 8 of 8 (pos_min = 48662, pos_max = 49685, size = 24.012 MiB)
slot print_timing: id  3 | task 8294 | 
prompt eval time =     804.45 ms /   352 tokens (    2.29 ms per token,   437.57 tokens per second)
       eval time =    6013.37 ms /   201 tokens (   29.92 ms per token,    33.43 tokens per second)
      total time =    6817.81 ms /   553 tokens
slot      release: id  3 | task 8294 | stop processing: n_tokens = 49950, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8497 | processing task, is_child = 0
slot update_slots: id  3 | task 8497 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 50215
slot update_slots: id  3 | task 8497 | n_tokens = 49750, memory_seq_rm [49750, end)
slot update_slots: id  3 | task 8497 | prompt processing progress, n_tokens = 50151, batch.n_tokens = 401, progress = 0.998725
slot update_slots: id  3 | task 8497 | n_tokens = 50151, memory_seq_rm [50151, end)
slot update_slots: id  3 | task 8497 | prompt processing progress, n_tokens = 50215, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8497 | prompt done, n_tokens = 50215, batch.n_tokens = 64
slot init_sampler: id  3 | task 8497 | init sampler, took 6.89 ms, tokens: text = 50215, total = 50215
slot update_slots: id  3 | task 8497 | erasing old context checkpoint (pos_min = 46480, pos_max = 47503, size = 24.012 MiB)
slot update_slots: id  3 | task 8497 | created context checkpoint 8 of 8 (pos_min = 49127, pos_max = 50150, size = 24.012 MiB)
slot print_timing: id  3 | task 8497 | 
prompt eval time =    1020.07 ms /   465 tokens (    2.19 ms per token,   455.85 tokens per second)
       eval time =    2521.89 ms /    85 tokens (   29.67 ms per token,    33.70 tokens per second)
      total time =    3541.96 ms /   550 tokens
slot      release: id  3 | task 8497 | stop processing: n_tokens = 50299, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8584 | processing task, is_child = 0
slot update_slots: id  3 | task 8584 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 50446
slot update_slots: id  3 | task 8584 | n_tokens = 50215, memory_seq_rm [50215, end)
slot update_slots: id  3 | task 8584 | prompt processing progress, n_tokens = 50382, batch.n_tokens = 167, progress = 0.998731
slot update_slots: id  3 | task 8584 | n_tokens = 50382, memory_seq_rm [50382, end)
slot update_slots: id  3 | task 8584 | prompt processing progress, n_tokens = 50446, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8584 | prompt done, n_tokens = 50446, batch.n_tokens = 64
slot init_sampler: id  3 | task 8584 | init sampler, took 6.96 ms, tokens: text = 50446, total = 50446
slot update_slots: id  3 | task 8584 | erasing old context checkpoint (pos_min = 46735, pos_max = 47660, size = 21.714 MiB)
slot update_slots: id  3 | task 8584 | created context checkpoint 8 of 8 (pos_min = 49358, pos_max = 50381, size = 24.012 MiB)
slot print_timing: id  3 | task 8584 | 
prompt eval time =     642.30 ms /   231 tokens (    2.78 ms per token,   359.64 tokens per second)
       eval time =    2899.50 ms /    97 tokens (   29.89 ms per token,    33.45 tokens per second)
      total time =    3541.80 ms /   328 tokens
slot      release: id  3 | task 8584 | stop processing: n_tokens = 50542, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8683 | processing task, is_child = 0
slot update_slots: id  3 | task 8683 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 50990
slot update_slots: id  3 | task 8683 | n_tokens = 50446, memory_seq_rm [50446, end)
slot update_slots: id  3 | task 8683 | prompt processing progress, n_tokens = 50926, batch.n_tokens = 480, progress = 0.998745
slot update_slots: id  3 | task 8683 | n_tokens = 50926, memory_seq_rm [50926, end)
slot update_slots: id  3 | task 8683 | prompt processing progress, n_tokens = 50990, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8683 | prompt done, n_tokens = 50990, batch.n_tokens = 64
slot init_sampler: id  3 | task 8683 | init sampler, took 9.23 ms, tokens: text = 50990, total = 50990
slot update_slots: id  3 | task 8683 | erasing old context checkpoint (pos_min = 46889, pos_max = 47817, size = 21.784 MiB)
slot update_slots: id  3 | task 8683 | created context checkpoint 8 of 8 (pos_min = 49902, pos_max = 50925, size = 24.012 MiB)
slot print_timing: id  3 | task 8683 | 
prompt eval time =    1139.38 ms /   544 tokens (    2.09 ms per token,   477.45 tokens per second)
       eval time =    3398.28 ms /   114 tokens (   29.81 ms per token,    33.55 tokens per second)
      total time =    4537.66 ms /   658 tokens
slot      release: id  3 | task 8683 | stop processing: n_tokens = 51103, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.994 (> 0.100 thold), f_keep = 0.998
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8799 | processing task, is_child = 0
slot update_slots: id  3 | task 8799 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 51313
slot update_slots: id  3 | task 8799 | n_tokens = 50990, memory_seq_rm [50990, end)
slot update_slots: id  3 | task 8799 | prompt processing progress, n_tokens = 51249, batch.n_tokens = 259, progress = 0.998753
slot update_slots: id  3 | task 8799 | n_tokens = 51249, memory_seq_rm [51249, end)
slot update_slots: id  3 | task 8799 | prompt processing progress, n_tokens = 51313, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8799 | prompt done, n_tokens = 51313, batch.n_tokens = 64
slot init_sampler: id  3 | task 8799 | init sampler, took 7.22 ms, tokens: text = 51313, total = 51313
slot update_slots: id  3 | task 8799 | erasing old context checkpoint (pos_min = 47796, pos_max = 48819, size = 24.012 MiB)
slot update_slots: id  3 | task 8799 | created context checkpoint 8 of 8 (pos_min = 50225, pos_max = 51248, size = 24.012 MiB)
slot print_timing: id  3 | task 8799 | 
prompt eval time =     774.65 ms /   323 tokens (    2.40 ms per token,   416.96 tokens per second)
       eval time =    5450.68 ms /   182 tokens (   29.95 ms per token,    33.39 tokens per second)
      total time =    6225.33 ms /   505 tokens
slot      release: id  3 | task 8799 | stop processing: n_tokens = 51494, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.997 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 8983 | processing task, is_child = 0
slot update_slots: id  3 | task 8983 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 51470
slot update_slots: id  3 | task 8983 | n_tokens = 51313, memory_seq_rm [51313, end)
slot update_slots: id  3 | task 8983 | prompt processing progress, n_tokens = 51406, batch.n_tokens = 93, progress = 0.998757
slot update_slots: id  3 | task 8983 | n_tokens = 51406, memory_seq_rm [51406, end)
slot update_slots: id  3 | task 8983 | prompt processing progress, n_tokens = 51470, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 8983 | prompt done, n_tokens = 51470, batch.n_tokens = 64
slot init_sampler: id  3 | task 8983 | init sampler, took 7.05 ms, tokens: text = 51470, total = 51470
slot update_slots: id  3 | task 8983 | erasing old context checkpoint (pos_min = 48238, pos_max = 49003, size = 17.962 MiB)
slot update_slots: id  3 | task 8983 | created context checkpoint 8 of 8 (pos_min = 50470, pos_max = 51405, size = 21.949 MiB)
slot print_timing: id  3 | task 8983 | 
prompt eval time =     546.75 ms /   157 tokens (    3.48 ms per token,   287.15 tokens per second)
       eval time =    2137.25 ms /    70 tokens (   30.53 ms per token,    32.75 tokens per second)
      total time =    2684.00 ms /   227 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 8983 | stop processing: n_tokens = 51539, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9055 | processing task, is_child = 0
slot update_slots: id  3 | task 9055 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 51677
slot update_slots: id  3 | task 9055 | n_tokens = 51470, memory_seq_rm [51470, end)
slot update_slots: id  3 | task 9055 | prompt processing progress, n_tokens = 51613, batch.n_tokens = 143, progress = 0.998762
slot update_slots: id  3 | task 9055 | n_tokens = 51613, memory_seq_rm [51613, end)
slot update_slots: id  3 | task 9055 | prompt processing progress, n_tokens = 51677, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9055 | prompt done, n_tokens = 51677, batch.n_tokens = 64
slot init_sampler: id  3 | task 9055 | init sampler, took 8.56 ms, tokens: text = 51677, total = 51677
slot update_slots: id  3 | task 9055 | erasing old context checkpoint (pos_min = 48362, pos_max = 49176, size = 19.111 MiB)
slot update_slots: id  3 | task 9055 | created context checkpoint 8 of 8 (pos_min = 50589, pos_max = 51612, size = 24.012 MiB)
slot print_timing: id  3 | task 9055 | 
prompt eval time =     601.01 ms /   207 tokens (    2.90 ms per token,   344.42 tokens per second)
       eval time =   13396.44 ms /   446 tokens (   30.04 ms per token,    33.29 tokens per second)
      total time =   13997.45 ms /   653 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 9055 | stop processing: n_tokens = 52122, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9503 | processing task, is_child = 0
slot update_slots: id  3 | task 9503 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 51796
slot update_slots: id  3 | task 9503 | n_tokens = 51677, memory_seq_rm [51677, end)
slot update_slots: id  3 | task 9503 | prompt processing progress, n_tokens = 51732, batch.n_tokens = 55, progress = 0.998764
slot update_slots: id  3 | task 9503 | n_tokens = 51732, memory_seq_rm [51732, end)
slot update_slots: id  3 | task 9503 | prompt processing progress, n_tokens = 51796, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9503 | prompt done, n_tokens = 51796, batch.n_tokens = 64
slot init_sampler: id  3 | task 9503 | init sampler, took 9.70 ms, tokens: text = 51796, total = 51796
slot update_slots: id  3 | task 9503 | erasing old context checkpoint (pos_min = 48415, pos_max = 49333, size = 21.550 MiB)
slot update_slots: id  3 | task 9503 | created context checkpoint 8 of 8 (pos_min = 51098, pos_max = 51731, size = 14.867 MiB)
slot print_timing: id  3 | task 9503 | 
prompt eval time =     478.65 ms /   119 tokens (    4.02 ms per token,   248.62 tokens per second)
       eval time =    1566.29 ms /    52 tokens (   30.12 ms per token,    33.20 tokens per second)
      total time =    2044.94 ms /   171 tokens
slot      release: id  3 | task 9503 | stop processing: n_tokens = 51847, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.996 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9557 | processing task, is_child = 0
slot update_slots: id  3 | task 9557 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 52000
slot update_slots: id  3 | task 9557 | n_tokens = 51796, memory_seq_rm [51796, end)
slot update_slots: id  3 | task 9557 | prompt processing progress, n_tokens = 51936, batch.n_tokens = 140, progress = 0.998769
slot update_slots: id  3 | task 9557 | n_tokens = 51936, memory_seq_rm [51936, end)
slot update_slots: id  3 | task 9557 | prompt processing progress, n_tokens = 52000, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9557 | prompt done, n_tokens = 52000, batch.n_tokens = 64
slot init_sampler: id  3 | task 9557 | init sampler, took 7.11 ms, tokens: text = 52000, total = 52000
slot update_slots: id  3 | task 9557 | erasing old context checkpoint (pos_min = 48662, pos_max = 49685, size = 24.012 MiB)
slot update_slots: id  3 | task 9557 | created context checkpoint 8 of 8 (pos_min = 51098, pos_max = 51935, size = 19.651 MiB)
slot print_timing: id  3 | task 9557 | 
prompt eval time =     602.48 ms /   204 tokens (    2.95 ms per token,   338.60 tokens per second)
       eval time =    1800.60 ms /    60 tokens (   30.01 ms per token,    33.32 tokens per second)
      total time =    2403.08 ms /   264 tokens
slot      release: id  3 | task 9557 | stop processing: n_tokens = 52059, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.969 (> 0.100 thold), f_keep = 0.999
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9619 | processing task, is_child = 0
slot update_slots: id  3 | task 9619 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 53655
slot update_slots: id  3 | task 9619 | n_tokens = 52000, memory_seq_rm [52000, end)
slot update_slots: id  3 | task 9619 | prompt processing progress, n_tokens = 53591, batch.n_tokens = 1591, progress = 0.998807
slot update_slots: id  3 | task 9619 | n_tokens = 53591, memory_seq_rm [53591, end)
slot update_slots: id  3 | task 9619 | prompt processing progress, n_tokens = 53655, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9619 | prompt done, n_tokens = 53655, batch.n_tokens = 64
slot init_sampler: id  3 | task 9619 | init sampler, took 7.50 ms, tokens: text = 53655, total = 53655
slot update_slots: id  3 | task 9619 | erasing old context checkpoint (pos_min = 49127, pos_max = 50150, size = 24.012 MiB)
slot update_slots: id  3 | task 9619 | created context checkpoint 8 of 8 (pos_min = 52567, pos_max = 53590, size = 24.012 MiB)
slot print_timing: id  3 | task 9619 | 
prompt eval time =    3257.51 ms /  1655 tokens (    1.97 ms per token,   508.06 tokens per second)
       eval time =   57547.83 ms /  1888 tokens (   30.48 ms per token,    32.81 tokens per second)
      total time =   60805.34 ms /  3543 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 9619 | stop processing: n_tokens = 55542, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11509 | processing task, is_child = 0
slot update_slots: id  3 | task 11509 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 55586
slot update_slots: id  3 | task 11509 | n_past = 53655, slot.prompt.tokens.size() = 55542, seq_id = 3, pos_min = 54518, n_swa = 128
slot update_slots: id  3 | task 11509 | restored context checkpoint (pos_min = 52567, pos_max = 53590, size = 24.012 MiB)
slot update_slots: id  3 | task 11509 | n_tokens = 53590, memory_seq_rm [53590, end)
slot update_slots: id  3 | task 11509 | prompt processing progress, n_tokens = 55522, batch.n_tokens = 1932, progress = 0.998849
slot update_slots: id  3 | task 11509 | n_tokens = 55522, memory_seq_rm [55522, end)
slot update_slots: id  3 | task 11509 | prompt processing progress, n_tokens = 55586, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11509 | prompt done, n_tokens = 55586, batch.n_tokens = 64
slot init_sampler: id  3 | task 11509 | init sampler, took 7.85 ms, tokens: text = 55586, total = 55586
slot update_slots: id  3 | task 11509 | erasing old context checkpoint (pos_min = 49358, pos_max = 50381, size = 24.012 MiB)
slot update_slots: id  3 | task 11509 | created context checkpoint 8 of 8 (pos_min = 54498, pos_max = 55521, size = 24.012 MiB)
slot print_timing: id  3 | task 11509 | 
prompt eval time =    3970.36 ms /  1996 tokens (    1.99 ms per token,   502.72 tokens per second)
       eval time =    7304.35 ms /   241 tokens (   30.31 ms per token,    32.99 tokens per second)
      total time =   11274.72 ms /  2237 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 11509 | stop processing: n_tokens = 55826, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.998 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11752 | processing task, is_child = 0
slot update_slots: id  3 | task 11752 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 55686
slot update_slots: id  3 | task 11752 | n_tokens = 55586, memory_seq_rm [55586, end)
slot update_slots: id  3 | task 11752 | prompt processing progress, n_tokens = 55622, batch.n_tokens = 36, progress = 0.998851
slot update_slots: id  3 | task 11752 | n_tokens = 55622, memory_seq_rm [55622, end)
slot update_slots: id  3 | task 11752 | prompt processing progress, n_tokens = 55686, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11752 | prompt done, n_tokens = 55686, batch.n_tokens = 64
slot init_sampler: id  3 | task 11752 | init sampler, took 10.56 ms, tokens: text = 55686, total = 55686
slot update_slots: id  3 | task 11752 | erasing old context checkpoint (pos_min = 49902, pos_max = 50925, size = 24.012 MiB)
slot update_slots: id  3 | task 11752 | created context checkpoint 8 of 8 (pos_min = 54802, pos_max = 55621, size = 19.228 MiB)
slot print_timing: id  3 | task 11752 | 
prompt eval time =     425.25 ms /   100 tokens (    4.25 ms per token,   235.16 tokens per second)
       eval time =   66455.60 ms /  2172 tokens (   30.60 ms per token,    32.68 tokens per second)
      total time =   66880.85 ms /  2272 tokens
slot      release: id  3 | task 11752 | stop processing: n_tokens = 57857, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.006
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 57857, total state size = 1380.698 MiB
srv          load:  - looking for better prompt, base f_keep = 0.006, sim = 0.978
srv        update:  - cache state: 1 prompts, 1552.440 MiB (limits: 8192.000 MiB, 64000 tokens, 305302 est)
srv        update:    - prompt 0x5b63bb539db0:   57857 tokens, checkpoints:  8,  1552.440 MiB
srv  get_availabl: prompt cache update took 1152.80 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 13926 | processing task, is_child = 0
slot update_slots: id  3 | task 13926 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 366
slot update_slots: id  3 | task 13926 | n_past = 358, slot.prompt.tokens.size() = 57857, seq_id = 3, pos_min = 56833, n_swa = 128
slot update_slots: id  3 | task 13926 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 13926 | erased invalidated context checkpoint (pos_min = 50225, pos_max = 51248, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 13926 | erased invalidated context checkpoint (pos_min = 50470, pos_max = 51405, n_swa = 128, size = 21.949 MiB)
slot update_slots: id  3 | task 13926 | erased invalidated context checkpoint (pos_min = 50589, pos_max = 51612, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 13926 | erased invalidated context checkpoint (pos_min = 51098, pos_max = 51731, n_swa = 128, size = 14.867 MiB)
slot update_slots: id  3 | task 13926 | erased invalidated context checkpoint (pos_min = 51098, pos_max = 51935, n_swa = 128, size = 19.651 MiB)
slot update_slots: id  3 | task 13926 | erased invalidated context checkpoint (pos_min = 52567, pos_max = 53590, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 13926 | erased invalidated context checkpoint (pos_min = 54498, pos_max = 55521, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 13926 | erased invalidated context checkpoint (pos_min = 54802, pos_max = 55621, n_swa = 128, size = 19.228 MiB)
slot update_slots: id  3 | task 13926 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 13926 | prompt processing progress, n_tokens = 302, batch.n_tokens = 302, progress = 0.825137
slot update_slots: id  3 | task 13926 | n_tokens = 302, memory_seq_rm [302, end)
slot update_slots: id  3 | task 13926 | prompt processing progress, n_tokens = 366, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 13926 | prompt done, n_tokens = 366, batch.n_tokens = 64
slot init_sampler: id  3 | task 13926 | init sampler, took 0.07 ms, tokens: text = 366, total = 366
slot update_slots: id  3 | task 13926 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 301, size = 7.082 MiB)
slot print_timing: id  3 | task 13926 | 
prompt eval time =     580.95 ms /   366 tokens (    1.59 ms per token,   630.01 tokens per second)
       eval time =     766.62 ms /    33 tokens (   23.23 ms per token,    43.05 tokens per second)
      total time =    1347.57 ms /   399 tokens
slot      release: id  3 | task 13926 | stop processing: n_tokens = 398, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13961 | processing task, is_child = 0
slot update_slots: id  2 | task 13961 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 38487
slot update_slots: id  2 | task 13961 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.053213
slot update_slots: id  2 | task 13961 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.106426
slot update_slots: id  2 | task 13961 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.159638
slot update_slots: id  2 | task 13961 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.212851
slot update_slots: id  2 | task 13961 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.266064
slot update_slots: id  2 | task 13961 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.319277
slot update_slots: id  2 | task 13961 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.372489
slot update_slots: id  2 | task 13961 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.425702
slot update_slots: id  2 | task 13961 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.478915
slot update_slots: id  2 | task 13961 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.532128
slot update_slots: id  2 | task 13961 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.585340
slot update_slots: id  2 | task 13961 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.638553
slot update_slots: id  2 | task 13961 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.691766
slot update_slots: id  2 | task 13961 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.744979
slot update_slots: id  2 | task 13961 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.798192
slot update_slots: id  2 | task 13961 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.851404
slot update_slots: id  2 | task 13961 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.904617
slot update_slots: id  2 | task 13961 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 36864, batch.n_tokens = 2048, progress = 0.957830
slot update_slots: id  2 | task 13961 | n_tokens = 36864, memory_seq_rm [36864, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 38423, batch.n_tokens = 1559, progress = 0.998337
slot update_slots: id  2 | task 13961 | n_tokens = 38423, memory_seq_rm [38423, end)
slot update_slots: id  2 | task 13961 | prompt processing progress, n_tokens = 38487, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13961 | prompt done, n_tokens = 38487, batch.n_tokens = 64
slot init_sampler: id  2 | task 13961 | init sampler, took 5.40 ms, tokens: text = 38487, total = 38487
slot update_slots: id  2 | task 13961 | created context checkpoint 1 of 8 (pos_min = 37526, pos_max = 38422, size = 21.034 MiB)
slot print_timing: id  2 | task 13961 | 
prompt eval time =   48658.36 ms / 38487 tokens (    1.26 ms per token,   790.96 tokens per second)
       eval time =   22919.68 ms /   823 tokens (   27.85 ms per token,    35.91 tokens per second)
      total time =   71578.04 ms / 39310 tokens
slot      release: id  2 | task 13961 | stop processing: n_tokens = 39309, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.812 (> 0.100 thold), f_keep = 0.922
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 14804 | processing task, is_child = 0
slot update_slots: id  3 | task 14804 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 452
slot update_slots: id  3 | task 14804 | n_past = 367, slot.prompt.tokens.size() = 398, seq_id = 3, pos_min = 271, n_swa = 128
slot update_slots: id  3 | task 14804 | restored context checkpoint (pos_min = 0, pos_max = 301, size = 7.082 MiB)
slot update_slots: id  3 | task 14804 | n_tokens = 301, memory_seq_rm [301, end)
slot update_slots: id  3 | task 14804 | prompt processing progress, n_tokens = 388, batch.n_tokens = 87, progress = 0.858407
slot update_slots: id  3 | task 14804 | n_tokens = 388, memory_seq_rm [388, end)
slot update_slots: id  3 | task 14804 | prompt processing progress, n_tokens = 452, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 14804 | prompt done, n_tokens = 452, batch.n_tokens = 64
slot init_sampler: id  3 | task 14804 | init sampler, took 0.08 ms, tokens: text = 452, total = 452
slot update_slots: id  3 | task 14804 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 387, size = 9.098 MiB)
slot print_timing: id  3 | task 14804 | 
prompt eval time =     630.76 ms /   151 tokens (    4.18 ms per token,   239.39 tokens per second)
       eval time =    1578.08 ms /    58 tokens (   27.21 ms per token,    36.75 tokens per second)
      total time =    2208.84 ms /   209 tokens
slot      release: id  3 | task 14804 | stop processing: n_tokens = 509, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.284 (> 0.100 thold), f_keep = 0.279
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 39309, total state size = 933.831 MiB
srv          load:  - looking for better prompt, base f_keep = 0.279, sim = 0.284
srv        update:  - cache state: 2 prompts, 2507.306 MiB (limits: 8192.000 MiB, 64000 tokens, 317465 est)
srv        update:    - prompt 0x5b63bb539db0:   57857 tokens, checkpoints:  8,  1552.440 MiB
srv        update:    - prompt 0x5b63c2140830:   39309 tokens, checkpoints:  1,   954.865 MiB
srv  get_availabl: prompt cache update took 715.27 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 14864 | processing task, is_child = 0
slot update_slots: id  2 | task 14864 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 38536
slot update_slots: id  2 | task 14864 | n_past = 10963, slot.prompt.tokens.size() = 39309, seq_id = 2, pos_min = 38794, n_swa = 128
slot update_slots: id  2 | task 14864 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 14864 | erased invalidated context checkpoint (pos_min = 37526, pos_max = 38422, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 14864 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.053145
slot update_slots: id  2 | task 14864 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.106290
slot update_slots: id  2 | task 14864 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.159435
slot update_slots: id  2 | task 14864 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.212580
slot update_slots: id  2 | task 14864 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.265726
slot update_slots: id  2 | task 14864 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.318871
slot update_slots: id  2 | task 14864 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.372016
slot update_slots: id  2 | task 14864 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.425161
slot update_slots: id  2 | task 14864 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.478306
slot update_slots: id  2 | task 14864 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.531451
slot update_slots: id  2 | task 14864 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.584596
slot update_slots: id  2 | task 14864 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.637741
slot update_slots: id  2 | task 14864 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.690886
slot update_slots: id  2 | task 14864 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.744032
slot update_slots: id  2 | task 14864 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.797177
slot update_slots: id  2 | task 14864 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.850322
slot update_slots: id  2 | task 14864 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.903467
slot update_slots: id  2 | task 14864 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 36864, batch.n_tokens = 2048, progress = 0.956612
slot update_slots: id  2 | task 14864 | n_tokens = 36864, memory_seq_rm [36864, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 38472, batch.n_tokens = 1608, progress = 0.998339
slot update_slots: id  2 | task 14864 | n_tokens = 38472, memory_seq_rm [38472, end)
slot update_slots: id  2 | task 14864 | prompt processing progress, n_tokens = 38536, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 14864 | prompt done, n_tokens = 38536, batch.n_tokens = 64
slot init_sampler: id  2 | task 14864 | init sampler, took 5.29 ms, tokens: text = 38536, total = 38536
slot update_slots: id  2 | task 14864 | created context checkpoint 1 of 8 (pos_min = 37575, pos_max = 38471, size = 21.034 MiB)
slot print_timing: id  2 | task 14864 | 
prompt eval time =   61106.42 ms / 38536 tokens (    1.59 ms per token,   630.64 tokens per second)
       eval time =   85273.40 ms /  2963 tokens (   28.78 ms per token,    34.75 tokens per second)
      total time =  146379.83 ms / 41499 tokens
slot      release: id  2 | task 14864 | stop processing: n_tokens = 41498, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.984 (> 0.100 thold), f_keep = 0.707
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17847 | processing task, is_child = 0
slot update_slots: id  3 | task 17847 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 366
slot update_slots: id  3 | task 17847 | n_past = 360, slot.prompt.tokens.size() = 509, seq_id = 3, pos_min = 382, n_swa = 128
slot update_slots: id  3 | task 17847 | restored context checkpoint (pos_min = 0, pos_max = 387, size = 9.098 MiB)
slot update_slots: id  3 | task 17847 | n_tokens = 360, memory_seq_rm [360, end)
slot update_slots: id  3 | task 17847 | prompt processing progress, n_tokens = 366, batch.n_tokens = 6, progress = 1.000000
slot update_slots: id  3 | task 17847 | prompt done, n_tokens = 366, batch.n_tokens = 6
slot init_sampler: id  3 | task 17847 | init sampler, took 0.06 ms, tokens: text = 366, total = 366
slot print_timing: id  3 | task 17847 | 
prompt eval time =     134.63 ms /     6 tokens (   22.44 ms per token,    44.57 tokens per second)
       eval time =    1503.55 ms /    51 tokens (   29.48 ms per token,    33.92 tokens per second)
      total time =    1638.18 ms /    57 tokens
slot      release: id  3 | task 17847 | stop processing: n_tokens = 416, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.882
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17899 | processing task, is_child = 0
slot update_slots: id  3 | task 17899 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 383
slot update_slots: id  3 | task 17899 | n_tokens = 367, memory_seq_rm [367, end)
slot update_slots: id  3 | task 17899 | prompt processing progress, n_tokens = 383, batch.n_tokens = 16, progress = 1.000000
slot update_slots: id  3 | task 17899 | prompt done, n_tokens = 383, batch.n_tokens = 16
slot init_sampler: id  3 | task 17899 | init sampler, took 0.08 ms, tokens: text = 383, total = 383
slot print_timing: id  3 | task 17899 | 
prompt eval time =     148.22 ms /    16 tokens (    9.26 ms per token,   107.95 tokens per second)
       eval time =     897.54 ms /    32 tokens (   28.05 ms per token,    35.65 tokens per second)
      total time =    1045.76 ms /    48 tokens
slot      release: id  3 | task 17899 | stop processing: n_tokens = 414, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.903 (> 0.100 thold), f_keep = 0.925
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17932 | processing task, is_child = 0
slot update_slots: id  3 | task 17932 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 424
slot update_slots: id  3 | task 17932 | n_tokens = 383, memory_seq_rm [383, end)
slot update_slots: id  3 | task 17932 | prompt processing progress, n_tokens = 424, batch.n_tokens = 41, progress = 1.000000
slot update_slots: id  3 | task 17932 | prompt done, n_tokens = 424, batch.n_tokens = 41
slot init_sampler: id  3 | task 17932 | init sampler, took 0.09 ms, tokens: text = 424, total = 424
slot print_timing: id  3 | task 17932 | 
prompt eval time =     268.42 ms /    41 tokens (    6.55 ms per token,   152.75 tokens per second)
       eval time =    1441.89 ms /    50 tokens (   28.84 ms per token,    34.68 tokens per second)
      total time =    1710.31 ms /    91 tokens
slot      release: id  3 | task 17932 | stop processing: n_tokens = 473, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.893 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 17983 | processing task, is_child = 0
slot update_slots: id  3 | task 17983 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 475
slot update_slots: id  3 | task 17983 | n_tokens = 424, memory_seq_rm [424, end)
slot update_slots: id  3 | task 17983 | prompt processing progress, n_tokens = 475, batch.n_tokens = 51, progress = 1.000000
slot update_slots: id  3 | task 17983 | prompt done, n_tokens = 475, batch.n_tokens = 51
slot init_sampler: id  3 | task 17983 | init sampler, took 0.08 ms, tokens: text = 475, total = 475
slot print_timing: id  3 | task 17983 | 
prompt eval time =     253.41 ms /    51 tokens (    4.97 ms per token,   201.26 tokens per second)
       eval time =    2880.00 ms /   100 tokens (   28.80 ms per token,    34.72 tokens per second)
      total time =    3133.41 ms /   151 tokens
slot      release: id  3 | task 17983 | stop processing: n_tokens = 574, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.890 (> 0.100 thold), f_keep = 0.828
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18084 | processing task, is_child = 0
slot update_slots: id  3 | task 18084 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 534
slot update_slots: id  3 | task 18084 | n_tokens = 475, memory_seq_rm [475, end)
slot update_slots: id  3 | task 18084 | prompt processing progress, n_tokens = 534, batch.n_tokens = 59, progress = 1.000000
slot update_slots: id  3 | task 18084 | prompt done, n_tokens = 534, batch.n_tokens = 59
slot init_sampler: id  3 | task 18084 | init sampler, took 0.10 ms, tokens: text = 534, total = 534
slot update_slots: id  3 | task 18084 | created context checkpoint 3 of 8 (pos_min = 0, pos_max = 474, size = 11.139 MiB)
slot print_timing: id  3 | task 18084 | 
prompt eval time =     265.62 ms /    59 tokens (    4.50 ms per token,   222.12 tokens per second)
       eval time =     858.45 ms /    31 tokens (   27.69 ms per token,    36.11 tokens per second)
      total time =    1124.08 ms /    90 tokens
slot      release: id  3 | task 18084 | stop processing: n_tokens = 564, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.989 (> 0.100 thold), f_keep = 0.642
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 18116 | processing task, is_child = 0
slot update_slots: id  3 | task 18116 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 366
slot update_slots: id  3 | task 18116 | n_tokens = 362, memory_seq_rm [362, end)
slot update_slots: id  3 | task 18116 | prompt processing progress, n_tokens = 366, batch.n_tokens = 4, progress = 1.000000
slot update_slots: id  3 | task 18116 | prompt done, n_tokens = 366, batch.n_tokens = 4
slot init_sampler: id  3 | task 18116 | init sampler, took 0.06 ms, tokens: text = 366, total = 366
slot print_timing: id  3 | task 18116 | 
prompt eval time =      87.11 ms /     4 tokens (   21.78 ms per token,    45.92 tokens per second)
       eval time =    1031.75 ms /    37 tokens (   27.89 ms per token,    35.86 tokens per second)
      total time =    1118.86 ms /    41 tokens
slot      release: id  3 | task 18116 | stop processing: n_tokens = 402, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.265 (> 0.100 thold), f_keep = 0.245
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 41498, total state size = 983.637 MiB
srv          load:  - looking for better prompt, base f_keep = 0.245, sim = 0.265
srv        update:  - cache state: 3 prompts, 3511.976 MiB (limits: 8192.000 MiB, 64000 tokens, 323446 est)
srv        update:    - prompt 0x5b63bb539db0:   57857 tokens, checkpoints:  8,  1552.440 MiB
srv        update:    - prompt 0x5b63c2140830:   39309 tokens, checkpoints:  1,   954.865 MiB
srv        update:    - prompt 0x5b63c0f15a30:   41498 tokens, checkpoints:  1,  1004.671 MiB
srv  get_availabl: prompt cache update took 780.39 ms
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 18154 | processing task, is_child = 0
slot update_slots: id  2 | task 18154 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 38457
slot update_slots: id  2 | task 18154 | n_past = 10173, slot.prompt.tokens.size() = 41498, seq_id = 2, pos_min = 41048, n_swa = 128
slot update_slots: id  2 | task 18154 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  2 | task 18154 | erased invalidated context checkpoint (pos_min = 37575, pos_max = 38471, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  2 | task 18154 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.053254
slot update_slots: id  2 | task 18154 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.106509
slot update_slots: id  2 | task 18154 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.159763
slot update_slots: id  2 | task 18154 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 8192, batch.n_tokens = 2048, progress = 0.213017
slot update_slots: id  2 | task 18154 | n_tokens = 8192, memory_seq_rm [8192, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 10240, batch.n_tokens = 2048, progress = 0.266271
slot update_slots: id  2 | task 18154 | n_tokens = 10240, memory_seq_rm [10240, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 12288, batch.n_tokens = 2048, progress = 0.319526
slot update_slots: id  2 | task 18154 | n_tokens = 12288, memory_seq_rm [12288, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 14336, batch.n_tokens = 2048, progress = 0.372780
slot update_slots: id  2 | task 18154 | n_tokens = 14336, memory_seq_rm [14336, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 16384, batch.n_tokens = 2048, progress = 0.426034
slot update_slots: id  2 | task 18154 | n_tokens = 16384, memory_seq_rm [16384, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 18432, batch.n_tokens = 2048, progress = 0.479289
slot update_slots: id  2 | task 18154 | n_tokens = 18432, memory_seq_rm [18432, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 20480, batch.n_tokens = 2048, progress = 0.532543
slot update_slots: id  2 | task 18154 | n_tokens = 20480, memory_seq_rm [20480, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 22528, batch.n_tokens = 2048, progress = 0.585797
slot update_slots: id  2 | task 18154 | n_tokens = 22528, memory_seq_rm [22528, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 24576, batch.n_tokens = 2048, progress = 0.639051
slot update_slots: id  2 | task 18154 | n_tokens = 24576, memory_seq_rm [24576, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 26624, batch.n_tokens = 2048, progress = 0.692306
slot update_slots: id  2 | task 18154 | n_tokens = 26624, memory_seq_rm [26624, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 28672, batch.n_tokens = 2048, progress = 0.745560
slot update_slots: id  2 | task 18154 | n_tokens = 28672, memory_seq_rm [28672, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 30720, batch.n_tokens = 2048, progress = 0.798814
slot update_slots: id  2 | task 18154 | n_tokens = 30720, memory_seq_rm [30720, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 32768, batch.n_tokens = 2048, progress = 0.852069
slot update_slots: id  2 | task 18154 | n_tokens = 32768, memory_seq_rm [32768, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 34816, batch.n_tokens = 2048, progress = 0.905323
slot update_slots: id  2 | task 18154 | n_tokens = 34816, memory_seq_rm [34816, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 36864, batch.n_tokens = 2048, progress = 0.958577
slot update_slots: id  2 | task 18154 | n_tokens = 36864, memory_seq_rm [36864, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 38393, batch.n_tokens = 1529, progress = 0.998336
slot update_slots: id  2 | task 18154 | n_tokens = 38393, memory_seq_rm [38393, end)
slot update_slots: id  2 | task 18154 | prompt processing progress, n_tokens = 38457, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 18154 | prompt done, n_tokens = 38457, batch.n_tokens = 64
slot init_sampler: id  2 | task 18154 | init sampler, took 7.79 ms, tokens: text = 38457, total = 38457
slot update_slots: id  2 | task 18154 | created context checkpoint 1 of 8 (pos_min = 37496, pos_max = 38392, size = 21.034 MiB)
slot print_timing: id  2 | task 18154 | 
prompt eval time =   60803.35 ms / 38457 tokens (    1.58 ms per token,   632.48 tokens per second)
       eval time =   41279.10 ms /  1454 tokens (   28.39 ms per token,    35.22 tokens per second)
      total time =  102082.45 ms / 39911 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  2 | task 18154 | stop processing: n_tokens = 39910, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.867 (> 0.100 thold), f_keep = 0.896
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 19628 | processing task, is_child = 0
slot update_slots: id  3 | task 19628 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 415
slot update_slots: id  3 | task 19628 | n_past = 360, slot.prompt.tokens.size() = 402, seq_id = 3, pos_min = 275, n_swa = 128
slot update_slots: id  3 | task 19628 | restored context checkpoint (pos_min = 0, pos_max = 474, size = 11.139 MiB)
slot update_slots: id  3 | task 19628 | n_tokens = 360, memory_seq_rm [360, end)
slot update_slots: id  3 | task 19628 | prompt processing progress, n_tokens = 415, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 19628 | prompt done, n_tokens = 415, batch.n_tokens = 55
slot init_sampler: id  3 | task 19628 | init sampler, took 0.07 ms, tokens: text = 415, total = 415
slot print_timing: id  3 | task 19628 | 
prompt eval time =     264.38 ms /    55 tokens (    4.81 ms per token,   208.03 tokens per second)
       eval time =   45157.43 ms /  1595 tokens (   28.31 ms per token,    35.32 tokens per second)
      total time =   45421.81 ms /  1650 tokens
slot      release: id  3 | task 19628 | stop processing: n_tokens = 2009, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.308 (> 0.100 thold), f_keep = 0.207
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2009, total state size = 68.143 MiB
srv          load:  - looking for better prompt, base f_keep = 0.207, sim = 0.308
srv        update:  - cache state: 4 prompts, 3607.439 MiB (limits: 8192.000 MiB, 64000 tokens, 319449 est)
srv        update:    - prompt 0x5b63bb539db0:   57857 tokens, checkpoints:  8,  1552.440 MiB
srv        update:    - prompt 0x5b63c2140830:   39309 tokens, checkpoints:  1,   954.865 MiB
srv        update:    - prompt 0x5b63c0f15a30:   41498 tokens, checkpoints:  1,  1004.671 MiB
srv        update:    - prompt 0x5b63c388d2f0:    2009 tokens, checkpoints:  3,    95.462 MiB
srv  get_availabl: prompt cache update took 44.38 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 21224 | processing task, is_child = 0
slot update_slots: id  3 | task 21224 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1346
slot update_slots: id  3 | task 21224 | n_past = 415, slot.prompt.tokens.size() = 2009, seq_id = 3, pos_min = 1112, n_swa = 128
slot update_slots: id  3 | task 21224 | restored context checkpoint (pos_min = 0, pos_max = 474, size = 11.139 MiB)
slot update_slots: id  3 | task 21224 | n_tokens = 415, memory_seq_rm [415, end)
slot update_slots: id  3 | task 21224 | prompt processing progress, n_tokens = 1282, batch.n_tokens = 867, progress = 0.952452
slot update_slots: id  3 | task 21224 | n_tokens = 1282, memory_seq_rm [1282, end)
slot update_slots: id  3 | task 21224 | prompt processing progress, n_tokens = 1346, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 21224 | prompt done, n_tokens = 1346, batch.n_tokens = 64
slot init_sampler: id  3 | task 21224 | init sampler, took 0.27 ms, tokens: text = 1346, total = 1346
slot update_slots: id  3 | task 21224 | created context checkpoint 4 of 8 (pos_min = 385, pos_max = 1281, size = 21.034 MiB)
slot print_timing: id  3 | task 21224 | 
prompt eval time =    1907.98 ms /   931 tokens (    2.05 ms per token,   487.95 tokens per second)
       eval time =   27497.04 ms /   927 tokens (   29.66 ms per token,    33.71 tokens per second)
      total time =   29405.02 ms /  1858 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 21224 | stop processing: n_tokens = 2272, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.849 (> 0.100 thold), f_keep = 0.159
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2272, total state size = 74.310 MiB
srv          load:  - looking for better prompt, base f_keep = 0.159, sim = 0.849
srv        update:  - cache state: 5 prompts, 3730.102 MiB (limits: 8192.000 MiB, 64000 tokens, 313933 est)
srv        update:    - prompt 0x5b63bb539db0:   57857 tokens, checkpoints:  8,  1552.440 MiB
srv        update:    - prompt 0x5b63c2140830:   39309 tokens, checkpoints:  1,   954.865 MiB
srv        update:    - prompt 0x5b63c0f15a30:   41498 tokens, checkpoints:  1,  1004.671 MiB
srv        update:    - prompt 0x5b63c388d2f0:    2009 tokens, checkpoints:  3,    95.462 MiB
srv        update:    - prompt 0x5b63c225b8d0:    2272 tokens, checkpoints:  4,   122.663 MiB
srv  get_availabl: prompt cache update took 112.95 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22153 | processing task, is_child = 0
slot update_slots: id  3 | task 22153 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 425
slot update_slots: id  3 | task 22153 | n_past = 361, slot.prompt.tokens.size() = 2272, seq_id = 3, pos_min = 1375, n_swa = 128
slot update_slots: id  3 | task 22153 | restored context checkpoint (pos_min = 0, pos_max = 474, size = 11.139 MiB)
slot update_slots: id  3 | task 22153 | erased invalidated context checkpoint (pos_min = 385, pos_max = 1281, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 22153 | n_tokens = 361, memory_seq_rm [361, end)
slot update_slots: id  3 | task 22153 | prompt processing progress, n_tokens = 425, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22153 | prompt done, n_tokens = 425, batch.n_tokens = 64
slot init_sampler: id  3 | task 22153 | init sampler, took 0.08 ms, tokens: text = 425, total = 425
slot print_timing: id  3 | task 22153 | 
prompt eval time =     431.72 ms /    64 tokens (    6.75 ms per token,   148.24 tokens per second)
       eval time =    2975.99 ms /   105 tokens (   28.34 ms per token,    35.28 tokens per second)
      total time =    3407.71 ms /   169 tokens
slot      release: id  3 | task 22153 | stop processing: n_tokens = 529, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.495 (> 0.100 thold), f_keep = 0.803
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22259 | processing task, is_child = 0
slot update_slots: id  3 | task 22259 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 859
slot update_slots: id  3 | task 22259 | n_tokens = 425, memory_seq_rm [425, end)
slot update_slots: id  3 | task 22259 | prompt processing progress, n_tokens = 795, batch.n_tokens = 370, progress = 0.925495
slot update_slots: id  3 | task 22259 | n_tokens = 795, memory_seq_rm [795, end)
slot update_slots: id  3 | task 22259 | prompt processing progress, n_tokens = 859, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22259 | prompt done, n_tokens = 859, batch.n_tokens = 64
slot init_sampler: id  3 | task 22259 | init sampler, took 0.16 ms, tokens: text = 859, total = 859
slot update_slots: id  3 | task 22259 | created context checkpoint 4 of 8 (pos_min = 0, pos_max = 794, size = 18.642 MiB)
slot print_timing: id  3 | task 22259 | 
prompt eval time =     875.05 ms /   434 tokens (    2.02 ms per token,   495.97 tokens per second)
       eval time =    3554.44 ms /   120 tokens (   29.62 ms per token,    33.76 tokens per second)
      total time =    4429.49 ms /   554 tokens
slot      release: id  3 | task 22259 | stop processing: n_tokens = 978, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.913 (> 0.100 thold), f_keep = 0.878
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22381 | processing task, is_child = 0
slot update_slots: id  3 | task 22381 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 941
slot update_slots: id  3 | task 22381 | n_tokens = 859, memory_seq_rm [859, end)
slot update_slots: id  3 | task 22381 | prompt processing progress, n_tokens = 877, batch.n_tokens = 18, progress = 0.931987
slot update_slots: id  3 | task 22381 | n_tokens = 877, memory_seq_rm [877, end)
slot update_slots: id  3 | task 22381 | prompt processing progress, n_tokens = 941, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22381 | prompt done, n_tokens = 941, batch.n_tokens = 64
slot init_sampler: id  3 | task 22381 | init sampler, took 0.17 ms, tokens: text = 941, total = 941
slot update_slots: id  3 | task 22381 | created context checkpoint 5 of 8 (pos_min = 81, pos_max = 876, size = 18.666 MiB)
slot print_timing: id  3 | task 22381 | 
prompt eval time =     357.72 ms /    82 tokens (    4.36 ms per token,   229.23 tokens per second)
       eval time =    1707.22 ms /    57 tokens (   29.95 ms per token,    33.39 tokens per second)
      total time =    2064.94 ms /   139 tokens
slot      release: id  3 | task 22381 | stop processing: n_tokens = 997, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.920 (> 0.100 thold), f_keep = 0.944
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22440 | processing task, is_child = 0
slot update_slots: id  3 | task 22440 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1023
slot update_slots: id  3 | task 22440 | n_tokens = 941, memory_seq_rm [941, end)
slot update_slots: id  3 | task 22440 | prompt processing progress, n_tokens = 959, batch.n_tokens = 18, progress = 0.937439
slot update_slots: id  3 | task 22440 | n_tokens = 959, memory_seq_rm [959, end)
slot update_slots: id  3 | task 22440 | prompt processing progress, n_tokens = 1023, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22440 | prompt done, n_tokens = 1023, batch.n_tokens = 64
slot init_sampler: id  3 | task 22440 | init sampler, took 0.18 ms, tokens: text = 1023, total = 1023
slot update_slots: id  3 | task 22440 | created context checkpoint 6 of 8 (pos_min = 138, pos_max = 958, size = 19.252 MiB)
slot print_timing: id  3 | task 22440 | 
prompt eval time =     371.90 ms /    82 tokens (    4.54 ms per token,   220.49 tokens per second)
       eval time =    1743.63 ms /    57 tokens (   30.59 ms per token,    32.69 tokens per second)
      total time =    2115.53 ms /   139 tokens
slot      release: id  3 | task 22440 | stop processing: n_tokens = 1079, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.926 (> 0.100 thold), f_keep = 0.948
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22499 | processing task, is_child = 0
slot update_slots: id  3 | task 22499 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1105
slot update_slots: id  3 | task 22499 | n_tokens = 1023, memory_seq_rm [1023, end)
slot update_slots: id  3 | task 22499 | prompt processing progress, n_tokens = 1041, batch.n_tokens = 18, progress = 0.942081
slot update_slots: id  3 | task 22499 | n_tokens = 1041, memory_seq_rm [1041, end)
slot update_slots: id  3 | task 22499 | prompt processing progress, n_tokens = 1105, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22499 | prompt done, n_tokens = 1105, batch.n_tokens = 64
slot init_sampler: id  3 | task 22499 | init sampler, took 0.21 ms, tokens: text = 1105, total = 1105
slot update_slots: id  3 | task 22499 | created context checkpoint 7 of 8 (pos_min = 220, pos_max = 1040, size = 19.252 MiB)
slot print_timing: id  3 | task 22499 | 
prompt eval time =     369.22 ms /    82 tokens (    4.50 ms per token,   222.09 tokens per second)
       eval time =    3308.32 ms /   107 tokens (   30.92 ms per token,    32.34 tokens per second)
      total time =    3677.53 ms /   189 tokens
slot      release: id  3 | task 22499 | stop processing: n_tokens = 1211, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.615 (> 0.100 thold), f_keep = 0.912
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22608 | processing task, is_child = 0
slot update_slots: id  3 | task 22608 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1797
slot update_slots: id  3 | task 22608 | n_tokens = 1105, memory_seq_rm [1105, end)
slot update_slots: id  3 | task 22608 | prompt processing progress, n_tokens = 1733, batch.n_tokens = 628, progress = 0.964385
slot update_slots: id  3 | task 22608 | n_tokens = 1733, memory_seq_rm [1733, end)
slot update_slots: id  3 | task 22608 | prompt processing progress, n_tokens = 1797, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22608 | prompt done, n_tokens = 1797, batch.n_tokens = 64
slot init_sampler: id  3 | task 22608 | init sampler, took 0.30 ms, tokens: text = 1797, total = 1797
slot update_slots: id  3 | task 22608 | created context checkpoint 8 of 8 (pos_min = 859, pos_max = 1732, size = 20.495 MiB)
slot print_timing: id  3 | task 22608 | 
prompt eval time =    1536.82 ms /   692 tokens (    2.22 ms per token,   450.28 tokens per second)
       eval time =    4251.27 ms /   137 tokens (   31.03 ms per token,    32.23 tokens per second)
      total time =    5788.09 ms /   829 tokens
slot      release: id  3 | task 22608 | stop processing: n_tokens = 1933, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.955 (> 0.100 thold), f_keep = 0.930
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22747 | processing task, is_child = 0
slot update_slots: id  3 | task 22747 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1881
slot update_slots: id  3 | task 22747 | n_tokens = 1797, memory_seq_rm [1797, end)
slot update_slots: id  3 | task 22747 | prompt processing progress, n_tokens = 1817, batch.n_tokens = 20, progress = 0.965976
slot update_slots: id  3 | task 22747 | n_tokens = 1817, memory_seq_rm [1817, end)
slot update_slots: id  3 | task 22747 | prompt processing progress, n_tokens = 1881, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22747 | prompt done, n_tokens = 1881, batch.n_tokens = 64
slot init_sampler: id  3 | task 22747 | init sampler, took 0.34 ms, tokens: text = 1881, total = 1881
slot update_slots: id  3 | task 22747 | erasing old context checkpoint (pos_min = 0, pos_max = 301, size = 7.082 MiB)
slot update_slots: id  3 | task 22747 | created context checkpoint 8 of 8 (pos_min = 1036, pos_max = 1816, size = 18.314 MiB)
slot print_timing: id  3 | task 22747 | 
prompt eval time =     361.46 ms /    84 tokens (    4.30 ms per token,   232.39 tokens per second)
       eval time =    2858.03 ms /    93 tokens (   30.73 ms per token,    32.54 tokens per second)
      total time =    3219.49 ms /   177 tokens
slot      release: id  3 | task 22747 | stop processing: n_tokens = 1973, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.925 (> 0.100 thold), f_keep = 0.182
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 1973, total state size = 67.299 MiB
srv          load:  - looking for better prompt, base f_keep = 0.182, sim = 0.925
srv        update:  - cache state: 6 prompts, 3932.258 MiB (limits: 8192.000 MiB, 64000 tokens, 301904 est)
srv        update:    - prompt 0x5b63bb539db0:   57857 tokens, checkpoints:  8,  1552.440 MiB
srv        update:    - prompt 0x5b63c2140830:   39309 tokens, checkpoints:  1,   954.865 MiB
srv        update:    - prompt 0x5b63c0f15a30:   41498 tokens, checkpoints:  1,  1004.671 MiB
srv        update:    - prompt 0x5b63c388d2f0:    2009 tokens, checkpoints:  3,    95.462 MiB
srv        update:    - prompt 0x5b63c225b8d0:    2272 tokens, checkpoints:  4,   122.663 MiB
srv        update:    - prompt 0x5b63d02bf130:    1973 tokens, checkpoints:  8,   202.156 MiB
srv  get_availabl: prompt cache update took 179.01 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22842 | processing task, is_child = 0
slot update_slots: id  3 | task 22842 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 389
slot update_slots: id  3 | task 22842 | n_past = 360, slot.prompt.tokens.size() = 1973, seq_id = 3, pos_min = 1076, n_swa = 128
slot update_slots: id  3 | task 22842 | restored context checkpoint (pos_min = 220, pos_max = 1040, size = 19.252 MiB)
slot update_slots: id  3 | task 22842 | erased invalidated context checkpoint (pos_min = 859, pos_max = 1732, n_swa = 128, size = 20.495 MiB)
slot update_slots: id  3 | task 22842 | erased invalidated context checkpoint (pos_min = 1036, pos_max = 1816, n_swa = 128, size = 18.314 MiB)
slot update_slots: id  3 | task 22842 | n_tokens = 360, memory_seq_rm [360, end)
slot update_slots: id  3 | task 22842 | prompt processing progress, n_tokens = 389, batch.n_tokens = 29, progress = 1.000000
slot update_slots: id  3 | task 22842 | prompt done, n_tokens = 389, batch.n_tokens = 29
slot init_sampler: id  3 | task 22842 | init sampler, took 0.07 ms, tokens: text = 389, total = 389
slot print_timing: id  3 | task 22842 | 
prompt eval time =     411.34 ms /    29 tokens (   14.18 ms per token,    70.50 tokens per second)
       eval time =    1655.72 ms /    58 tokens (   28.55 ms per token,    35.03 tokens per second)
      total time =    2067.06 ms /    87 tokens
slot      release: id  3 | task 22842 | stop processing: n_tokens = 446, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.476 (> 0.100 thold), f_keep = 0.872
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22901 | processing task, is_child = 0
slot update_slots: id  3 | task 22901 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 817
slot update_slots: id  3 | task 22901 | n_tokens = 389, memory_seq_rm [389, end)
slot update_slots: id  3 | task 22901 | prompt processing progress, n_tokens = 753, batch.n_tokens = 364, progress = 0.921665
slot update_slots: id  3 | task 22901 | n_tokens = 753, memory_seq_rm [753, end)
slot update_slots: id  3 | task 22901 | prompt processing progress, n_tokens = 817, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22901 | prompt done, n_tokens = 817, batch.n_tokens = 64
slot init_sampler: id  3 | task 22901 | init sampler, took 0.15 ms, tokens: text = 817, total = 817
slot print_timing: id  3 | task 22901 | 
prompt eval time =     877.52 ms /   428 tokens (    2.05 ms per token,   487.74 tokens per second)
       eval time =    1878.80 ms /    62 tokens (   30.30 ms per token,    33.00 tokens per second)
      total time =    2756.33 ms /   490 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 22901 | stop processing: n_tokens = 878, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.791 (> 0.100 thold), f_keep = 0.931
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 22965 | processing task, is_child = 0
slot update_slots: id  3 | task 22965 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1033
slot update_slots: id  3 | task 22965 | n_tokens = 817, memory_seq_rm [817, end)
slot update_slots: id  3 | task 22965 | prompt processing progress, n_tokens = 969, batch.n_tokens = 152, progress = 0.938045
slot update_slots: id  3 | task 22965 | n_tokens = 969, memory_seq_rm [969, end)
slot update_slots: id  3 | task 22965 | prompt processing progress, n_tokens = 1033, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 22965 | prompt done, n_tokens = 1033, batch.n_tokens = 64
slot init_sampler: id  3 | task 22965 | init sampler, took 0.20 ms, tokens: text = 1033, total = 1033
slot print_timing: id  3 | task 22965 | 
prompt eval time =     561.29 ms /   216 tokens (    2.60 ms per token,   384.83 tokens per second)
       eval time =    2217.44 ms /    70 tokens (   31.68 ms per token,    31.57 tokens per second)
      total time =    2778.73 ms /   286 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 22965 | stop processing: n_tokens = 1102, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.362 (> 0.100 thold), f_keep = 0.937
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23037 | processing task, is_child = 0
slot update_slots: id  3 | task 23037 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 2850
slot update_slots: id  3 | task 23037 | n_tokens = 1033, memory_seq_rm [1033, end)
slot update_slots: id  3 | task 23037 | prompt processing progress, n_tokens = 2786, batch.n_tokens = 1753, progress = 0.977544
slot update_slots: id  3 | task 23037 | n_tokens = 2786, memory_seq_rm [2786, end)
slot update_slots: id  3 | task 23037 | prompt processing progress, n_tokens = 2850, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23037 | prompt done, n_tokens = 2850, batch.n_tokens = 64
slot init_sampler: id  3 | task 23037 | init sampler, took 0.53 ms, tokens: text = 2850, total = 2850
slot update_slots: id  3 | task 23037 | created context checkpoint 7 of 8 (pos_min = 1889, pos_max = 2785, size = 21.034 MiB)
slot print_timing: id  3 | task 23037 | 
prompt eval time =    3432.59 ms /  1817 tokens (    1.89 ms per token,   529.34 tokens per second)
       eval time =    3264.58 ms /   104 tokens (   31.39 ms per token,    31.86 tokens per second)
      total time =    6697.17 ms /  1921 tokens
slot      release: id  3 | task 23037 | stop processing: n_tokens = 2953, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.751 (> 0.100 thold), f_keep = 0.965
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23143 | processing task, is_child = 0
slot update_slots: id  3 | task 23143 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 3797
slot update_slots: id  3 | task 23143 | n_tokens = 2850, memory_seq_rm [2850, end)
slot update_slots: id  3 | task 23143 | prompt processing progress, n_tokens = 3733, batch.n_tokens = 883, progress = 0.983145
slot update_slots: id  3 | task 23143 | n_tokens = 3733, memory_seq_rm [3733, end)
slot update_slots: id  3 | task 23143 | prompt processing progress, n_tokens = 3797, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23143 | prompt done, n_tokens = 3797, batch.n_tokens = 64
slot init_sampler: id  3 | task 23143 | init sampler, took 1.00 ms, tokens: text = 3797, total = 3797
slot update_slots: id  3 | task 23143 | created context checkpoint 8 of 8 (pos_min = 2836, pos_max = 3732, size = 21.034 MiB)
slot print_timing: id  3 | task 23143 | 
prompt eval time =    1860.91 ms /   947 tokens (    1.97 ms per token,   508.89 tokens per second)
       eval time =    2568.05 ms /    83 tokens (   30.94 ms per token,    32.32 tokens per second)
      total time =    4428.97 ms /  1030 tokens
slot      release: id  3 | task 23143 | stop processing: n_tokens = 3879, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.812 (> 0.100 thold), f_keep = 0.979
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23228 | processing task, is_child = 0
slot update_slots: id  3 | task 23228 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4675
slot update_slots: id  3 | task 23228 | n_tokens = 3797, memory_seq_rm [3797, end)
slot update_slots: id  3 | task 23228 | prompt processing progress, n_tokens = 4611, batch.n_tokens = 814, progress = 0.986310
slot update_slots: id  3 | task 23228 | n_tokens = 4611, memory_seq_rm [4611, end)
slot update_slots: id  3 | task 23228 | prompt processing progress, n_tokens = 4675, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23228 | prompt done, n_tokens = 4675, batch.n_tokens = 64
slot init_sampler: id  3 | task 23228 | init sampler, took 1.20 ms, tokens: text = 4675, total = 4675
slot update_slots: id  3 | task 23228 | erasing old context checkpoint (pos_min = 0, pos_max = 387, size = 9.098 MiB)
slot update_slots: id  3 | task 23228 | created context checkpoint 8 of 8 (pos_min = 3767, pos_max = 4610, size = 19.791 MiB)
slot print_timing: id  3 | task 23228 | 
prompt eval time =    1794.43 ms /   878 tokens (    2.04 ms per token,   489.29 tokens per second)
       eval time =    6130.01 ms /   200 tokens (   30.65 ms per token,    32.63 tokens per second)
      total time =    7924.44 ms /  1078 tokens
slot      release: id  3 | task 23228 | stop processing: n_tokens = 4874, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.959
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23430 | processing task, is_child = 0
slot update_slots: id  3 | task 23430 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 4797
slot update_slots: id  3 | task 23430 | n_tokens = 4675, memory_seq_rm [4675, end)
slot update_slots: id  3 | task 23430 | prompt processing progress, n_tokens = 4733, batch.n_tokens = 58, progress = 0.986658
slot update_slots: id  3 | task 23430 | n_tokens = 4733, memory_seq_rm [4733, end)
slot update_slots: id  3 | task 23430 | prompt processing progress, n_tokens = 4797, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23430 | prompt done, n_tokens = 4797, batch.n_tokens = 64
slot init_sampler: id  3 | task 23430 | init sampler, took 0.94 ms, tokens: text = 4797, total = 4797
slot update_slots: id  3 | task 23430 | erasing old context checkpoint (pos_min = 0, pos_max = 474, size = 11.139 MiB)
slot update_slots: id  3 | task 23430 | created context checkpoint 8 of 8 (pos_min = 3977, pos_max = 4732, size = 17.728 MiB)
slot print_timing: id  3 | task 23430 | 
prompt eval time =     454.02 ms /   122 tokens (    3.72 ms per token,   268.71 tokens per second)
       eval time =    9635.00 ms /   317 tokens (   30.39 ms per token,    32.90 tokens per second)
      total time =   10089.01 ms /   439 tokens
slot      release: id  3 | task 23430 | stop processing: n_tokens = 5113, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.762 (> 0.100 thold), f_keep = 0.938
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 23749 | processing task, is_child = 0
slot update_slots: id  3 | task 23749 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 6294
slot update_slots: id  3 | task 23749 | n_tokens = 4797, memory_seq_rm [4797, end)
slot update_slots: id  3 | task 23749 | prompt processing progress, n_tokens = 6230, batch.n_tokens = 1433, progress = 0.989832
slot update_slots: id  3 | task 23749 | n_tokens = 6230, memory_seq_rm [6230, end)
slot update_slots: id  3 | task 23749 | prompt processing progress, n_tokens = 6294, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 23749 | prompt done, n_tokens = 6294, batch.n_tokens = 64
slot init_sampler: id  3 | task 23749 | init sampler, took 1.21 ms, tokens: text = 6294, total = 6294
slot update_slots: id  3 | task 23749 | erasing old context checkpoint (pos_min = 0, pos_max = 794, size = 18.642 MiB)
slot update_slots: id  3 | task 23749 | created context checkpoint 8 of 8 (pos_min = 5333, pos_max = 6229, size = 21.034 MiB)
slot print_timing: id  3 | task 23749 | 
prompt eval time =    2686.64 ms /  1497 tokens (    1.79 ms per token,   557.20 tokens per second)
       eval time =   39415.58 ms /  1335 tokens (   29.52 ms per token,    33.87 tokens per second)
      total time =   42102.22 ms /  2832 tokens
slot      release: id  3 | task 23749 | stop processing: n_tokens = 7628, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 25086 | processing task, is_child = 0
slot update_slots: id  1 | task 25086 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7135
slot update_slots: id  1 | task 25086 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 25086 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.287036
slot update_slots: id  1 | task 25086 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  1 | task 25086 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.574071
slot update_slots: id  1 | task 25086 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  1 | task 25086 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.861107
slot update_slots: id  1 | task 25086 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  1 | task 25086 | prompt processing progress, n_tokens = 7071, batch.n_tokens = 927, progress = 0.991030
slot update_slots: id  1 | task 25086 | n_tokens = 7071, memory_seq_rm [7071, end)
slot update_slots: id  1 | task 25086 | prompt processing progress, n_tokens = 7135, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 25086 | prompt done, n_tokens = 7135, batch.n_tokens = 64
slot init_sampler: id  1 | task 25086 | init sampler, took 1.62 ms, tokens: text = 7135, total = 7135
slot update_slots: id  1 | task 25086 | created context checkpoint 1 of 8 (pos_min = 6301, pos_max = 7070, size = 18.056 MiB)
slot print_timing: id  1 | task 25086 | 
prompt eval time =   12958.53 ms /  7135 tokens (    1.82 ms per token,   550.60 tokens per second)
       eval time =   39188.15 ms /  1245 tokens (   31.48 ms per token,    31.77 tokens per second)
      total time =   52146.68 ms /  8380 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 25086 | stop processing: n_tokens = 8379, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.896 (> 0.100 thold), f_keep = 0.852
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 26336 | processing task, is_child = 0
slot update_slots: id  1 | task 26336 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7966
slot update_slots: id  1 | task 26336 | n_past = 7135, slot.prompt.tokens.size() = 8379, seq_id = 1, pos_min = 7609, n_swa = 128
slot update_slots: id  1 | task 26336 | restored context checkpoint (pos_min = 6301, pos_max = 7070, size = 18.056 MiB)
slot update_slots: id  1 | task 26336 | n_tokens = 7070, memory_seq_rm [7070, end)
slot update_slots: id  1 | task 26336 | prompt processing progress, n_tokens = 7902, batch.n_tokens = 832, progress = 0.991966
slot update_slots: id  1 | task 26336 | n_tokens = 7902, memory_seq_rm [7902, end)
slot update_slots: id  1 | task 26336 | prompt processing progress, n_tokens = 7966, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 26336 | prompt done, n_tokens = 7966, batch.n_tokens = 64
slot init_sampler: id  1 | task 26336 | init sampler, took 1.53 ms, tokens: text = 7966, total = 7966
slot update_slots: id  1 | task 26336 | created context checkpoint 2 of 8 (pos_min = 7132, pos_max = 7901, size = 18.056 MiB)
slot print_timing: id  1 | task 26336 | 
prompt eval time =    2009.84 ms /   896 tokens (    2.24 ms per token,   445.81 tokens per second)
       eval time =   27230.74 ms /   886 tokens (   30.73 ms per token,    32.54 tokens per second)
      total time =   29240.58 ms /  1782 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 26336 | stop processing: n_tokens = 8851, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.897 (> 0.100 thold), f_keep = 0.806
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27224 | processing task, is_child = 0
slot update_slots: id  1 | task 27224 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 7951
slot update_slots: id  1 | task 27224 | n_past = 7135, slot.prompt.tokens.size() = 8851, seq_id = 1, pos_min = 8081, n_swa = 128
slot update_slots: id  1 | task 27224 | restored context checkpoint (pos_min = 6301, pos_max = 7070, size = 18.056 MiB)
slot update_slots: id  1 | task 27224 | erased invalidated context checkpoint (pos_min = 7132, pos_max = 7901, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 27224 | n_tokens = 7070, memory_seq_rm [7070, end)
slot update_slots: id  1 | task 27224 | prompt processing progress, n_tokens = 7887, batch.n_tokens = 817, progress = 0.991951
slot update_slots: id  1 | task 27224 | n_tokens = 7887, memory_seq_rm [7887, end)
slot update_slots: id  1 | task 27224 | prompt processing progress, n_tokens = 7951, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27224 | prompt done, n_tokens = 7951, batch.n_tokens = 64
slot init_sampler: id  1 | task 27224 | init sampler, took 1.53 ms, tokens: text = 7951, total = 7951
slot update_slots: id  1 | task 27224 | created context checkpoint 2 of 8 (pos_min = 7117, pos_max = 7886, size = 18.056 MiB)
slot print_timing: id  1 | task 27224 | 
prompt eval time =    2077.17 ms /   881 tokens (    2.36 ms per token,   424.13 tokens per second)
       eval time =    4037.75 ms /   134 tokens (   30.13 ms per token,    33.19 tokens per second)
      total time =    6114.92 ms /  1015 tokens
slot      release: id  1 | task 27224 | stop processing: n_tokens = 8084, truncated = 0
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.991 (> 0.100 thold), f_keep = 0.984
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27360 | processing task, is_child = 0
slot update_slots: id  1 | task 27360 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8021
slot update_slots: id  1 | task 27360 | n_tokens = 7951, memory_seq_rm [7951, end)
slot update_slots: id  1 | task 27360 | prompt processing progress, n_tokens = 7957, batch.n_tokens = 6, progress = 0.992021
slot update_slots: id  1 | task 27360 | n_tokens = 7957, memory_seq_rm [7957, end)
slot update_slots: id  1 | task 27360 | prompt processing progress, n_tokens = 8021, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27360 | prompt done, n_tokens = 8021, batch.n_tokens = 64
slot init_sampler: id  1 | task 27360 | init sampler, took 1.52 ms, tokens: text = 8021, total = 8021
slot update_slots: id  1 | task 27360 | created context checkpoint 3 of 8 (pos_min = 7314, pos_max = 7956, size = 15.078 MiB)
slot print_timing: id  1 | task 27360 | 
prompt eval time =     324.82 ms /    70 tokens (    4.64 ms per token,   215.50 tokens per second)
       eval time =    1391.76 ms /    44 tokens (   31.63 ms per token,    31.61 tokens per second)
      total time =    1716.58 ms /   114 tokens
slot      release: id  1 | task 27360 | stop processing: n_tokens = 8064, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.922 (> 0.100 thold), f_keep = 0.995
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27406 | processing task, is_child = 0
slot update_slots: id  1 | task 27406 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 8695
slot update_slots: id  1 | task 27406 | n_tokens = 8021, memory_seq_rm [8021, end)
slot update_slots: id  1 | task 27406 | prompt processing progress, n_tokens = 8631, batch.n_tokens = 610, progress = 0.992639
slot update_slots: id  1 | task 27406 | n_tokens = 8631, memory_seq_rm [8631, end)
slot update_slots: id  1 | task 27406 | prompt processing progress, n_tokens = 8695, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27406 | prompt done, n_tokens = 8695, batch.n_tokens = 64
slot init_sampler: id  1 | task 27406 | init sampler, took 3.65 ms, tokens: text = 8695, total = 8695
slot update_slots: id  1 | task 27406 | created context checkpoint 4 of 8 (pos_min = 7861, pos_max = 8630, size = 18.056 MiB)
slot print_timing: id  1 | task 27406 | 
prompt eval time =    1620.51 ms /   674 tokens (    2.40 ms per token,   415.92 tokens per second)
       eval time =    2961.64 ms /    94 tokens (   31.51 ms per token,    31.74 tokens per second)
      total time =    4582.15 ms /   768 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 27406 | stop processing: n_tokens = 8788, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.945 (> 0.100 thold), f_keep = 0.041
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 8788, total state size = 224.126 MiB
srv          load:  - looking for better prompt, base f_keep = 0.041, sim = 0.945
srv        update:  - cache state: 7 prompts, 4225.629 MiB (limits: 8192.000 MiB, 64000 tokens, 297981 est)
srv        update:    - prompt 0x5b63bb539db0:   57857 tokens, checkpoints:  8,  1552.440 MiB
srv        update:    - prompt 0x5b63c2140830:   39309 tokens, checkpoints:  1,   954.865 MiB
srv        update:    - prompt 0x5b63c0f15a30:   41498 tokens, checkpoints:  1,  1004.671 MiB
srv        update:    - prompt 0x5b63c388d2f0:    2009 tokens, checkpoints:  3,    95.462 MiB
srv        update:    - prompt 0x5b63c225b8d0:    2272 tokens, checkpoints:  4,   122.663 MiB
srv        update:    - prompt 0x5b63d02bf130:    1973 tokens, checkpoints:  8,   202.156 MiB
srv        update:    - prompt 0x5b63c307be00:    8788 tokens, checkpoints:  4,   293.371 MiB
srv  get_availabl: prompt cache update took 298.39 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27502 | processing task, is_child = 0
slot update_slots: id  1 | task 27502 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 381
slot update_slots: id  1 | task 27502 | n_past = 360, slot.prompt.tokens.size() = 8788, seq_id = 1, pos_min = 8018, n_swa = 128
slot update_slots: id  1 | task 27502 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  1 | task 27502 | erased invalidated context checkpoint (pos_min = 6301, pos_max = 7070, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 27502 | erased invalidated context checkpoint (pos_min = 7117, pos_max = 7886, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 27502 | erased invalidated context checkpoint (pos_min = 7314, pos_max = 7956, n_swa = 128, size = 15.078 MiB)
slot update_slots: id  1 | task 27502 | erased invalidated context checkpoint (pos_min = 7861, pos_max = 8630, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 27502 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  1 | task 27502 | prompt processing progress, n_tokens = 317, batch.n_tokens = 317, progress = 0.832021
slot update_slots: id  1 | task 27502 | n_tokens = 317, memory_seq_rm [317, end)
slot update_slots: id  1 | task 27502 | prompt processing progress, n_tokens = 381, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27502 | prompt done, n_tokens = 381, batch.n_tokens = 64
slot init_sampler: id  1 | task 27502 | init sampler, took 0.08 ms, tokens: text = 381, total = 381
slot update_slots: id  1 | task 27502 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 316, size = 7.434 MiB)
slot print_timing: id  1 | task 27502 | 
prompt eval time =     932.16 ms /   381 tokens (    2.45 ms per token,   408.73 tokens per second)
       eval time =    1890.66 ms /    61 tokens (   30.99 ms per token,    32.26 tokens per second)
      total time =    2822.82 ms /   442 tokens
slot      release: id  1 | task 27502 | stop processing: n_tokens = 441, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.460 (> 0.100 thold), f_keep = 0.864
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27565 | processing task, is_child = 0
slot update_slots: id  1 | task 27565 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 828
slot update_slots: id  1 | task 27565 | n_tokens = 381, memory_seq_rm [381, end)
slot update_slots: id  1 | task 27565 | prompt processing progress, n_tokens = 764, batch.n_tokens = 383, progress = 0.922705
slot update_slots: id  1 | task 27565 | n_tokens = 764, memory_seq_rm [764, end)
slot update_slots: id  1 | task 27565 | prompt processing progress, n_tokens = 828, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 27565 | prompt done, n_tokens = 828, batch.n_tokens = 64
slot init_sampler: id  1 | task 27565 | init sampler, took 0.77 ms, tokens: text = 828, total = 828
slot update_slots: id  1 | task 27565 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 763, size = 17.915 MiB)
slot print_timing: id  1 | task 27565 | 
prompt eval time =    1020.86 ms /   447 tokens (    2.28 ms per token,   437.87 tokens per second)
       eval time =    2307.66 ms /    76 tokens (   30.36 ms per token,    32.93 tokens per second)
      total time =    3328.52 ms /   523 tokens
slot      release: id  1 | task 27565 | stop processing: n_tokens = 903, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.960 (> 0.100 thold), f_keep = 0.399
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 903, total state size = 39.231 MiB
srv          load:  - looking for better prompt, base f_keep = 0.399, sim = 0.960
srv        update:  - cache state: 8 prompts, 4290.209 MiB (limits: 8192.000 MiB, 64000 tokens, 295220 est)
srv        update:    - prompt 0x5b63bb539db0:   57857 tokens, checkpoints:  8,  1552.440 MiB
srv        update:    - prompt 0x5b63c2140830:   39309 tokens, checkpoints:  1,   954.865 MiB
srv        update:    - prompt 0x5b63c0f15a30:   41498 tokens, checkpoints:  1,  1004.671 MiB
srv        update:    - prompt 0x5b63c388d2f0:    2009 tokens, checkpoints:  3,    95.462 MiB
srv        update:    - prompt 0x5b63c225b8d0:    2272 tokens, checkpoints:  4,   122.663 MiB
srv        update:    - prompt 0x5b63d02bf130:    1973 tokens, checkpoints:  8,   202.156 MiB
srv        update:    - prompt 0x5b63c307be00:    8788 tokens, checkpoints:  4,   293.371 MiB
srv        update:    - prompt 0x5b63c754e760:     903 tokens, checkpoints:  2,    64.580 MiB
srv  get_availabl: prompt cache update took 36.49 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 27643 | processing task, is_child = 0
slot update_slots: id  1 | task 27643 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 375
slot update_slots: id  1 | task 27643 | n_tokens = 360, memory_seq_rm [360, end)
slot update_slots: id  1 | task 27643 | prompt processing progress, n_tokens = 375, batch.n_tokens = 15, progress = 1.000000
slot update_slots: id  1 | task 27643 | prompt done, n_tokens = 375, batch.n_tokens = 15
slot init_sampler: id  1 | task 27643 | init sampler, took 0.07 ms, tokens: text = 375, total = 375
slot print_timing: id  1 | task 27643 | 
prompt eval time =     157.39 ms /    15 tokens (   10.49 ms per token,    95.30 tokens per second)
       eval time =   10803.22 ms /   356 tokens (   30.35 ms per token,    32.95 tokens per second)
      total time =   10960.61 ms /   371 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 27643 | stop processing: n_tokens = 730, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.596 (> 0.100 thold), f_keep = 0.515
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28000 | processing task, is_child = 0
slot update_slots: id  1 | task 28000 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 631
slot update_slots: id  1 | task 28000 | n_past = 376, slot.prompt.tokens.size() = 730, seq_id = 1, pos_min = 360, n_swa = 128
slot update_slots: id  1 | task 28000 | restored context checkpoint (pos_min = 0, pos_max = 763, size = 17.915 MiB)
slot update_slots: id  1 | task 28000 | n_tokens = 376, memory_seq_rm [376, end)
slot update_slots: id  1 | task 28000 | prompt processing progress, n_tokens = 567, batch.n_tokens = 191, progress = 0.898574
slot update_slots: id  1 | task 28000 | n_tokens = 567, memory_seq_rm [567, end)
slot update_slots: id  1 | task 28000 | prompt processing progress, n_tokens = 631, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28000 | prompt done, n_tokens = 631, batch.n_tokens = 64
slot init_sampler: id  1 | task 28000 | init sampler, took 0.12 ms, tokens: text = 631, total = 631
slot print_timing: id  1 | task 28000 | 
prompt eval time =     954.91 ms /   255 tokens (    3.74 ms per token,   267.04 tokens per second)
       eval time =    6774.12 ms /   225 tokens (   30.11 ms per token,    33.21 tokens per second)
      total time =    7729.03 ms /   480 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 28000 | stop processing: n_tokens = 855, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.739
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28227 | processing task, is_child = 0
slot update_slots: id  1 | task 28227 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 648
slot update_slots: id  1 | task 28227 | n_tokens = 632, memory_seq_rm [632, end)
slot update_slots: id  1 | task 28227 | prompt processing progress, n_tokens = 648, batch.n_tokens = 16, progress = 1.000000
slot update_slots: id  1 | task 28227 | prompt done, n_tokens = 648, batch.n_tokens = 16
slot init_sampler: id  1 | task 28227 | init sampler, took 0.14 ms, tokens: text = 648, total = 648
slot print_timing: id  1 | task 28227 | 
prompt eval time =     154.94 ms /    16 tokens (    9.68 ms per token,   103.27 tokens per second)
       eval time =    1903.99 ms /    65 tokens (   29.29 ms per token,    34.14 tokens per second)
      total time =    2058.93 ms /    81 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  1 | task 28227 | stop processing: n_tokens = 712, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.593 (> 0.100 thold), f_keep = 0.910
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28293 | processing task, is_child = 0
slot update_slots: id  1 | task 28293 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1092
slot update_slots: id  1 | task 28293 | n_tokens = 648, memory_seq_rm [648, end)
slot update_slots: id  1 | task 28293 | prompt processing progress, n_tokens = 1028, batch.n_tokens = 380, progress = 0.941392
slot update_slots: id  1 | task 28293 | n_tokens = 1028, memory_seq_rm [1028, end)
slot update_slots: id  1 | task 28293 | prompt processing progress, n_tokens = 1092, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28293 | prompt done, n_tokens = 1092, batch.n_tokens = 64
slot init_sampler: id  1 | task 28293 | init sampler, took 0.28 ms, tokens: text = 1092, total = 1092
slot update_slots: id  1 | task 28293 | created context checkpoint 3 of 8 (pos_min = 396, pos_max = 1027, size = 14.820 MiB)
slot print_timing: id  1 | task 28293 | 
prompt eval time =     981.35 ms /   444 tokens (    2.21 ms per token,   452.44 tokens per second)
       eval time =    1710.64 ms /    56 tokens (   30.55 ms per token,    32.74 tokens per second)
      total time =    2691.99 ms /   500 tokens
slot      release: id  1 | task 28293 | stop processing: n_tokens = 1147, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.566 (> 0.100 thold), f_keep = 0.952
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28351 | processing task, is_child = 0
slot update_slots: id  1 | task 28351 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1930
slot update_slots: id  1 | task 28351 | n_tokens = 1092, memory_seq_rm [1092, end)
slot update_slots: id  1 | task 28351 | prompt processing progress, n_tokens = 1866, batch.n_tokens = 774, progress = 0.966839
slot update_slots: id  1 | task 28351 | n_tokens = 1866, memory_seq_rm [1866, end)
slot update_slots: id  1 | task 28351 | prompt processing progress, n_tokens = 1930, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28351 | prompt done, n_tokens = 1930, batch.n_tokens = 64
slot init_sampler: id  1 | task 28351 | init sampler, took 0.35 ms, tokens: text = 1930, total = 1930
slot update_slots: id  1 | task 28351 | created context checkpoint 4 of 8 (pos_min = 1096, pos_max = 1865, size = 18.056 MiB)
slot print_timing: id  1 | task 28351 | 
prompt eval time =    1756.26 ms /   838 tokens (    2.10 ms per token,   477.15 tokens per second)
       eval time =    7492.90 ms /   235 tokens (   31.88 ms per token,    31.36 tokens per second)
      total time =    9249.16 ms /  1073 tokens
slot      release: id  1 | task 28351 | stop processing: n_tokens = 2164, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.978 (> 0.100 thold), f_keep = 0.168
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2164, total state size = 68.800 MiB
srv          load:  - looking for better prompt, base f_keep = 0.168, sim = 0.978
srv        update:  - cache state: 9 prompts, 4417.234 MiB (limits: 8192.000 MiB, 64000 tokens, 290744 est)
srv        update:    - prompt 0x5b63bb539db0:   57857 tokens, checkpoints:  8,  1552.440 MiB
srv        update:    - prompt 0x5b63c2140830:   39309 tokens, checkpoints:  1,   954.865 MiB
srv        update:    - prompt 0x5b63c0f15a30:   41498 tokens, checkpoints:  1,  1004.671 MiB
srv        update:    - prompt 0x5b63c388d2f0:    2009 tokens, checkpoints:  3,    95.462 MiB
srv        update:    - prompt 0x5b63c225b8d0:    2272 tokens, checkpoints:  4,   122.663 MiB
srv        update:    - prompt 0x5b63d02bf130:    1973 tokens, checkpoints:  8,   202.156 MiB
srv        update:    - prompt 0x5b63c307be00:    8788 tokens, checkpoints:  4,   293.371 MiB
srv        update:    - prompt 0x5b63c754e760:     903 tokens, checkpoints:  2,    64.580 MiB
srv        update:    - prompt 0x5b63c2011e50:    2164 tokens, checkpoints:  4,   127.025 MiB
srv  get_availabl: prompt cache update took 309.69 ms
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28588 | processing task, is_child = 0
slot update_slots: id  1 | task 28588 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 371
slot update_slots: id  1 | task 28588 | n_past = 363, slot.prompt.tokens.size() = 2164, seq_id = 1, pos_min = 1394, n_swa = 128
slot update_slots: id  1 | task 28588 | restored context checkpoint (pos_min = 0, pos_max = 763, size = 17.915 MiB)
slot update_slots: id  1 | task 28588 | erased invalidated context checkpoint (pos_min = 396, pos_max = 1027, n_swa = 128, size = 14.820 MiB)
slot update_slots: id  1 | task 28588 | erased invalidated context checkpoint (pos_min = 1096, pos_max = 1865, n_swa = 128, size = 18.056 MiB)
slot update_slots: id  1 | task 28588 | n_tokens = 363, memory_seq_rm [363, end)
slot update_slots: id  1 | task 28588 | prompt processing progress, n_tokens = 371, batch.n_tokens = 8, progress = 1.000000
slot update_slots: id  1 | task 28588 | prompt done, n_tokens = 371, batch.n_tokens = 8
slot init_sampler: id  1 | task 28588 | init sampler, took 0.07 ms, tokens: text = 371, total = 371
slot print_timing: id  1 | task 28588 | 
prompt eval time =     623.55 ms /     8 tokens (   77.94 ms per token,    12.83 tokens per second)
       eval time =    2047.95 ms /    65 tokens (   31.51 ms per token,    31.74 tokens per second)
      total time =    2671.49 ms /    73 tokens
slot      release: id  1 | task 28588 | stop processing: n_tokens = 435, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.364 (> 0.100 thold), f_keep = 0.853
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28654 | processing task, is_child = 0
slot update_slots: id  1 | task 28654 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1018
slot update_slots: id  1 | task 28654 | n_tokens = 371, memory_seq_rm [371, end)
slot update_slots: id  1 | task 28654 | prompt processing progress, n_tokens = 954, batch.n_tokens = 583, progress = 0.937132
slot update_slots: id  1 | task 28654 | n_tokens = 954, memory_seq_rm [954, end)
slot update_slots: id  1 | task 28654 | prompt processing progress, n_tokens = 1018, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28654 | prompt done, n_tokens = 1018, batch.n_tokens = 64
slot init_sampler: id  1 | task 28654 | init sampler, took 0.20 ms, tokens: text = 1018, total = 1018
slot update_slots: id  1 | task 28654 | created context checkpoint 3 of 8 (pos_min = 184, pos_max = 953, size = 18.056 MiB)
slot print_timing: id  1 | task 28654 | 
prompt eval time =    1530.45 ms /   647 tokens (    2.37 ms per token,   422.75 tokens per second)
       eval time =    2173.62 ms /    71 tokens (   30.61 ms per token,    32.66 tokens per second)
      total time =    3704.07 ms /   718 tokens
slot      release: id  1 | task 28654 | stop processing: n_tokens = 1088, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  1 | task -1 | selected slot by LCP similarity, sim_best = 0.639 (> 0.100 thold), f_keep = 0.936
slot launch_slot_: id  1 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  1 | task 28727 | processing task, is_child = 0
slot update_slots: id  1 | task 28727 | new prompt, n_ctx_slot = 64000, n_keep = 0, task.n_tokens = 1593
slot update_slots: id  1 | task 28727 | n_tokens = 1018, memory_seq_rm [1018, end)
slot update_slots: id  1 | task 28727 | prompt processing progress, n_tokens = 1529, batch.n_tokens = 511, progress = 0.959824
slot update_slots: id  1 | task 28727 | n_tokens = 1529, memory_seq_rm [1529, end)
slot update_slots: id  1 | task 28727 | prompt processing progress, n_tokens = 1593, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  1 | task 28727 | prompt done, n_tokens = 1593, batch.n_tokens = 64
slot init_sampler: id  1 | task 28727 | init sampler, took 0.40 ms, tokens: text = 1593, total = 1593
slot update_slots: id  1 | task 28727 | created context checkpoint 4 of 8 (pos_min = 759, pos_max = 1528, size = 18.056 MiB)
slot print_timing: id  1 | task 28727 | 
prompt eval time =    1214.11 ms /   575 tokens (    2.11 ms per token,   473.60 tokens per second)
       eval time =   16041.14 ms /   502 tokens (   31.95 ms per token,    31.29 tokens per second)
      total time =   17255.25 ms /  1077 tokens
slot      release: id  1 | task 28727 | stop processing: n_tokens = 2094, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
