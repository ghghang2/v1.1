ggml_cuda_init: found 1 CUDA devices:
  Device 0: Tesla T4, compute capability 7.5, VMM: yes
common_download_file_single_online: no previous model file found /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_preset.ini
common_download_file_single_online: HEAD invalid http status code received: 404
no remote preset found, skipping
common_download_file_single_online: using cached file: /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf
main: n_parallel is set to auto, using n_parallel = 4 and kv_unified = true
build: 7772 (287a33017) with GNU 11.4.0 for Linux x86_64
system info: n_threads = 1, n_threads_batch = 1, total_threads = 2

system_info: n_threads = 1 (n_threads_batch = 1) / 2 | CUDA : ARCHS = 750 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | 

Running without SSL
init: using 6 threads for HTTP server
start: binding port with default address family
main: loading model
srv    load_model: loading model '/root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf'
common_init_result: fitting params to device memory, for bugs during this step try to reproduce them with -fit off, or provide --verbose logs if the bug only occurs with -fit on
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: projected to use 15546 MiB of device memory vs. 14807 MiB of free device memory
llama_params_fit_impl: cannot meet free memory target of 1024 MiB, need to reduce device memory by 1763 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
llama_params_fit_impl: context size reduced from 131072 to 56064 -> need 1767 MiB less memory in total
llama_params_fit_impl: entire model can be fit by reducing context
llama_params_fit: successfully fit params to free device memory
llama_params_fit: fitting params to free memory took 1.96 seconds
llama_model_load_from_file_impl: using device CUDA0 (Tesla T4) (0000:00:04.0) - 14807 MiB free
llama_model_loader: direct I/O is enabled, disabling mmap
llama_model_loader: loaded meta data with 37 key-value pairs and 459 tensors from /root/.cache/llama.cpp/unsloth_gpt-oss-20b-GGUF_gpt-oss-20b-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gpt-oss
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Gpt-Oss-20B
llama_model_loader: - kv   3:                           general.basename str              = Gpt-Oss-20B
llama_model_loader: - kv   4:                       general.quantized_by str              = Unsloth
llama_model_loader: - kv   5:                         general.size_label str              = 20B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                           general.repo_url str              = https://huggingface.co/unsloth
llama_model_loader: - kv   8:                               general.tags arr[str,2]       = ["vllm", "text-generation"]
llama_model_loader: - kv   9:                        gpt-oss.block_count u32              = 24
llama_model_loader: - kv  10:                     gpt-oss.context_length u32              = 131072
llama_model_loader: - kv  11:                   gpt-oss.embedding_length u32              = 2880
llama_model_loader: - kv  12:                gpt-oss.feed_forward_length u32              = 2880
llama_model_loader: - kv  13:               gpt-oss.attention.head_count u32              = 64
llama_model_loader: - kv  14:            gpt-oss.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                     gpt-oss.rope.freq_base f32              = 150000.000000
llama_model_loader: - kv  16:   gpt-oss.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                       gpt-oss.expert_count u32              = 32
llama_model_loader: - kv  18:                  gpt-oss.expert_used_count u32              = 4
llama_model_loader: - kv  19:               gpt-oss.attention.key_length u32              = 64
llama_model_loader: - kv  20:             gpt-oss.attention.value_length u32              = 64
llama_model_loader: - kv  21:                          general.file_type u32              = 1
llama_model_loader: - kv  22:           gpt-oss.attention.sliding_window u32              = 128
llama_model_loader: - kv  23:         gpt-oss.expert_feed_forward_length u32              = 2880
llama_model_loader: - kv  24:                  gpt-oss.rope.scaling.type str              = yarn
llama_model_loader: - kv  25:                gpt-oss.rope.scaling.factor f32              = 32.000000
llama_model_loader: - kv  26: gpt-oss.rope.scaling.original_context_length u32              = 4096
llama_model_loader: - kv  27:               general.quantization_version u32              = 2
llama_model_loader: - kv  28:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  29:                         tokenizer.ggml.pre str              = gpt-4o
llama_model_loader: - kv  30:                      tokenizer.ggml.tokens arr[str,201088]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  31:                  tokenizer.ggml.token_type arr[i32,201088]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  32:                      tokenizer.ggml.merges arr[str,446189]  = ["Ġ Ġ", "Ġ ĠĠĠ", "ĠĠ ĠĠ", "...
llama_model_loader: - kv  33:                tokenizer.ggml.bos_token_id u32              = 199998
llama_model_loader: - kv  34:                tokenizer.ggml.eos_token_id u32              = 200002
llama_model_loader: - kv  35:            tokenizer.ggml.padding_token_id u32              = 200017
llama_model_loader: - kv  36:                    tokenizer.chat_template str              = {# Chat template fixes by Unsloth #}\n...
llama_model_loader: - type  f32:  289 tensors
llama_model_loader: - type  f16:   98 tensors
llama_model_loader: - type mxfp4:   72 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 12.83 GiB (5.27 BPW) 
srv  log_server_r: request: GET /health 127.0.0.1 503
load: 0 unused tokens
load: setting token '<|message|>' (200008) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|start|>' (200006) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|constrain|>' (200003) attribute to USER_DEFINED (16), old attributes: 8
load: setting token '<|channel|>' (200005) attribute to USER_DEFINED (16), old attributes: 8
load: printing all EOG tokens:
load:   - 199999 ('<|endoftext|>')
load:   - 200002 ('<|return|>')
load:   - 200007 ('<|end|>')
load:   - 200012 ('<|call|>')
load: special_eog_ids contains both '<|return|>' and '<|call|>', or '<|calls|>' and '<|flush|>' tokens, removing '<|end|>' token from EOG list
load: special tokens cache size = 21
load: token to piece cache size = 1.3332 MB
print_info: arch                  = gpt-oss
print_info: vocab_only            = 0
print_info: no_alloc              = 0
print_info: n_ctx_train           = 131072
print_info: n_embd                = 2880
print_info: n_embd_inp            = 2880
print_info: n_layer               = 24
print_info: n_head                = 64
print_info: n_head_kv             = 8
print_info: n_rot                 = 64
print_info: n_swa                 = 128
print_info: is_swa_any            = 1
print_info: n_embd_head_k         = 64
print_info: n_embd_head_v         = 64
print_info: n_gqa                 = 8
print_info: n_embd_k_gqa          = 512
print_info: n_embd_v_gqa          = 512
print_info: f_norm_eps            = 0.0e+00
print_info: f_norm_rms_eps        = 1.0e-05
print_info: f_clamp_kqv           = 0.0e+00
print_info: f_max_alibi_bias      = 0.0e+00
print_info: f_logit_scale         = 0.0e+00
print_info: f_attn_scale          = 0.0e+00
print_info: n_ff                  = 2880
print_info: n_expert              = 32
print_info: n_expert_used         = 4
print_info: n_expert_groups       = 0
print_info: n_group_used          = 0
print_info: causal attn           = 1
print_info: pooling type          = 0
print_info: rope type             = 2
print_info: rope scaling          = yarn
print_info: freq_base_train       = 150000.0
print_info: freq_scale_train      = 0.03125
print_info: freq_base_swa         = 150000.0
print_info: freq_scale_swa        = 0.03125
print_info: n_ctx_orig_yarn       = 4096
print_info: rope_yarn_log_mul     = 0.0000
print_info: rope_finetuned        = unknown
print_info: model type            = 20B
print_info: model params          = 20.91 B
print_info: general.name          = Gpt-Oss-20B
print_info: n_ff_exp              = 2880
print_info: vocab type            = BPE
print_info: n_vocab               = 201088
print_info: n_merges              = 446189
print_info: BOS token             = 199998 '<|startoftext|>'
print_info: EOS token             = 200002 '<|return|>'
print_info: EOT token             = 199999 '<|endoftext|>'
print_info: PAD token             = 200017 '<|reserved_200017|>'
print_info: LF token              = 198 'Ċ'
print_info: EOG token             = 199999 '<|endoftext|>'
print_info: EOG token             = 200002 '<|return|>'
print_info: EOG token             = 200012 '<|call|>'
print_info: max token length      = 256
load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
srv  log_server_r: request: GET /health 127.0.0.1 503
load_tensors: offloading output layer to GPU
load_tensors: offloading 23 repeating layers to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors:        CUDA0 model buffer size = 12036.68 MiB
load_tensors:    CUDA_Host model buffer size =  1104.61 MiB
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
...srv  log_server_r: request: GET /health 127.0.0.1 503
.srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
..srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
srv  log_server_r: request: GET /health 127.0.0.1 503
.
common_init_result: added <|endoftext|> logit bias = -inf
common_init_result: added <|return|> logit bias = -inf
common_init_result: added <|call|> logit bias = -inf
llama_context: constructing llama_context
llama_context: n_seq_max     = 4
llama_context: n_ctx         = 56064
llama_context: n_ctx_seq     = 56064
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = auto
llama_context: kv_unified    = true
llama_context: freq_base     = 150000.0
llama_context: freq_scale    = 0.03125
llama_context: n_ctx_seq (56064) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_context:  CUDA_Host  output buffer size =     3.07 MiB
llama_kv_cache_iswa: creating non-SWA KV cache, size = 56064 cells
llama_kv_cache:      CUDA0 KV buffer size =  1314.00 MiB
llama_kv_cache: size = 1314.00 MiB ( 56064 cells,  12 layers,  4/1 seqs), K (f16):  657.00 MiB, V (f16):  657.00 MiB
llama_kv_cache_iswa: creating     SWA KV cache, size = 1024 cells
llama_kv_cache:      CUDA0 KV buffer size =    24.00 MiB
llama_kv_cache: size =   24.00 MiB (  1024 cells,  12 layers,  4/1 seqs), K (f16):   12.00 MiB, V (f16):   12.00 MiB
sched_reserve: reserving ...
sched_reserve: Flash Attention was auto, set to enabled
srv  log_server_r: request: GET /health 127.0.0.1 503
sched_reserve:      CUDA0 compute buffer size =   398.38 MiB
sched_reserve:  CUDA_Host compute buffer size =   117.15 MiB
sched_reserve: graph nodes  = 1352
sched_reserve: graph splits = 2
sched_reserve: reserve took 78.09 ms, sched copies = 1
common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
srv    load_model: initializing slots, n_slots = 4
slot   load_model: id  0 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  1 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  2 | task -1 | new slot, n_ctx = 56064
slot   load_model: id  3 | task -1 | new slot, n_ctx = 56064
srv    load_model: prompt cache is enabled, size limit: 8192 MiB
srv    load_model: use `--cache-ram 0` to disable the prompt cache
srv    load_model: for more info see https://github.com/ggml-org/llama.cpp/pull/16391
srv    load_model: thinking = 0
load_model: chat template, example_format: '<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2026-02-12

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

You are a helpful assistant<|end|><|start|>user<|message|>Hello<|end|><|start|>assistant<|channel|>final<|message|>Hi there<|end|><|start|>user<|message|>How are you?<|end|><|start|>assistant'
main: model loaded
main: server is listening on http://127.0.0.1:8000
main: starting the main loop...
srv  update_slots: all slots are idle
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 0 | processing task, is_child = 0
slot update_slots: id  3 | task 0 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 704
slot update_slots: id  3 | task 0 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 640, batch.n_tokens = 640, progress = 0.909091
slot update_slots: id  3 | task 0 | n_tokens = 640, memory_seq_rm [640, end)
slot update_slots: id  3 | task 0 | prompt processing progress, n_tokens = 704, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 0 | prompt done, n_tokens = 704, batch.n_tokens = 64
slot init_sampler: id  3 | task 0 | init sampler, took 0.17 ms, tokens: text = 704, total = 704
slot update_slots: id  3 | task 0 | created context checkpoint 1 of 8 (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot print_timing: id  3 | task 0 | 
prompt eval time =    1233.53 ms /   704 tokens (    1.75 ms per token,   570.72 tokens per second)
       eval time =     791.14 ms /    33 tokens (   23.97 ms per token,    41.71 tokens per second)
      total time =    2024.67 ms /   737 tokens
slot      release: id  3 | task 0 | stop processing: n_tokens = 736, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.810 (> 0.100 thold), f_keep = 0.957
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 35 | processing task, is_child = 0
slot update_slots: id  3 | task 35 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 869
slot update_slots: id  3 | task 35 | n_tokens = 704, memory_seq_rm [704, end)
slot update_slots: id  3 | task 35 | prompt processing progress, n_tokens = 805, batch.n_tokens = 101, progress = 0.926352
slot update_slots: id  3 | task 35 | n_tokens = 805, memory_seq_rm [805, end)
slot update_slots: id  3 | task 35 | prompt processing progress, n_tokens = 869, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 35 | prompt done, n_tokens = 869, batch.n_tokens = 64
slot init_sampler: id  3 | task 35 | init sampler, took 0.16 ms, tokens: text = 869, total = 869
slot update_slots: id  3 | task 35 | created context checkpoint 2 of 8 (pos_min = 0, pos_max = 804, size = 18.877 MiB)
slot print_timing: id  3 | task 35 | 
prompt eval time =     400.61 ms /   165 tokens (    2.43 ms per token,   411.87 tokens per second)
       eval time =    3050.52 ms /   121 tokens (   25.21 ms per token,    39.67 tokens per second)
      total time =    3451.13 ms /   286 tokens
slot      release: id  3 | task 35 | stop processing: n_tokens = 989, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.727 (> 0.100 thold), f_keep = 0.683
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 158 | processing task, is_child = 0
slot update_slots: id  3 | task 158 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 928
slot update_slots: id  3 | task 158 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 158 | prompt processing progress, n_tokens = 864, batch.n_tokens = 189, progress = 0.931035
slot update_slots: id  3 | task 158 | n_tokens = 864, memory_seq_rm [864, end)
slot update_slots: id  3 | task 158 | prompt processing progress, n_tokens = 928, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 158 | prompt done, n_tokens = 928, batch.n_tokens = 64
slot init_sampler: id  3 | task 158 | init sampler, took 0.18 ms, tokens: text = 928, total = 928
slot print_timing: id  3 | task 158 | 
prompt eval time =     570.75 ms /   253 tokens (    2.26 ms per token,   443.27 tokens per second)
       eval time =    2980.28 ms /   116 tokens (   25.69 ms per token,    38.92 tokens per second)
      total time =    3551.04 ms /   369 tokens
slot      release: id  3 | task 158 | stop processing: n_tokens = 1043, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.937 (> 0.100 thold), f_keep = 0.890
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 276 | processing task, is_child = 0
slot update_slots: id  3 | task 276 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 990
slot update_slots: id  3 | task 276 | n_tokens = 928, memory_seq_rm [928, end)
slot update_slots: id  3 | task 276 | prompt processing progress, n_tokens = 990, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 276 | prompt done, n_tokens = 990, batch.n_tokens = 62
slot init_sampler: id  3 | task 276 | init sampler, took 0.22 ms, tokens: text = 990, total = 990
slot update_slots: id  3 | task 276 | created context checkpoint 3 of 8 (pos_min = 19, pos_max = 927, size = 21.315 MiB)
slot print_timing: id  3 | task 276 | 
prompt eval time =     259.74 ms /    62 tokens (    4.19 ms per token,   238.70 tokens per second)
       eval time =    2783.11 ms /   106 tokens (   26.26 ms per token,    38.09 tokens per second)
      total time =    3042.85 ms /   168 tokens
slot      release: id  3 | task 276 | stop processing: n_tokens = 1095, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.930 (> 0.100 thold), f_keep = 0.904
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 383 | processing task, is_child = 0
slot update_slots: id  3 | task 383 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1065
slot update_slots: id  3 | task 383 | n_tokens = 990, memory_seq_rm [990, end)
slot update_slots: id  3 | task 383 | prompt processing progress, n_tokens = 1001, batch.n_tokens = 11, progress = 0.939906
slot update_slots: id  3 | task 383 | n_tokens = 1001, memory_seq_rm [1001, end)
slot update_slots: id  3 | task 383 | prompt processing progress, n_tokens = 1065, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 383 | prompt done, n_tokens = 1065, batch.n_tokens = 64
slot init_sampler: id  3 | task 383 | init sampler, took 0.20 ms, tokens: text = 1065, total = 1065
slot update_slots: id  3 | task 383 | created context checkpoint 4 of 8 (pos_min = 167, pos_max = 1000, size = 19.557 MiB)
slot print_timing: id  3 | task 383 | 
prompt eval time =     356.82 ms /    75 tokens (    4.76 ms per token,   210.19 tokens per second)
       eval time =    3169.05 ms /   116 tokens (   27.32 ms per token,    36.60 tokens per second)
      total time =    3525.87 ms /   191 tokens
slot      release: id  3 | task 383 | stop processing: n_tokens = 1180, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.903
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 501 | processing task, is_child = 0
slot update_slots: id  3 | task 501 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1144
slot update_slots: id  3 | task 501 | n_tokens = 1065, memory_seq_rm [1065, end)
slot update_slots: id  3 | task 501 | prompt processing progress, n_tokens = 1080, batch.n_tokens = 15, progress = 0.944056
slot update_slots: id  3 | task 501 | n_tokens = 1080, memory_seq_rm [1080, end)
slot update_slots: id  3 | task 501 | prompt processing progress, n_tokens = 1144, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 501 | prompt done, n_tokens = 1144, batch.n_tokens = 64
slot init_sampler: id  3 | task 501 | init sampler, took 0.20 ms, tokens: text = 1144, total = 1144
slot update_slots: id  3 | task 501 | created context checkpoint 5 of 8 (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot print_timing: id  3 | task 501 | 
prompt eval time =     345.48 ms /    79 tokens (    4.37 ms per token,   228.67 tokens per second)
       eval time =    3481.67 ms /   128 tokens (   27.20 ms per token,    36.76 tokens per second)
      total time =    3827.15 ms /   207 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 501 | stop processing: n_tokens = 1271, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.949 (> 0.100 thold), f_keep = 0.900
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 631 | processing task, is_child = 0
slot update_slots: id  3 | task 631 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1206
slot update_slots: id  3 | task 631 | n_tokens = 1144, memory_seq_rm [1144, end)
slot update_slots: id  3 | task 631 | prompt processing progress, n_tokens = 1206, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 631 | prompt done, n_tokens = 1206, batch.n_tokens = 62
slot init_sampler: id  3 | task 631 | init sampler, took 0.20 ms, tokens: text = 1206, total = 1206
slot print_timing: id  3 | task 631 | 
prompt eval time =     232.42 ms /    62 tokens (    3.75 ms per token,   266.76 tokens per second)
       eval time =    5218.02 ms /   188 tokens (   27.76 ms per token,    36.03 tokens per second)
      total time =    5450.44 ms /   250 tokens
slot      release: id  3 | task 631 | stop processing: n_tokens = 1393, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.330 (> 0.100 thold), f_keep = 0.866
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 820 | processing task, is_child = 0
slot update_slots: id  3 | task 820 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3658
slot update_slots: id  3 | task 820 | n_tokens = 1206, memory_seq_rm [1206, end)
slot update_slots: id  3 | task 820 | prompt processing progress, n_tokens = 3254, batch.n_tokens = 2048, progress = 0.889557
slot update_slots: id  3 | task 820 | n_tokens = 3254, memory_seq_rm [3254, end)
slot update_slots: id  3 | task 820 | prompt processing progress, n_tokens = 3594, batch.n_tokens = 340, progress = 0.982504
slot update_slots: id  3 | task 820 | n_tokens = 3594, memory_seq_rm [3594, end)
slot update_slots: id  3 | task 820 | prompt processing progress, n_tokens = 3658, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 820 | prompt done, n_tokens = 3658, batch.n_tokens = 64
slot init_sampler: id  3 | task 820 | init sampler, took 0.76 ms, tokens: text = 3658, total = 3658
slot update_slots: id  3 | task 820 | created context checkpoint 6 of 8 (pos_min = 2570, pos_max = 3593, size = 24.012 MiB)
slot print_timing: id  3 | task 820 | 
prompt eval time =    2850.32 ms /  2452 tokens (    1.16 ms per token,   860.25 tokens per second)
       eval time =    5826.35 ms /   198 tokens (   29.43 ms per token,    33.98 tokens per second)
      total time =    8676.67 ms /  2650 tokens
slot      release: id  3 | task 820 | stop processing: n_tokens = 3855, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.909 (> 0.100 thold), f_keep = 0.949
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1021 | processing task, is_child = 0
slot update_slots: id  3 | task 1021 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4025
slot update_slots: id  3 | task 1021 | n_tokens = 3658, memory_seq_rm [3658, end)
slot update_slots: id  3 | task 1021 | prompt processing progress, n_tokens = 3961, batch.n_tokens = 303, progress = 0.984099
slot update_slots: id  3 | task 1021 | n_tokens = 3961, memory_seq_rm [3961, end)
slot update_slots: id  3 | task 1021 | prompt processing progress, n_tokens = 4025, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1021 | prompt done, n_tokens = 4025, batch.n_tokens = 64
slot init_sampler: id  3 | task 1021 | init sampler, took 0.59 ms, tokens: text = 4025, total = 4025
slot update_slots: id  3 | task 1021 | created context checkpoint 7 of 8 (pos_min = 2937, pos_max = 3960, size = 24.012 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 1021
slot      release: id  3 | task 1021 | stop processing: n_tokens = 4127, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.164
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4127, total state size = 120.786 MiB
srv          load:  - looking for better prompt, base f_keep = 0.164, sim = 0.956
srv        update:  - cache state: 1 prompts, 262.983 MiB (limits: 8192.000 MiB, 56064 tokens, 128557 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv  get_availabl: prompt cache update took 200.92 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1127 | processing task, is_child = 0
slot update_slots: id  3 | task 1127 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 706
slot update_slots: id  3 | task 1127 | n_past = 675, slot.prompt.tokens.size() = 4127, seq_id = 3, pos_min = 3103, n_swa = 128
slot update_slots: id  3 | task 1127 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 1127 | erased invalidated context checkpoint (pos_min = 2570, pos_max = 3593, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1127 | erased invalidated context checkpoint (pos_min = 2937, pos_max = 3960, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 1127 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 1127 | prompt processing progress, n_tokens = 706, batch.n_tokens = 31, progress = 1.000000
slot update_slots: id  3 | task 1127 | prompt done, n_tokens = 706, batch.n_tokens = 31
slot init_sampler: id  3 | task 1127 | init sampler, took 0.12 ms, tokens: text = 706, total = 706
slot print_timing: id  3 | task 1127 | 
prompt eval time =     189.00 ms /    31 tokens (    6.10 ms per token,   164.02 tokens per second)
       eval time =   13677.85 ms /   526 tokens (   26.00 ms per token,    38.46 tokens per second)
      total time =   13866.86 ms /   557 tokens
slot      release: id  3 | task 1127 | stop processing: n_tokens = 1231, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.432 (> 0.100 thold), f_keep = 0.548
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 1654 | processing task, is_child = 0
slot update_slots: id  3 | task 1654 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1562
slot update_slots: id  3 | task 1654 | n_past = 675, slot.prompt.tokens.size() = 1231, seq_id = 3, pos_min = 656, n_swa = 128
slot update_slots: id  3 | task 1654 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 1654 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 1654 | prompt processing progress, n_tokens = 1498, batch.n_tokens = 823, progress = 0.959027
slot update_slots: id  3 | task 1654 | n_tokens = 1498, memory_seq_rm [1498, end)
slot update_slots: id  3 | task 1654 | prompt processing progress, n_tokens = 1562, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 1654 | prompt done, n_tokens = 1562, batch.n_tokens = 64
slot init_sampler: id  3 | task 1654 | init sampler, took 0.34 ms, tokens: text = 1562, total = 1562
slot update_slots: id  3 | task 1654 | created context checkpoint 6 of 8 (pos_min = 548, pos_max = 1497, size = 22.277 MiB)
slot print_timing: id  3 | task 1654 | 
prompt eval time =    1165.60 ms /   887 tokens (    1.31 ms per token,   760.98 tokens per second)
       eval time =   10953.55 ms /   396 tokens (   27.66 ms per token,    36.15 tokens per second)
      total time =   12119.15 ms /  1283 tokens
slot      release: id  3 | task 1654 | stop processing: n_tokens = 1957, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.951 (> 0.100 thold), f_keep = 0.798
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2052 | processing task, is_child = 0
slot update_slots: id  3 | task 2052 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1643
slot update_slots: id  3 | task 2052 | n_tokens = 1562, memory_seq_rm [1562, end)
slot update_slots: id  3 | task 2052 | prompt processing progress, n_tokens = 1579, batch.n_tokens = 17, progress = 0.961047
slot update_slots: id  3 | task 2052 | n_tokens = 1579, memory_seq_rm [1579, end)
slot update_slots: id  3 | task 2052 | prompt processing progress, n_tokens = 1643, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2052 | prompt done, n_tokens = 1643, batch.n_tokens = 64
slot init_sampler: id  3 | task 2052 | init sampler, took 0.28 ms, tokens: text = 1643, total = 1643
slot update_slots: id  3 | task 2052 | created context checkpoint 7 of 8 (pos_min = 1060, pos_max = 1578, size = 12.170 MiB)
slot print_timing: id  3 | task 2052 | 
prompt eval time =     349.39 ms /    81 tokens (    4.31 ms per token,   231.83 tokens per second)
       eval time =    3599.11 ms /   135 tokens (   26.66 ms per token,    37.51 tokens per second)
      total time =    3948.51 ms /   216 tokens
slot      release: id  3 | task 2052 | stop processing: n_tokens = 1777, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.956 (> 0.100 thold), f_keep = 0.925
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2189 | processing task, is_child = 0
slot update_slots: id  3 | task 2189 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1718
slot update_slots: id  3 | task 2189 | n_tokens = 1643, memory_seq_rm [1643, end)
slot update_slots: id  3 | task 2189 | prompt processing progress, n_tokens = 1654, batch.n_tokens = 11, progress = 0.962747
slot update_slots: id  3 | task 2189 | n_tokens = 1654, memory_seq_rm [1654, end)
slot update_slots: id  3 | task 2189 | prompt processing progress, n_tokens = 1718, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2189 | prompt done, n_tokens = 1718, batch.n_tokens = 64
slot init_sampler: id  3 | task 2189 | init sampler, took 0.32 ms, tokens: text = 1718, total = 1718
slot update_slots: id  3 | task 2189 | created context checkpoint 8 of 8 (pos_min = 1060, pos_max = 1653, size = 13.929 MiB)
slot print_timing: id  3 | task 2189 | 
prompt eval time =     337.70 ms /    75 tokens (    4.50 ms per token,   222.09 tokens per second)
       eval time =    1309.37 ms /    50 tokens (   26.19 ms per token,    38.19 tokens per second)
      total time =    1647.07 ms /   125 tokens
slot      release: id  3 | task 2189 | stop processing: n_tokens = 1767, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.972
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2241 | processing task, is_child = 0
slot update_slots: id  3 | task 2241 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1780
slot update_slots: id  3 | task 2241 | n_tokens = 1718, memory_seq_rm [1718, end)
slot update_slots: id  3 | task 2241 | prompt processing progress, n_tokens = 1780, batch.n_tokens = 62, progress = 1.000000
slot update_slots: id  3 | task 2241 | prompt done, n_tokens = 1780, batch.n_tokens = 62
slot init_sampler: id  3 | task 2241 | init sampler, took 0.35 ms, tokens: text = 1780, total = 1780
slot print_timing: id  3 | task 2241 | 
prompt eval time =     254.25 ms /    62 tokens (    4.10 ms per token,   243.86 tokens per second)
       eval time =    4489.37 ms /   174 tokens (   25.80 ms per token,    38.76 tokens per second)
      total time =    4743.62 ms /   236 tokens
slot      release: id  3 | task 2241 | stop processing: n_tokens = 1953, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.911
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2416 | processing task, is_child = 0
slot update_slots: id  3 | task 2416 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1844
slot update_slots: id  3 | task 2416 | n_tokens = 1780, memory_seq_rm [1780, end)
slot update_slots: id  3 | task 2416 | prompt processing progress, n_tokens = 1844, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2416 | prompt done, n_tokens = 1844, batch.n_tokens = 64
slot init_sampler: id  3 | task 2416 | init sampler, took 0.35 ms, tokens: text = 1844, total = 1844
slot update_slots: id  3 | task 2416 | erasing old context checkpoint (pos_min = 0, pos_max = 639, size = 15.008 MiB)
slot update_slots: id  3 | task 2416 | created context checkpoint 8 of 8 (pos_min = 1066, pos_max = 1779, size = 16.743 MiB)
slot print_timing: id  3 | task 2416 | 
prompt eval time =     239.37 ms /    64 tokens (    3.74 ms per token,   267.36 tokens per second)
       eval time =    2794.49 ms /   106 tokens (   26.36 ms per token,    37.93 tokens per second)
      total time =    3033.86 ms /   170 tokens
slot      release: id  3 | task 2416 | stop processing: n_tokens = 1949, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.959 (> 0.100 thold), f_keep = 0.946
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2523 | processing task, is_child = 0
slot update_slots: id  3 | task 2523 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1922
slot update_slots: id  3 | task 2523 | n_tokens = 1844, memory_seq_rm [1844, end)
slot update_slots: id  3 | task 2523 | prompt processing progress, n_tokens = 1858, batch.n_tokens = 14, progress = 0.966701
slot update_slots: id  3 | task 2523 | n_tokens = 1858, memory_seq_rm [1858, end)
slot update_slots: id  3 | task 2523 | prompt processing progress, n_tokens = 1922, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2523 | prompt done, n_tokens = 1922, batch.n_tokens = 64
slot init_sampler: id  3 | task 2523 | init sampler, took 0.30 ms, tokens: text = 1922, total = 1922
slot update_slots: id  3 | task 2523 | erasing old context checkpoint (pos_min = 0, pos_max = 804, size = 18.877 MiB)
slot update_slots: id  3 | task 2523 | created context checkpoint 8 of 8 (pos_min = 1066, pos_max = 1857, size = 18.572 MiB)
slot print_timing: id  3 | task 2523 | 
prompt eval time =     316.51 ms /    78 tokens (    4.06 ms per token,   246.44 tokens per second)
       eval time =    2090.62 ms /    78 tokens (   26.80 ms per token,    37.31 tokens per second)
      total time =    2407.13 ms /   156 tokens
slot      release: id  3 | task 2523 | stop processing: n_tokens = 1999, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.961 (> 0.100 thold), f_keep = 0.961
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2603 | processing task, is_child = 0
slot update_slots: id  3 | task 2603 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2000
slot update_slots: id  3 | task 2603 | n_tokens = 1922, memory_seq_rm [1922, end)
slot update_slots: id  3 | task 2603 | prompt processing progress, n_tokens = 1936, batch.n_tokens = 14, progress = 0.968000
slot update_slots: id  3 | task 2603 | n_tokens = 1936, memory_seq_rm [1936, end)
slot update_slots: id  3 | task 2603 | prompt processing progress, n_tokens = 2000, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2603 | prompt done, n_tokens = 2000, batch.n_tokens = 64
slot init_sampler: id  3 | task 2603 | init sampler, took 0.39 ms, tokens: text = 2000, total = 2000
slot update_slots: id  3 | task 2603 | erasing old context checkpoint (pos_min = 19, pos_max = 927, size = 21.315 MiB)
slot update_slots: id  3 | task 2603 | created context checkpoint 8 of 8 (pos_min = 1112, pos_max = 1935, size = 19.322 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 2603
slot      release: id  3 | task 2603 | stop processing: n_tokens = 2108, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.524 (> 0.100 thold), f_keep = 0.522
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 2715 | processing task, is_child = 0
slot update_slots: id  3 | task 2715 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2098
slot update_slots: id  3 | task 2715 | n_past = 1100, slot.prompt.tokens.size() = 2108, seq_id = 3, pos_min = 1123, n_swa = 128
slot update_slots: id  3 | task 2715 | restored context checkpoint (pos_min = 548, pos_max = 1497, size = 22.277 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1060, pos_max = 1578, n_swa = 128, size = 12.170 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1060, pos_max = 1653, n_swa = 128, size = 13.929 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1066, pos_max = 1779, n_swa = 128, size = 16.743 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1066, pos_max = 1857, n_swa = 128, size = 18.572 MiB)
slot update_slots: id  3 | task 2715 | erased invalidated context checkpoint (pos_min = 1112, pos_max = 1935, n_swa = 128, size = 19.322 MiB)
slot update_slots: id  3 | task 2715 | n_tokens = 1100, memory_seq_rm [1100, end)
slot update_slots: id  3 | task 2715 | prompt processing progress, n_tokens = 2034, batch.n_tokens = 934, progress = 0.969495
slot update_slots: id  3 | task 2715 | n_tokens = 2034, memory_seq_rm [2034, end)
slot update_slots: id  3 | task 2715 | prompt processing progress, n_tokens = 2098, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 2715 | prompt done, n_tokens = 2098, batch.n_tokens = 64
slot init_sampler: id  3 | task 2715 | init sampler, took 0.38 ms, tokens: text = 2098, total = 2098
slot update_slots: id  3 | task 2715 | created context checkpoint 4 of 8 (pos_min = 1010, pos_max = 2033, size = 24.012 MiB)
slot print_timing: id  3 | task 2715 | 
prompt eval time =    1244.34 ms /   998 tokens (    1.25 ms per token,   802.03 tokens per second)
       eval time =   16686.83 ms /   588 tokens (   28.38 ms per token,    35.24 tokens per second)
      total time =   17931.17 ms /  1586 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 2715 | stop processing: n_tokens = 2685, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.504 (> 0.100 thold), f_keep = 0.508
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3305 | processing task, is_child = 0
slot update_slots: id  3 | task 3305 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2708
slot update_slots: id  3 | task 3305 | n_past = 1364, slot.prompt.tokens.size() = 2685, seq_id = 3, pos_min = 1661, n_swa = 128
slot update_slots: id  3 | task 3305 | restored context checkpoint (pos_min = 1010, pos_max = 2033, size = 24.012 MiB)
slot update_slots: id  3 | task 3305 | n_tokens = 1364, memory_seq_rm [1364, end)
slot update_slots: id  3 | task 3305 | prompt processing progress, n_tokens = 2644, batch.n_tokens = 1280, progress = 0.976366
slot update_slots: id  3 | task 3305 | n_tokens = 2644, memory_seq_rm [2644, end)
slot update_slots: id  3 | task 3305 | prompt processing progress, n_tokens = 2708, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3305 | prompt done, n_tokens = 2708, batch.n_tokens = 64
slot init_sampler: id  3 | task 3305 | init sampler, took 0.55 ms, tokens: text = 2708, total = 2708
slot update_slots: id  3 | task 3305 | created context checkpoint 5 of 8 (pos_min = 1620, pos_max = 2643, size = 24.012 MiB)
slot print_timing: id  3 | task 3305 | 
prompt eval time =    1640.30 ms /  1344 tokens (    1.22 ms per token,   819.36 tokens per second)
       eval time =    5460.23 ms /   214 tokens (   25.52 ms per token,    39.19 tokens per second)
      total time =    7100.53 ms /  1558 tokens
slot      release: id  3 | task 3305 | stop processing: n_tokens = 2921, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.971 (> 0.100 thold), f_keep = 0.927
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3521 | processing task, is_child = 0
slot update_slots: id  3 | task 3521 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2790
slot update_slots: id  3 | task 3521 | n_tokens = 2708, memory_seq_rm [2708, end)
slot update_slots: id  3 | task 3521 | prompt processing progress, n_tokens = 2726, batch.n_tokens = 18, progress = 0.977061
slot update_slots: id  3 | task 3521 | n_tokens = 2726, memory_seq_rm [2726, end)
slot update_slots: id  3 | task 3521 | prompt processing progress, n_tokens = 2790, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3521 | prompt done, n_tokens = 2790, batch.n_tokens = 64
slot init_sampler: id  3 | task 3521 | init sampler, took 0.42 ms, tokens: text = 2790, total = 2790
slot update_slots: id  3 | task 3521 | created context checkpoint 6 of 8 (pos_min = 1897, pos_max = 2725, size = 19.439 MiB)
slot print_timing: id  3 | task 3521 | 
prompt eval time =     334.19 ms /    82 tokens (    4.08 ms per token,   245.37 tokens per second)
       eval time =    4462.25 ms /   167 tokens (   26.72 ms per token,    37.43 tokens per second)
      total time =    4796.44 ms /   249 tokens
slot      release: id  3 | task 3521 | stop processing: n_tokens = 2956, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.973 (> 0.100 thold), f_keep = 0.944
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3690 | processing task, is_child = 0
slot update_slots: id  3 | task 3690 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2868
slot update_slots: id  3 | task 3690 | n_tokens = 2790, memory_seq_rm [2790, end)
slot update_slots: id  3 | task 3690 | prompt processing progress, n_tokens = 2804, batch.n_tokens = 14, progress = 0.977685
slot update_slots: id  3 | task 3690 | n_tokens = 2804, memory_seq_rm [2804, end)
slot update_slots: id  3 | task 3690 | prompt processing progress, n_tokens = 2868, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3690 | prompt done, n_tokens = 2868, batch.n_tokens = 64
slot init_sampler: id  3 | task 3690 | init sampler, took 0.55 ms, tokens: text = 2868, total = 2868
slot update_slots: id  3 | task 3690 | created context checkpoint 7 of 8 (pos_min = 1932, pos_max = 2803, size = 20.448 MiB)
slot print_timing: id  3 | task 3690 | 
prompt eval time =     365.66 ms /    78 tokens (    4.69 ms per token,   213.31 tokens per second)
       eval time =    7951.49 ms /   288 tokens (   27.61 ms per token,    36.22 tokens per second)
      total time =    8317.15 ms /   366 tokens
slot      release: id  3 | task 3690 | stop processing: n_tokens = 3155, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.970 (> 0.100 thold), f_keep = 0.909
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 3980 | processing task, is_child = 0
slot update_slots: id  3 | task 3980 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2958
slot update_slots: id  3 | task 3980 | n_tokens = 2868, memory_seq_rm [2868, end)
slot update_slots: id  3 | task 3980 | prompt processing progress, n_tokens = 2894, batch.n_tokens = 26, progress = 0.978364
slot update_slots: id  3 | task 3980 | n_tokens = 2894, memory_seq_rm [2894, end)
slot update_slots: id  3 | task 3980 | prompt processing progress, n_tokens = 2958, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 3980 | prompt done, n_tokens = 2958, batch.n_tokens = 64
slot init_sampler: id  3 | task 3980 | init sampler, took 0.48 ms, tokens: text = 2958, total = 2958
slot update_slots: id  3 | task 3980 | created context checkpoint 8 of 8 (pos_min = 2131, pos_max = 2893, size = 17.892 MiB)
slot print_timing: id  3 | task 3980 | 
prompt eval time =     365.55 ms /    90 tokens (    4.06 ms per token,   246.21 tokens per second)
       eval time =    4260.52 ms /   152 tokens (   28.03 ms per token,    35.68 tokens per second)
      total time =    4626.07 ms /   242 tokens
slot      release: id  3 | task 3980 | stop processing: n_tokens = 3109, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.951
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4134 | processing task, is_child = 0
slot update_slots: id  3 | task 4134 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3021
slot update_slots: id  3 | task 4134 | n_tokens = 2958, memory_seq_rm [2958, end)
slot update_slots: id  3 | task 4134 | prompt processing progress, n_tokens = 3021, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 4134 | prompt done, n_tokens = 3021, batch.n_tokens = 63
slot init_sampler: id  3 | task 4134 | init sampler, took 0.54 ms, tokens: text = 3021, total = 3021
slot print_timing: id  3 | task 4134 | 
prompt eval time =     253.03 ms /    63 tokens (    4.02 ms per token,   248.99 tokens per second)
       eval time =    2924.87 ms /   108 tokens (   27.08 ms per token,    36.92 tokens per second)
      total time =    3177.89 ms /   171 tokens
slot      release: id  3 | task 4134 | stop processing: n_tokens = 3128, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.966
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4243 | processing task, is_child = 0
slot update_slots: id  3 | task 4243 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3099
slot update_slots: id  3 | task 4243 | n_tokens = 3021, memory_seq_rm [3021, end)
slot update_slots: id  3 | task 4243 | prompt processing progress, n_tokens = 3035, batch.n_tokens = 14, progress = 0.979348
slot update_slots: id  3 | task 4243 | n_tokens = 3035, memory_seq_rm [3035, end)
slot update_slots: id  3 | task 4243 | prompt processing progress, n_tokens = 3099, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4243 | prompt done, n_tokens = 3099, batch.n_tokens = 64
slot init_sampler: id  3 | task 4243 | init sampler, took 0.47 ms, tokens: text = 3099, total = 3099
slot update_slots: id  3 | task 4243 | erasing old context checkpoint (pos_min = 167, pos_max = 1000, size = 19.557 MiB)
slot update_slots: id  3 | task 4243 | created context checkpoint 8 of 8 (pos_min = 2131, pos_max = 3034, size = 21.198 MiB)
slot print_timing: id  3 | task 4243 | 
prompt eval time =     337.13 ms /    78 tokens (    4.32 ms per token,   231.36 tokens per second)
       eval time =    3777.04 ms /   142 tokens (   26.60 ms per token,    37.60 tokens per second)
      total time =    4114.17 ms /   220 tokens
slot      release: id  3 | task 4243 | stop processing: n_tokens = 3240, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.268 (> 0.100 thold), f_keep = 0.210
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3240, total state size = 99.987 MiB
srv          load:  - looking for better prompt, base f_keep = 0.210, sim = 0.268
srv        update:  - cache state: 2 prompts, 531.663 MiB (limits: 8192.000 MiB, 56064 tokens, 113512 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv  get_availabl: prompt cache update took 187.18 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4387 | processing task, is_child = 0
slot update_slots: id  3 | task 4387 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2531
slot update_slots: id  3 | task 4387 | n_past = 679, slot.prompt.tokens.size() = 3240, seq_id = 3, pos_min = 2216, n_swa = 128
slot update_slots: id  3 | task 4387 | restored context checkpoint (pos_min = 548, pos_max = 1497, size = 22.277 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 1010, pos_max = 2033, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 1620, pos_max = 2643, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 1897, pos_max = 2725, n_swa = 128, size = 19.439 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 1932, pos_max = 2803, n_swa = 128, size = 20.448 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 2131, pos_max = 2893, n_swa = 128, size = 17.892 MiB)
slot update_slots: id  3 | task 4387 | erased invalidated context checkpoint (pos_min = 2131, pos_max = 3034, n_swa = 128, size = 21.198 MiB)
slot update_slots: id  3 | task 4387 | n_tokens = 679, memory_seq_rm [679, end)
slot update_slots: id  3 | task 4387 | prompt processing progress, n_tokens = 2467, batch.n_tokens = 1788, progress = 0.974714
slot update_slots: id  3 | task 4387 | n_tokens = 2467, memory_seq_rm [2467, end)
slot update_slots: id  3 | task 4387 | prompt processing progress, n_tokens = 2531, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4387 | prompt done, n_tokens = 2531, batch.n_tokens = 64
slot init_sampler: id  3 | task 4387 | init sampler, took 0.45 ms, tokens: text = 2531, total = 2531
slot update_slots: id  3 | task 4387 | created context checkpoint 3 of 8 (pos_min = 1443, pos_max = 2466, size = 24.012 MiB)
slot print_timing: id  3 | task 4387 | 
prompt eval time =    2157.28 ms /  1852 tokens (    1.16 ms per token,   858.49 tokens per second)
       eval time =    4500.48 ms /   173 tokens (   26.01 ms per token,    38.44 tokens per second)
      total time =    6657.76 ms /  2025 tokens
slot      release: id  3 | task 4387 | stop processing: n_tokens = 2703, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.976 (> 0.100 thold), f_keep = 0.936
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4562 | processing task, is_child = 0
slot update_slots: id  3 | task 4562 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2594
slot update_slots: id  3 | task 4562 | n_tokens = 2531, memory_seq_rm [2531, end)
slot update_slots: id  3 | task 4562 | prompt processing progress, n_tokens = 2594, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 4562 | prompt done, n_tokens = 2594, batch.n_tokens = 63
slot init_sampler: id  3 | task 4562 | init sampler, took 0.75 ms, tokens: text = 2594, total = 2594
slot print_timing: id  3 | task 4562 | 
prompt eval time =     229.97 ms /    63 tokens (    3.65 ms per token,   273.95 tokens per second)
       eval time =    4439.49 ms /   165 tokens (   26.91 ms per token,    37.17 tokens per second)
      total time =    4669.46 ms /   228 tokens
slot      release: id  3 | task 4562 | stop processing: n_tokens = 2758, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.941
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 4728 | processing task, is_child = 0
slot update_slots: id  3 | task 4728 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2709
slot update_slots: id  3 | task 4728 | n_tokens = 2594, memory_seq_rm [2594, end)
slot update_slots: id  3 | task 4728 | prompt processing progress, n_tokens = 2645, batch.n_tokens = 51, progress = 0.976375
slot update_slots: id  3 | task 4728 | n_tokens = 2645, memory_seq_rm [2645, end)
slot update_slots: id  3 | task 4728 | prompt processing progress, n_tokens = 2709, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 4728 | prompt done, n_tokens = 2709, batch.n_tokens = 64
slot init_sampler: id  3 | task 4728 | init sampler, took 0.41 ms, tokens: text = 2709, total = 2709
slot update_slots: id  3 | task 4728 | created context checkpoint 4 of 8 (pos_min = 1803, pos_max = 2644, size = 19.744 MiB)
slot print_timing: id  3 | task 4728 | 
prompt eval time =     465.17 ms /   115 tokens (    4.04 ms per token,   247.22 tokens per second)
       eval time =    8104.02 ms /   286 tokens (   28.34 ms per token,    35.29 tokens per second)
      total time =    8569.19 ms /   401 tokens
slot      release: id  3 | task 4728 | stop processing: n_tokens = 2994, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.905
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5016 | processing task, is_child = 0
slot update_slots: id  3 | task 5016 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2787
slot update_slots: id  3 | task 5016 | n_tokens = 2709, memory_seq_rm [2709, end)
slot update_slots: id  3 | task 5016 | prompt processing progress, n_tokens = 2723, batch.n_tokens = 14, progress = 0.977036
slot update_slots: id  3 | task 5016 | n_tokens = 2723, memory_seq_rm [2723, end)
slot update_slots: id  3 | task 5016 | prompt processing progress, n_tokens = 2787, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5016 | prompt done, n_tokens = 2787, batch.n_tokens = 64
slot init_sampler: id  3 | task 5016 | init sampler, took 0.50 ms, tokens: text = 2787, total = 2787
slot update_slots: id  3 | task 5016 | created context checkpoint 5 of 8 (pos_min = 2039, pos_max = 2722, size = 16.039 MiB)
slot print_timing: id  3 | task 5016 | 
prompt eval time =     327.67 ms /    78 tokens (    4.20 ms per token,   238.05 tokens per second)
       eval time =    4553.99 ms /   158 tokens (   28.82 ms per token,    34.69 tokens per second)
      total time =    4881.66 ms /   236 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 5016 | stop processing: n_tokens = 2944, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.915 (> 0.100 thold), f_keep = 0.947
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5176 | processing task, is_child = 0
slot update_slots: id  3 | task 5176 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3047
slot update_slots: id  3 | task 5176 | n_tokens = 2787, memory_seq_rm [2787, end)
slot update_slots: id  3 | task 5176 | prompt processing progress, n_tokens = 2983, batch.n_tokens = 196, progress = 0.978996
slot update_slots: id  3 | task 5176 | n_tokens = 2983, memory_seq_rm [2983, end)
slot update_slots: id  3 | task 5176 | prompt processing progress, n_tokens = 3047, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5176 | prompt done, n_tokens = 3047, batch.n_tokens = 64
slot init_sampler: id  3 | task 5176 | init sampler, took 0.48 ms, tokens: text = 3047, total = 3047
slot update_slots: id  3 | task 5176 | created context checkpoint 6 of 8 (pos_min = 2039, pos_max = 2982, size = 22.136 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 5176
slot      release: id  3 | task 5176 | stop processing: n_tokens = 3108, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.273 (> 0.100 thold), f_keep = 0.218
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 3108, total state size = 95.274 MiB
srv          load:  - looking for better prompt, base f_keep = 0.218, sim = 0.273
srv        update:  - cache state: 3 prompts, 750.562 MiB (limits: 8192.000 MiB, 56064 tokens, 114329 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv  get_availabl: prompt cache update took 154.01 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5241 | processing task, is_child = 0
slot update_slots: id  3 | task 5241 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2472
slot update_slots: id  3 | task 5241 | n_past = 676, slot.prompt.tokens.size() = 3108, seq_id = 3, pos_min = 2153, n_swa = 128
slot update_slots: id  3 | task 5241 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 5241 | erased invalidated context checkpoint (pos_min = 1443, pos_max = 2466, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5241 | erased invalidated context checkpoint (pos_min = 1803, pos_max = 2644, n_swa = 128, size = 19.744 MiB)
slot update_slots: id  3 | task 5241 | erased invalidated context checkpoint (pos_min = 2039, pos_max = 2722, n_swa = 128, size = 16.039 MiB)
slot update_slots: id  3 | task 5241 | erased invalidated context checkpoint (pos_min = 2039, pos_max = 2982, n_swa = 128, size = 22.136 MiB)
slot update_slots: id  3 | task 5241 | n_tokens = 676, memory_seq_rm [676, end)
slot update_slots: id  3 | task 5241 | prompt processing progress, n_tokens = 2408, batch.n_tokens = 1732, progress = 0.974110
slot update_slots: id  3 | task 5241 | n_tokens = 2408, memory_seq_rm [2408, end)
slot update_slots: id  3 | task 5241 | prompt processing progress, n_tokens = 2472, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5241 | prompt done, n_tokens = 2472, batch.n_tokens = 64
slot init_sampler: id  3 | task 5241 | init sampler, took 0.43 ms, tokens: text = 2472, total = 2472
slot update_slots: id  3 | task 5241 | created context checkpoint 3 of 8 (pos_min = 1384, pos_max = 2407, size = 24.012 MiB)
slot print_timing: id  3 | task 5241 | 
prompt eval time =    2097.48 ms /  1796 tokens (    1.17 ms per token,   856.26 tokens per second)
       eval time =    7991.59 ms /   294 tokens (   27.18 ms per token,    36.79 tokens per second)
      total time =   10089.07 ms /  2090 tokens
slot      release: id  3 | task 5241 | stop processing: n_tokens = 2765, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.811 (> 0.100 thold), f_keep = 0.244
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 2765, total state size = 88.849 MiB
srv          load:  - looking for better prompt, base f_keep = 0.244, sim = 0.811
srv        update:  - cache state: 4 prompts, 905.115 MiB (limits: 8192.000 MiB, 56064 tokens, 119832 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv  get_availabl: prompt cache update took 110.33 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5537 | processing task, is_child = 0
slot update_slots: id  3 | task 5537 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 832
slot update_slots: id  3 | task 5537 | n_past = 675, slot.prompt.tokens.size() = 2765, seq_id = 3, pos_min = 1741, n_swa = 128
slot update_slots: id  3 | task 5537 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 5537 | erased invalidated context checkpoint (pos_min = 548, pos_max = 1497, n_swa = 128, size = 22.277 MiB)
slot update_slots: id  3 | task 5537 | erased invalidated context checkpoint (pos_min = 1384, pos_max = 2407, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 5537 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 5537 | prompt processing progress, n_tokens = 768, batch.n_tokens = 93, progress = 0.923077
slot update_slots: id  3 | task 5537 | n_tokens = 768, memory_seq_rm [768, end)
slot update_slots: id  3 | task 5537 | prompt processing progress, n_tokens = 832, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5537 | prompt done, n_tokens = 832, batch.n_tokens = 64
slot init_sampler: id  3 | task 5537 | init sampler, took 0.15 ms, tokens: text = 832, total = 832
slot print_timing: id  3 | task 5537 | 
prompt eval time =     497.80 ms /   157 tokens (    3.17 ms per token,   315.39 tokens per second)
       eval time =    1008.68 ms /    40 tokens (   25.22 ms per token,    39.66 tokens per second)
      total time =    1506.49 ms /   197 tokens
slot      release: id  3 | task 5537 | stop processing: n_tokens = 871, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.626 (> 0.100 thold), f_keep = 0.955
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5579 | processing task, is_child = 0
slot update_slots: id  3 | task 5579 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 1330
slot update_slots: id  3 | task 5579 | n_tokens = 832, memory_seq_rm [832, end)
slot update_slots: id  3 | task 5579 | prompt processing progress, n_tokens = 1266, batch.n_tokens = 434, progress = 0.951880
slot update_slots: id  3 | task 5579 | n_tokens = 1266, memory_seq_rm [1266, end)
slot update_slots: id  3 | task 5579 | prompt processing progress, n_tokens = 1330, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5579 | prompt done, n_tokens = 1330, batch.n_tokens = 64
slot init_sampler: id  3 | task 5579 | init sampler, took 0.22 ms, tokens: text = 1330, total = 1330
slot update_slots: id  3 | task 5579 | created context checkpoint 2 of 8 (pos_min = 675, pos_max = 1265, size = 13.859 MiB)
slot print_timing: id  3 | task 5579 | 
prompt eval time =     598.12 ms /   498 tokens (    1.20 ms per token,   832.62 tokens per second)
       eval time =    1187.39 ms /    45 tokens (   26.39 ms per token,    37.90 tokens per second)
      total time =    1785.50 ms /   543 tokens
slot      release: id  3 | task 5579 | stop processing: n_tokens = 1374, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.459 (> 0.100 thold), f_keep = 0.968
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5626 | processing task, is_child = 0
slot update_slots: id  3 | task 5626 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2898
slot update_slots: id  3 | task 5626 | n_tokens = 1330, memory_seq_rm [1330, end)
slot update_slots: id  3 | task 5626 | prompt processing progress, n_tokens = 2834, batch.n_tokens = 1504, progress = 0.977916
slot update_slots: id  3 | task 5626 | n_tokens = 2834, memory_seq_rm [2834, end)
slot update_slots: id  3 | task 5626 | prompt processing progress, n_tokens = 2898, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5626 | prompt done, n_tokens = 2898, batch.n_tokens = 64
slot init_sampler: id  3 | task 5626 | init sampler, took 0.55 ms, tokens: text = 2898, total = 2898
slot update_slots: id  3 | task 5626 | created context checkpoint 3 of 8 (pos_min = 1810, pos_max = 2833, size = 24.012 MiB)
slot print_timing: id  3 | task 5626 | 
prompt eval time =    1753.27 ms /  1568 tokens (    1.12 ms per token,   894.33 tokens per second)
       eval time =    3547.19 ms /   131 tokens (   27.08 ms per token,    36.93 tokens per second)
      total time =    5300.46 ms /  1699 tokens
slot      release: id  3 | task 5626 | stop processing: n_tokens = 3028, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.979 (> 0.100 thold), f_keep = 0.957
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5759 | processing task, is_child = 0
slot update_slots: id  3 | task 5759 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2961
slot update_slots: id  3 | task 5759 | n_tokens = 2898, memory_seq_rm [2898, end)
slot update_slots: id  3 | task 5759 | prompt processing progress, n_tokens = 2961, batch.n_tokens = 63, progress = 1.000000
slot update_slots: id  3 | task 5759 | prompt done, n_tokens = 2961, batch.n_tokens = 63
slot init_sampler: id  3 | task 5759 | init sampler, took 0.62 ms, tokens: text = 2961, total = 2961
slot print_timing: id  3 | task 5759 | 
prompt eval time =     252.33 ms /    63 tokens (    4.01 ms per token,   249.67 tokens per second)
       eval time =    2610.25 ms /    97 tokens (   26.91 ms per token,    37.16 tokens per second)
      total time =    2862.58 ms /   160 tokens
slot      release: id  3 | task 5759 | stop processing: n_tokens = 3057, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.965 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 5857 | processing task, is_child = 0
slot update_slots: id  3 | task 5857 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3069
slot update_slots: id  3 | task 5857 | n_tokens = 2961, memory_seq_rm [2961, end)
slot update_slots: id  3 | task 5857 | prompt processing progress, n_tokens = 3005, batch.n_tokens = 44, progress = 0.979146
slot update_slots: id  3 | task 5857 | n_tokens = 3005, memory_seq_rm [3005, end)
slot update_slots: id  3 | task 5857 | prompt processing progress, n_tokens = 3069, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 5857 | prompt done, n_tokens = 3069, batch.n_tokens = 64
slot init_sampler: id  3 | task 5857 | init sampler, took 0.68 ms, tokens: text = 3069, total = 3069
slot update_slots: id  3 | task 5857 | created context checkpoint 4 of 8 (pos_min = 2033, pos_max = 3004, size = 22.793 MiB)
slot print_timing: id  3 | task 5857 | 
prompt eval time =     325.31 ms /   108 tokens (    3.01 ms per token,   331.99 tokens per second)
       eval time =    7445.88 ms /   269 tokens (   27.68 ms per token,    36.13 tokens per second)
      total time =    7771.19 ms /   377 tokens
slot      release: id  3 | task 5857 | stop processing: n_tokens = 3337, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 6128 | processing task, is_child = 0
slot update_slots: id  3 | task 6128 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 3124
slot update_slots: id  3 | task 6128 | n_tokens = 3069, memory_seq_rm [3069, end)
slot update_slots: id  3 | task 6128 | prompt processing progress, n_tokens = 3124, batch.n_tokens = 55, progress = 1.000000
slot update_slots: id  3 | task 6128 | prompt done, n_tokens = 3124, batch.n_tokens = 55
slot init_sampler: id  3 | task 6128 | init sampler, took 0.63 ms, tokens: text = 3124, total = 3124
slot print_timing: id  3 | task 6128 | 
prompt eval time =     182.90 ms /    55 tokens (    3.33 ms per token,   300.71 tokens per second)
       eval time =   27471.33 ms /   960 tokens (   28.62 ms per token,    34.95 tokens per second)
      total time =   27654.23 ms /  1015 tokens
slot      release: id  3 | task 6128 | stop processing: n_tokens = 4083, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.333 (> 0.100 thold), f_keep = 0.165
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 4083, total state size = 119.754 MiB
srv          load:  - looking for better prompt, base f_keep = 0.165, sim = 0.333
srv        update:  - cache state: 5 prompts, 1104.949 MiB (limits: 8192.000 MiB, 56064 tokens, 128431 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv  get_availabl: prompt cache update took 157.07 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7089 | processing task, is_child = 0
slot update_slots: id  3 | task 7089 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2024
slot update_slots: id  3 | task 7089 | n_past = 675, slot.prompt.tokens.size() = 4083, seq_id = 3, pos_min = 3059, n_swa = 128
slot update_slots: id  3 | task 7089 | restored context checkpoint (pos_min = 252, pos_max = 1079, size = 19.416 MiB)
slot update_slots: id  3 | task 7089 | erased invalidated context checkpoint (pos_min = 675, pos_max = 1265, n_swa = 128, size = 13.859 MiB)
slot update_slots: id  3 | task 7089 | erased invalidated context checkpoint (pos_min = 1810, pos_max = 2833, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 7089 | erased invalidated context checkpoint (pos_min = 2033, pos_max = 3004, n_swa = 128, size = 22.793 MiB)
slot update_slots: id  3 | task 7089 | n_tokens = 675, memory_seq_rm [675, end)
slot update_slots: id  3 | task 7089 | prompt processing progress, n_tokens = 1960, batch.n_tokens = 1285, progress = 0.968379
slot update_slots: id  3 | task 7089 | n_tokens = 1960, memory_seq_rm [1960, end)
slot update_slots: id  3 | task 7089 | prompt processing progress, n_tokens = 2024, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7089 | prompt done, n_tokens = 2024, batch.n_tokens = 64
slot init_sampler: id  3 | task 7089 | init sampler, took 0.35 ms, tokens: text = 2024, total = 2024
slot update_slots: id  3 | task 7089 | created context checkpoint 2 of 8 (pos_min = 1063, pos_max = 1959, size = 21.034 MiB)
slot print_timing: id  3 | task 7089 | 
prompt eval time =    1552.36 ms /  1349 tokens (    1.15 ms per token,   869.00 tokens per second)
       eval time =   15507.73 ms /   604 tokens (   25.68 ms per token,    38.95 tokens per second)
      total time =   17060.08 ms /  1953 tokens
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
slot      release: id  3 | task 7089 | stop processing: n_tokens = 2627, truncated = 0
srv  update_slots: all slots are idle
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.450 (> 0.100 thold), f_keep = 0.508
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7695 | processing task, is_child = 0
slot update_slots: id  3 | task 7695 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 2966
slot update_slots: id  3 | task 7695 | n_past = 1334, slot.prompt.tokens.size() = 2627, seq_id = 3, pos_min = 1603, n_swa = 128
slot update_slots: id  3 | task 7695 | restored context checkpoint (pos_min = 1063, pos_max = 1959, size = 21.034 MiB)
slot update_slots: id  3 | task 7695 | n_tokens = 1334, memory_seq_rm [1334, end)
slot update_slots: id  3 | task 7695 | prompt processing progress, n_tokens = 2902, batch.n_tokens = 1568, progress = 0.978422
slot update_slots: id  3 | task 7695 | n_tokens = 2902, memory_seq_rm [2902, end)
slot update_slots: id  3 | task 7695 | prompt processing progress, n_tokens = 2966, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7695 | prompt done, n_tokens = 2966, batch.n_tokens = 64
slot init_sampler: id  3 | task 7695 | init sampler, took 0.48 ms, tokens: text = 2966, total = 2966
slot update_slots: id  3 | task 7695 | created context checkpoint 3 of 8 (pos_min = 1878, pos_max = 2901, size = 24.012 MiB)
slot print_timing: id  3 | task 7695 | 
prompt eval time =    1996.91 ms /  1632 tokens (    1.22 ms per token,   817.26 tokens per second)
       eval time =    5986.18 ms /   215 tokens (   27.84 ms per token,    35.92 tokens per second)
      total time =    7983.09 ms /  1847 tokens
slot      release: id  3 | task 7695 | stop processing: n_tokens = 3180, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.654 (> 0.100 thold), f_keep = 0.933
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 7912 | processing task, is_child = 0
slot update_slots: id  3 | task 7912 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 4534
slot update_slots: id  3 | task 7912 | n_tokens = 2966, memory_seq_rm [2966, end)
slot update_slots: id  3 | task 7912 | prompt processing progress, n_tokens = 4470, batch.n_tokens = 1504, progress = 0.985884
slot update_slots: id  3 | task 7912 | n_tokens = 4470, memory_seq_rm [4470, end)
slot update_slots: id  3 | task 7912 | prompt processing progress, n_tokens = 4534, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 7912 | prompt done, n_tokens = 4534, batch.n_tokens = 64
slot init_sampler: id  3 | task 7912 | init sampler, took 0.72 ms, tokens: text = 4534, total = 4534
slot update_slots: id  3 | task 7912 | created context checkpoint 4 of 8 (pos_min = 3446, pos_max = 4469, size = 24.012 MiB)
slot print_timing: id  3 | task 7912 | 
prompt eval time =    1942.80 ms /  1568 tokens (    1.24 ms per token,   807.08 tokens per second)
       eval time =   42060.06 ms /  1511 tokens (   27.84 ms per token,    35.92 tokens per second)
      total time =   44002.86 ms /  3079 tokens
slot      release: id  3 | task 7912 | stop processing: n_tokens = 6044, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.787 (> 0.100 thold), f_keep = 0.750
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9425 | processing task, is_child = 0
slot update_slots: id  3 | task 9425 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 5761
slot update_slots: id  3 | task 9425 | n_past = 4534, slot.prompt.tokens.size() = 6044, seq_id = 3, pos_min = 5020, n_swa = 128
slot update_slots: id  3 | task 9425 | restored context checkpoint (pos_min = 3446, pos_max = 4469, size = 24.012 MiB)
slot update_slots: id  3 | task 9425 | n_tokens = 4469, memory_seq_rm [4469, end)
slot update_slots: id  3 | task 9425 | prompt processing progress, n_tokens = 5697, batch.n_tokens = 1228, progress = 0.988891
slot update_slots: id  3 | task 9425 | n_tokens = 5697, memory_seq_rm [5697, end)
slot update_slots: id  3 | task 9425 | prompt processing progress, n_tokens = 5761, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9425 | prompt done, n_tokens = 5761, batch.n_tokens = 64
slot init_sampler: id  3 | task 9425 | init sampler, took 1.09 ms, tokens: text = 5761, total = 5761
slot update_slots: id  3 | task 9425 | created context checkpoint 5 of 8 (pos_min = 4673, pos_max = 5696, size = 24.012 MiB)
slot print_timing: id  3 | task 9425 | 
prompt eval time =    1584.60 ms /  1292 tokens (    1.23 ms per token,   815.35 tokens per second)
       eval time =    3376.44 ms /   130 tokens (   25.97 ms per token,    38.50 tokens per second)
      total time =    4961.04 ms /  1422 tokens
slot      release: id  3 | task 9425 | stop processing: n_tokens = 5890, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.868 (> 0.100 thold), f_keep = 0.978
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 9557 | processing task, is_child = 0
slot update_slots: id  3 | task 9557 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 6639
slot update_slots: id  3 | task 9557 | n_tokens = 5761, memory_seq_rm [5761, end)
slot update_slots: id  3 | task 9557 | prompt processing progress, n_tokens = 6575, batch.n_tokens = 814, progress = 0.990360
slot update_slots: id  3 | task 9557 | n_tokens = 6575, memory_seq_rm [6575, end)
slot update_slots: id  3 | task 9557 | prompt processing progress, n_tokens = 6639, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 9557 | prompt done, n_tokens = 6639, batch.n_tokens = 64
slot init_sampler: id  3 | task 9557 | init sampler, took 1.06 ms, tokens: text = 6639, total = 6639
slot update_slots: id  3 | task 9557 | created context checkpoint 6 of 8 (pos_min = 5551, pos_max = 6574, size = 24.012 MiB)
slot print_timing: id  3 | task 9557 | 
prompt eval time =    1150.85 ms /   878 tokens (    1.31 ms per token,   762.92 tokens per second)
       eval time =   45669.90 ms /  1697 tokens (   26.91 ms per token,    37.16 tokens per second)
      total time =   46820.75 ms /  2575 tokens
slot      release: id  3 | task 9557 | stop processing: n_tokens = 8335, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.856 (> 0.100 thold), f_keep = 0.797
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11256 | processing task, is_child = 0
slot update_slots: id  3 | task 11256 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 7755
slot update_slots: id  3 | task 11256 | n_past = 6639, slot.prompt.tokens.size() = 8335, seq_id = 3, pos_min = 7311, n_swa = 128
slot update_slots: id  3 | task 11256 | restored context checkpoint (pos_min = 5551, pos_max = 6574, size = 24.012 MiB)
slot update_slots: id  3 | task 11256 | n_tokens = 6574, memory_seq_rm [6574, end)
slot update_slots: id  3 | task 11256 | prompt processing progress, n_tokens = 7691, batch.n_tokens = 1117, progress = 0.991747
slot update_slots: id  3 | task 11256 | n_tokens = 7691, memory_seq_rm [7691, end)
slot update_slots: id  3 | task 11256 | prompt processing progress, n_tokens = 7755, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11256 | prompt done, n_tokens = 7755, batch.n_tokens = 64
slot init_sampler: id  3 | task 11256 | init sampler, took 1.56 ms, tokens: text = 7755, total = 7755
slot update_slots: id  3 | task 11256 | created context checkpoint 7 of 8 (pos_min = 6667, pos_max = 7690, size = 24.012 MiB)
slot print_timing: id  3 | task 11256 | 
prompt eval time =    1623.34 ms /  1181 tokens (    1.37 ms per token,   727.51 tokens per second)
       eval time =    3491.99 ms /   125 tokens (   27.94 ms per token,    35.80 tokens per second)
      total time =    5115.33 ms /  1306 tokens
slot      release: id  3 | task 11256 | stop processing: n_tokens = 7879, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  log_server_r: request: GET /health 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.868 (> 0.100 thold), f_keep = 0.007
srv  get_availabl: updating prompt cache
srv   prompt_save:  - saving prompt with length 7879, total state size = 208.767 MiB
srv          load:  - looking for better prompt, base f_keep = 0.007, sim = 0.868
srv        update:  - cache state: 6 prompts, 1474.225 MiB (limits: 8192.000 MiB, 56064 tokens, 140042 est)
srv        update:    - prompt 0x5a03c4076720:    4127 tokens, checkpoints:  7,   262.983 MiB
srv        update:    - prompt 0x5a03cc50f900:    3240 tokens, checkpoints:  8,   268.681 MiB
srv        update:    - prompt 0x5a03cb342990:    3108 tokens, checkpoints:  6,   218.898 MiB
srv        update:    - prompt 0x5a03cbf690f0:    2765 tokens, checkpoints:  3,   154.553 MiB
srv        update:    - prompt 0x5a03cbebdd20:    4083 tokens, checkpoints:  4,   199.834 MiB
srv        update:    - prompt 0x5a03cc1c4b80:    7879 tokens, checkpoints:  7,   369.277 MiB
srv  get_availabl: prompt cache update took 303.31 ms
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11383 | processing task, is_child = 0
slot update_slots: id  3 | task 11383 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 68
slot update_slots: id  3 | task 11383 | n_past = 59, slot.prompt.tokens.size() = 7879, seq_id = 3, pos_min = 6855, n_swa = 128
slot update_slots: id  3 | task 11383 | forcing full prompt re-processing due to lack of cache data (likely due to SWA or hybrid/recurrent memory, see https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 252, pos_max = 1079, n_swa = 128, size = 19.416 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 1063, pos_max = 1959, n_swa = 128, size = 21.034 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 1878, pos_max = 2901, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 3446, pos_max = 4469, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 4673, pos_max = 5696, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 5551, pos_max = 6574, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | erased invalidated context checkpoint (pos_min = 6667, pos_max = 7690, n_swa = 128, size = 24.012 MiB)
slot update_slots: id  3 | task 11383 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  3 | task 11383 | prompt processing progress, n_tokens = 4, batch.n_tokens = 4, progress = 0.058824
slot update_slots: id  3 | task 11383 | n_tokens = 4, memory_seq_rm [4, end)
slot update_slots: id  3 | task 11383 | prompt processing progress, n_tokens = 68, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  3 | task 11383 | prompt done, n_tokens = 68, batch.n_tokens = 64
slot init_sampler: id  3 | task 11383 | init sampler, took 0.01 ms, tokens: text = 68, total = 68
slot print_timing: id  3 | task 11383 | 
prompt eval time =     302.25 ms /    68 tokens (    4.44 ms per token,   224.98 tokens per second)
       eval time =     873.62 ms /    35 tokens (   24.96 ms per token,    40.06 tokens per second)
      total time =    1175.87 ms /   103 tokens
slot      release: id  3 | task 11383 | stop processing: n_tokens = 102, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  3 | task -1 | selected slot by LCP similarity, sim_best = 0.928 (> 0.100 thold), f_keep = 0.627
slot launch_slot_: id  3 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  3 | task 11420 | processing task, is_child = 0
slot update_slots: id  3 | task 11420 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 69
slot update_slots: id  3 | task 11420 | n_tokens = 64, memory_seq_rm [64, end)
slot update_slots: id  3 | task 11420 | prompt processing progress, n_tokens = 69, batch.n_tokens = 5, progress = 1.000000
slot update_slots: id  3 | task 11420 | prompt done, n_tokens = 69, batch.n_tokens = 5
slot init_sampler: id  3 | task 11420 | init sampler, took 0.01 ms, tokens: text = 69, total = 69
slot print_timing: id  3 | task 11420 | 
prompt eval time =      61.61 ms /     5 tokens (   12.32 ms per token,    81.15 tokens per second)
       eval time =     721.65 ms /    29 tokens (   24.88 ms per token,    40.19 tokens per second)
      total time =     783.26 ms /    34 tokens
slot      release: id  3 | task 11420 | stop processing: n_tokens = 97, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LRU, t_last = -1
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11450 | processing task, is_child = 0
slot update_slots: id  2 | task 11450 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 8201
slot update_slots: id  2 | task 11450 | n_tokens = 0, memory_seq_rm [0, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 2048, batch.n_tokens = 2048, progress = 0.249726
slot update_slots: id  2 | task 11450 | n_tokens = 2048, memory_seq_rm [2048, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 4096, batch.n_tokens = 2048, progress = 0.499451
slot update_slots: id  2 | task 11450 | n_tokens = 4096, memory_seq_rm [4096, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 6144, batch.n_tokens = 2048, progress = 0.749177
slot update_slots: id  2 | task 11450 | n_tokens = 6144, memory_seq_rm [6144, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 8137, batch.n_tokens = 1993, progress = 0.992196
slot update_slots: id  2 | task 11450 | n_tokens = 8137, memory_seq_rm [8137, end)
slot update_slots: id  2 | task 11450 | prompt processing progress, n_tokens = 8201, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11450 | prompt done, n_tokens = 8201, batch.n_tokens = 64
slot init_sampler: id  2 | task 11450 | init sampler, took 1.27 ms, tokens: text = 8201, total = 8201
slot update_slots: id  2 | task 11450 | created context checkpoint 1 of 8 (pos_min = 7210, pos_max = 8136, size = 21.737 MiB)
slot print_timing: id  2 | task 11450 | 
prompt eval time =    8849.27 ms /  8201 tokens (    1.08 ms per token,   926.74 tokens per second)
       eval time =    1296.92 ms /    48 tokens (   27.02 ms per token,    37.01 tokens per second)
      total time =   10146.19 ms /  8249 tokens
slot      release: id  2 | task 11450 | stop processing: n_tokens = 8248, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.884 (> 0.100 thold), f_keep = 0.994
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11503 | processing task, is_child = 0
slot update_slots: id  2 | task 11503 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9278
slot update_slots: id  2 | task 11503 | n_tokens = 8201, memory_seq_rm [8201, end)
slot update_slots: id  2 | task 11503 | prompt processing progress, n_tokens = 9214, batch.n_tokens = 1013, progress = 0.993102
slot update_slots: id  2 | task 11503 | n_tokens = 9214, memory_seq_rm [9214, end)
slot update_slots: id  2 | task 11503 | prompt processing progress, n_tokens = 9278, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11503 | prompt done, n_tokens = 9278, batch.n_tokens = 64
slot init_sampler: id  2 | task 11503 | init sampler, took 1.39 ms, tokens: text = 9278, total = 9278
slot update_slots: id  2 | task 11503 | created context checkpoint 2 of 8 (pos_min = 8287, pos_max = 9213, size = 21.737 MiB)
slot print_timing: id  2 | task 11503 | 
prompt eval time =    1414.80 ms /  1077 tokens (    1.31 ms per token,   761.24 tokens per second)
       eval time =    2287.95 ms /    84 tokens (   27.24 ms per token,    36.71 tokens per second)
      total time =    3702.75 ms /  1161 tokens
slot      release: id  2 | task 11503 | stop processing: n_tokens = 9361, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.966 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11589 | processing task, is_child = 0
slot update_slots: id  2 | task 11589 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9601
slot update_slots: id  2 | task 11589 | n_tokens = 9278, memory_seq_rm [9278, end)
slot update_slots: id  2 | task 11589 | prompt processing progress, n_tokens = 9537, batch.n_tokens = 259, progress = 0.993334
slot update_slots: id  2 | task 11589 | n_tokens = 9537, memory_seq_rm [9537, end)
slot update_slots: id  2 | task 11589 | prompt processing progress, n_tokens = 9601, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11589 | prompt done, n_tokens = 9601, batch.n_tokens = 64
slot init_sampler: id  2 | task 11589 | init sampler, took 1.39 ms, tokens: text = 9601, total = 9601
slot update_slots: id  2 | task 11589 | created context checkpoint 3 of 8 (pos_min = 8610, pos_max = 9536, size = 21.737 MiB)
slot print_timing: id  2 | task 11589 | 
prompt eval time =     593.04 ms /   323 tokens (    1.84 ms per token,   544.65 tokens per second)
       eval time =    2516.80 ms /    91 tokens (   27.66 ms per token,    36.16 tokens per second)
      total time =    3109.84 ms /   414 tokens
slot      release: id  2 | task 11589 | stop processing: n_tokens = 9691, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.972 (> 0.100 thold), f_keep = 0.991
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11682 | processing task, is_child = 0
slot update_slots: id  2 | task 11682 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 9879
slot update_slots: id  2 | task 11682 | n_tokens = 9601, memory_seq_rm [9601, end)
slot update_slots: id  2 | task 11682 | prompt processing progress, n_tokens = 9815, batch.n_tokens = 214, progress = 0.993522
slot update_slots: id  2 | task 11682 | n_tokens = 9815, memory_seq_rm [9815, end)
slot update_slots: id  2 | task 11682 | prompt processing progress, n_tokens = 9879, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11682 | prompt done, n_tokens = 9879, batch.n_tokens = 64
slot init_sampler: id  2 | task 11682 | init sampler, took 1.85 ms, tokens: text = 9879, total = 9879
slot update_slots: id  2 | task 11682 | created context checkpoint 4 of 8 (pos_min = 8888, pos_max = 9814, size = 21.737 MiB)
slot print_timing: id  2 | task 11682 | 
prompt eval time =     558.76 ms /   278 tokens (    2.01 ms per token,   497.53 tokens per second)
       eval time =    4986.97 ms /   183 tokens (   27.25 ms per token,    36.70 tokens per second)
      total time =    5545.73 ms /   461 tokens
slot      release: id  2 | task 11682 | stop processing: n_tokens = 10061, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.982
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 11867 | processing task, is_child = 0
slot update_slots: id  2 | task 11867 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10463
slot update_slots: id  2 | task 11867 | n_tokens = 9879, memory_seq_rm [9879, end)
slot update_slots: id  2 | task 11867 | prompt processing progress, n_tokens = 10399, batch.n_tokens = 520, progress = 0.993883
slot update_slots: id  2 | task 11867 | n_tokens = 10399, memory_seq_rm [10399, end)
slot update_slots: id  2 | task 11867 | prompt processing progress, n_tokens = 10463, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 11867 | prompt done, n_tokens = 10463, batch.n_tokens = 64
slot init_sampler: id  2 | task 11867 | init sampler, took 1.53 ms, tokens: text = 10463, total = 10463
slot update_slots: id  2 | task 11867 | created context checkpoint 5 of 8 (pos_min = 9472, pos_max = 10398, size = 21.737 MiB)
slot print_timing: id  2 | task 11867 | 
prompt eval time =     908.67 ms /   584 tokens (    1.56 ms per token,   642.70 tokens per second)
       eval time =   11826.28 ms /   433 tokens (   27.31 ms per token,    36.61 tokens per second)
      total time =   12734.95 ms /  1017 tokens
slot      release: id  2 | task 11867 | stop processing: n_tokens = 10895, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.988 (> 0.100 thold), f_keep = 0.960
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12302 | processing task, is_child = 0
slot update_slots: id  2 | task 12302 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10590
slot update_slots: id  2 | task 12302 | n_tokens = 10463, memory_seq_rm [10463, end)
slot update_slots: id  2 | task 12302 | prompt processing progress, n_tokens = 10526, batch.n_tokens = 63, progress = 0.993957
slot update_slots: id  2 | task 12302 | n_tokens = 10526, memory_seq_rm [10526, end)
slot update_slots: id  2 | task 12302 | prompt processing progress, n_tokens = 10590, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12302 | prompt done, n_tokens = 10590, batch.n_tokens = 64
slot init_sampler: id  2 | task 12302 | init sampler, took 1.50 ms, tokens: text = 10590, total = 10590
slot update_slots: id  2 | task 12302 | created context checkpoint 6 of 8 (pos_min = 9968, pos_max = 10525, size = 13.085 MiB)
slot print_timing: id  2 | task 12302 | 
prompt eval time =     386.90 ms /   127 tokens (    3.05 ms per token,   328.25 tokens per second)
       eval time =   10693.25 ms /   392 tokens (   27.28 ms per token,    36.66 tokens per second)
      total time =   11080.15 ms /   519 tokens
slot      release: id  2 | task 12302 | stop processing: n_tokens = 10981, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.983 (> 0.100 thold), f_keep = 0.964
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 12696 | processing task, is_child = 0
slot update_slots: id  2 | task 12696 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10774
slot update_slots: id  2 | task 12696 | n_tokens = 10590, memory_seq_rm [10590, end)
slot update_slots: id  2 | task 12696 | prompt processing progress, n_tokens = 10710, batch.n_tokens = 120, progress = 0.994060
slot update_slots: id  2 | task 12696 | n_tokens = 10710, memory_seq_rm [10710, end)
slot update_slots: id  2 | task 12696 | prompt processing progress, n_tokens = 10774, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 12696 | prompt done, n_tokens = 10774, batch.n_tokens = 64
slot init_sampler: id  2 | task 12696 | init sampler, took 1.56 ms, tokens: text = 10774, total = 10774
slot update_slots: id  2 | task 12696 | created context checkpoint 7 of 8 (pos_min = 10054, pos_max = 10709, size = 15.383 MiB)
slot print_timing: id  2 | task 12696 | 
prompt eval time =     518.59 ms /   184 tokens (    2.82 ms per token,   354.81 tokens per second)
       eval time =   14393.74 ms /   530 tokens (   27.16 ms per token,    36.82 tokens per second)
      total time =   14912.33 ms /   714 tokens
slot      release: id  2 | task 12696 | stop processing: n_tokens = 11303, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.982 (> 0.100 thold), f_keep = 0.953
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13228 | processing task, is_child = 0
slot update_slots: id  2 | task 13228 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 10975
slot update_slots: id  2 | task 13228 | n_tokens = 10774, memory_seq_rm [10774, end)
slot update_slots: id  2 | task 13228 | prompt processing progress, n_tokens = 10911, batch.n_tokens = 137, progress = 0.994169
slot update_slots: id  2 | task 13228 | n_tokens = 10911, memory_seq_rm [10911, end)
slot update_slots: id  2 | task 13228 | prompt processing progress, n_tokens = 10975, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13228 | prompt done, n_tokens = 10975, batch.n_tokens = 64
slot init_sampler: id  2 | task 13228 | init sampler, took 1.67 ms, tokens: text = 10975, total = 10975
slot update_slots: id  2 | task 13228 | created context checkpoint 8 of 8 (pos_min = 10376, pos_max = 10910, size = 12.545 MiB)
slot print_timing: id  2 | task 13228 | 
prompt eval time =     472.77 ms /   201 tokens (    2.35 ms per token,   425.16 tokens per second)
       eval time =   17315.05 ms /   636 tokens (   27.22 ms per token,    36.73 tokens per second)
      total time =   17787.82 ms /   837 tokens
slot      release: id  2 | task 13228 | stop processing: n_tokens = 11610, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.981 (> 0.100 thold), f_keep = 0.945
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 13866 | processing task, is_child = 0
slot update_slots: id  2 | task 13866 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 11183
slot update_slots: id  2 | task 13866 | n_tokens = 10975, memory_seq_rm [10975, end)
slot update_slots: id  2 | task 13866 | prompt processing progress, n_tokens = 11119, batch.n_tokens = 144, progress = 0.994277
slot update_slots: id  2 | task 13866 | n_tokens = 11119, memory_seq_rm [11119, end)
slot update_slots: id  2 | task 13866 | prompt processing progress, n_tokens = 11183, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 13866 | prompt done, n_tokens = 11183, batch.n_tokens = 64
slot init_sampler: id  2 | task 13866 | init sampler, took 1.59 ms, tokens: text = 11183, total = 11183
slot update_slots: id  2 | task 13866 | erasing old context checkpoint (pos_min = 7210, pos_max = 8136, size = 21.737 MiB)
slot update_slots: id  2 | task 13866 | created context checkpoint 8 of 8 (pos_min = 10774, pos_max = 11118, size = 8.090 MiB)
slot print_timing: id  2 | task 13866 | 
prompt eval time =     482.19 ms /   208 tokens (    2.32 ms per token,   431.37 tokens per second)
       eval time =   19830.68 ms /   728 tokens (   27.24 ms per token,    36.71 tokens per second)
      total time =   20312.87 ms /   936 tokens
slot      release: id  2 | task 13866 | stop processing: n_tokens = 11910, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.931 (> 0.100 thold), f_keep = 0.939
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 14596 | processing task, is_child = 0
slot update_slots: id  2 | task 14596 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 12012
slot update_slots: id  2 | task 14596 | n_tokens = 11183, memory_seq_rm [11183, end)
slot update_slots: id  2 | task 14596 | prompt processing progress, n_tokens = 11948, batch.n_tokens = 765, progress = 0.994672
slot update_slots: id  2 | task 14596 | n_tokens = 11948, memory_seq_rm [11948, end)
slot update_slots: id  2 | task 14596 | prompt processing progress, n_tokens = 12012, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 14596 | prompt done, n_tokens = 12012, batch.n_tokens = 64
slot init_sampler: id  2 | task 14596 | init sampler, took 1.73 ms, tokens: text = 12012, total = 12012
slot update_slots: id  2 | task 14596 | erasing old context checkpoint (pos_min = 8287, pos_max = 9213, size = 21.737 MiB)
slot update_slots: id  2 | task 14596 | created context checkpoint 8 of 8 (pos_min = 11056, pos_max = 11947, size = 20.917 MiB)
slot print_timing: id  2 | task 14596 | 
prompt eval time =    1265.94 ms /   829 tokens (    1.53 ms per token,   654.85 tokens per second)
       eval time =    1303.54 ms /    48 tokens (   27.16 ms per token,    36.82 tokens per second)
      total time =    2569.48 ms /   877 tokens
slot      release: id  2 | task 14596 | stop processing: n_tokens = 12059, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.913 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 14646 | processing task, is_child = 0
slot update_slots: id  2 | task 14646 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 13163
slot update_slots: id  2 | task 14646 | n_tokens = 12012, memory_seq_rm [12012, end)
slot update_slots: id  2 | task 14646 | prompt processing progress, n_tokens = 13099, batch.n_tokens = 1087, progress = 0.995138
slot update_slots: id  2 | task 14646 | n_tokens = 13099, memory_seq_rm [13099, end)
slot update_slots: id  2 | task 14646 | prompt processing progress, n_tokens = 13163, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 14646 | prompt done, n_tokens = 13163, batch.n_tokens = 64
slot init_sampler: id  2 | task 14646 | init sampler, took 1.94 ms, tokens: text = 13163, total = 13163
slot update_slots: id  2 | task 14646 | erasing old context checkpoint (pos_min = 8610, pos_max = 9536, size = 21.737 MiB)
slot update_slots: id  2 | task 14646 | created context checkpoint 8 of 8 (pos_min = 12172, pos_max = 13098, size = 21.737 MiB)
slot print_timing: id  2 | task 14646 | 
prompt eval time =    1660.95 ms /  1151 tokens (    1.44 ms per token,   692.98 tokens per second)
       eval time =   31451.53 ms /  1140 tokens (   27.59 ms per token,    36.25 tokens per second)
      total time =   33112.49 ms /  2291 tokens
slot      release: id  2 | task 14646 | stop processing: n_tokens = 14302, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.932 (> 0.100 thold), f_keep = 0.920
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 15788 | processing task, is_child = 0
slot update_slots: id  2 | task 15788 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14124
slot update_slots: id  2 | task 15788 | n_past = 13163, slot.prompt.tokens.size() = 14302, seq_id = 2, pos_min = 13375, n_swa = 128
slot update_slots: id  2 | task 15788 | restored context checkpoint (pos_min = 12172, pos_max = 13098, size = 21.737 MiB)
slot update_slots: id  2 | task 15788 | n_tokens = 13098, memory_seq_rm [13098, end)
slot update_slots: id  2 | task 15788 | prompt processing progress, n_tokens = 14060, batch.n_tokens = 962, progress = 0.995469
slot update_slots: id  2 | task 15788 | n_tokens = 14060, memory_seq_rm [14060, end)
slot update_slots: id  2 | task 15788 | prompt processing progress, n_tokens = 14124, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 15788 | prompt done, n_tokens = 14124, batch.n_tokens = 64
slot init_sampler: id  2 | task 15788 | init sampler, took 2.83 ms, tokens: text = 14124, total = 14124
slot update_slots: id  2 | task 15788 | erasing old context checkpoint (pos_min = 8888, pos_max = 9814, size = 21.737 MiB)
slot update_slots: id  2 | task 15788 | created context checkpoint 8 of 8 (pos_min = 13133, pos_max = 14059, size = 21.737 MiB)
slot print_timing: id  2 | task 15788 | 
prompt eval time =    1450.78 ms /  1026 tokens (    1.41 ms per token,   707.21 tokens per second)
       eval time =   17141.24 ms /   618 tokens (   27.74 ms per token,    36.05 tokens per second)
      total time =   18592.02 ms /  1644 tokens
slot      release: id  2 | task 15788 | stop processing: n_tokens = 14741, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.944 (> 0.100 thold), f_keep = 0.958
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16408 | processing task, is_child = 0
slot update_slots: id  2 | task 16408 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 14966
slot update_slots: id  2 | task 16408 | n_tokens = 14124, memory_seq_rm [14124, end)
slot update_slots: id  2 | task 16408 | prompt processing progress, n_tokens = 14902, batch.n_tokens = 778, progress = 0.995724
slot update_slots: id  2 | task 16408 | n_tokens = 14902, memory_seq_rm [14902, end)
slot update_slots: id  2 | task 16408 | prompt processing progress, n_tokens = 14966, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16408 | prompt done, n_tokens = 14966, batch.n_tokens = 64
slot init_sampler: id  2 | task 16408 | init sampler, took 2.15 ms, tokens: text = 14966, total = 14966
slot update_slots: id  2 | task 16408 | erasing old context checkpoint (pos_min = 9472, pos_max = 10398, size = 21.737 MiB)
slot update_slots: id  2 | task 16408 | created context checkpoint 8 of 8 (pos_min = 13975, pos_max = 14901, size = 21.737 MiB)
slot print_timing: id  2 | task 16408 | 
prompt eval time =    1258.54 ms /   842 tokens (    1.49 ms per token,   669.03 tokens per second)
       eval time =    1505.27 ms /    54 tokens (   27.88 ms per token,    35.87 tokens per second)
      total time =    2763.81 ms /   896 tokens
slot      release: id  2 | task 16408 | stop processing: n_tokens = 15019, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.995 (> 0.100 thold), f_keep = 0.996
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16464 | processing task, is_child = 0
slot update_slots: id  2 | task 16464 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15042
slot update_slots: id  2 | task 16464 | n_tokens = 14966, memory_seq_rm [14966, end)
slot update_slots: id  2 | task 16464 | prompt processing progress, n_tokens = 14978, batch.n_tokens = 12, progress = 0.995745
slot update_slots: id  2 | task 16464 | n_tokens = 14978, memory_seq_rm [14978, end)
slot update_slots: id  2 | task 16464 | prompt processing progress, n_tokens = 15042, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16464 | prompt done, n_tokens = 15042, batch.n_tokens = 64
slot init_sampler: id  2 | task 16464 | init sampler, took 2.15 ms, tokens: text = 15042, total = 15042
slot update_slots: id  2 | task 16464 | erasing old context checkpoint (pos_min = 9968, pos_max = 10525, size = 13.085 MiB)
slot update_slots: id  2 | task 16464 | created context checkpoint 8 of 8 (pos_min = 14092, pos_max = 14977, size = 20.776 MiB)
slot print_timing: id  2 | task 16464 | 
prompt eval time =     283.48 ms /    76 tokens (    3.73 ms per token,   268.10 tokens per second)
       eval time =    1444.51 ms /    52 tokens (   27.78 ms per token,    36.00 tokens per second)
      total time =    1727.99 ms /   128 tokens
slot      release: id  2 | task 16464 | stop processing: n_tokens = 15093, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.975 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 16518 | processing task, is_child = 0
slot update_slots: id  2 | task 16518 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 15435
slot update_slots: id  2 | task 16518 | n_tokens = 15042, memory_seq_rm [15042, end)
slot update_slots: id  2 | task 16518 | prompt processing progress, n_tokens = 15371, batch.n_tokens = 329, progress = 0.995854
slot update_slots: id  2 | task 16518 | n_tokens = 15371, memory_seq_rm [15371, end)
slot update_slots: id  2 | task 16518 | prompt processing progress, n_tokens = 15435, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 16518 | prompt done, n_tokens = 15435, batch.n_tokens = 64
slot init_sampler: id  2 | task 16518 | init sampler, took 2.21 ms, tokens: text = 15435, total = 15435
slot update_slots: id  2 | task 16518 | erasing old context checkpoint (pos_min = 10054, pos_max = 10709, size = 15.383 MiB)
slot update_slots: id  2 | task 16518 | created context checkpoint 8 of 8 (pos_min = 14444, pos_max = 15370, size = 21.737 MiB)
slot print_timing: id  2 | task 16518 | 
prompt eval time =     679.02 ms /   393 tokens (    1.73 ms per token,   578.77 tokens per second)
       eval time =   13717.06 ms /   496 tokens (   27.66 ms per token,    36.16 tokens per second)
      total time =   14396.08 ms /   889 tokens
slot      release: id  2 | task 16518 | stop processing: n_tokens = 15930, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.958 (> 0.100 thold), f_keep = 0.969
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17016 | processing task, is_child = 0
slot update_slots: id  2 | task 17016 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 16120
slot update_slots: id  2 | task 17016 | n_tokens = 15435, memory_seq_rm [15435, end)
slot update_slots: id  2 | task 17016 | prompt processing progress, n_tokens = 16056, batch.n_tokens = 621, progress = 0.996030
slot update_slots: id  2 | task 17016 | n_tokens = 16056, memory_seq_rm [16056, end)
slot update_slots: id  2 | task 17016 | prompt processing progress, n_tokens = 16120, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17016 | prompt done, n_tokens = 16120, batch.n_tokens = 64
slot init_sampler: id  2 | task 17016 | init sampler, took 2.52 ms, tokens: text = 16120, total = 16120
slot update_slots: id  2 | task 17016 | erasing old context checkpoint (pos_min = 10376, pos_max = 10910, size = 12.545 MiB)
slot update_slots: id  2 | task 17016 | created context checkpoint 8 of 8 (pos_min = 15385, pos_max = 16055, size = 15.735 MiB)
slot print_timing: id  2 | task 17016 | 
prompt eval time =    1136.35 ms /   685 tokens (    1.66 ms per token,   602.81 tokens per second)
       eval time =    1306.62 ms /    47 tokens (   27.80 ms per token,    35.97 tokens per second)
      total time =    2442.96 ms /   732 tokens
slot      release: id  2 | task 17016 | stop processing: n_tokens = 16166, truncated = 0
srv  update_slots: all slots are idle
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv  params_from_: Chat format: GPT-OSS
slot get_availabl: id  2 | task -1 | selected slot by LCP similarity, sim_best = 0.933 (> 0.100 thold), f_keep = 0.997
slot launch_slot_: id  2 | task -1 | sampler chain: logits -> ?penalties -> ?dry -> ?top-n-sigma -> top-k -> ?typical -> top-p -> min-p -> ?xtc -> temp-ext -> dist 
slot launch_slot_: id  2 | task 17065 | processing task, is_child = 0
slot update_slots: id  2 | task 17065 | new prompt, n_ctx_slot = 56064, n_keep = 0, task.n_tokens = 17271
slot update_slots: id  2 | task 17065 | n_tokens = 16120, memory_seq_rm [16120, end)
slot update_slots: id  2 | task 17065 | prompt processing progress, n_tokens = 17207, batch.n_tokens = 1087, progress = 0.996294
slot update_slots: id  2 | task 17065 | n_tokens = 17207, memory_seq_rm [17207, end)
slot update_slots: id  2 | task 17065 | prompt processing progress, n_tokens = 17271, batch.n_tokens = 64, progress = 1.000000
slot update_slots: id  2 | task 17065 | prompt done, n_tokens = 17271, batch.n_tokens = 64
slot init_sampler: id  2 | task 17065 | init sampler, took 3.70 ms, tokens: text = 17271, total = 17271
slot update_slots: id  2 | task 17065 | erasing old context checkpoint (pos_min = 10774, pos_max = 11118, size = 8.090 MiB)
slot update_slots: id  2 | task 17065 | created context checkpoint 8 of 8 (pos_min = 16280, pos_max = 17206, size = 21.737 MiB)
srv  log_server_r: request: POST /v1/chat/completions 127.0.0.1 200
srv          stop: cancel task, id_task = 17065
slot      release: id  2 | task 17065 | stop processing: n_tokens = 17282, truncated = 0
srv  update_slots: all slots are idle
